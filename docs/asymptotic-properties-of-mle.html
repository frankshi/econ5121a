<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>8 Asymptotic Properties of MLE | Econ5121</title>
<meta name="author" content="Zhentao Shi">
<meta name="description" content="8.1 Examples of MLE Normal, Logistic, Probit, Poisson  8.2 Consistency We specify a parametric distribution (pdf) \(f\left(x;\theta\right)\) and a parameter space \(\Theta\). Define...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="8 Asymptotic Properties of MLE | Econ5121">
<meta property="og:type" content="book">
<meta property="og:description" content="8.1 Examples of MLE Normal, Logistic, Probit, Poisson  8.2 Consistency We specify a parametric distribution (pdf) \(f\left(x;\theta\right)\) and a parameter space \(\Theta\). Define...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="8 Asymptotic Properties of MLE | Econ5121">
<meta name="twitter:description" content="8.1 Examples of MLE Normal, Logistic, Probit, Poisson  8.2 Consistency We specify a parametric distribution (pdf) \(f\left(x;\theta\right)\) and a parameter space \(\Theta\). Define...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Alice-0.4.1/font.css" rel="stylesheet">
<link href="libs/_DM%20Mono-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Spectral-0.4.1/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Econ5121</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="conditional-expectation.html"><span class="header-section-number">3</span> Conditional Expectation</a></li>
<li><a class="" href="least-squares-linear-algebra.html"><span class="header-section-number">4</span> Least Squares: Linear Algebra</a></li>
<li><a class="" href="least-squares-finite-sample-theory.html"><span class="header-section-number">5</span> Least Squares: Finite Sample Theory</a></li>
<li><a class="" href="basic-asymptotic-theory.html"><span class="header-section-number">6</span> Basic Asymptotic Theory</a></li>
<li><a class="" href="asymptotic-properties-of-least-squares.html"><span class="header-section-number">7</span> Asymptotic Properties of Least Squares</a></li>
<li><a class="active" href="asymptotic-properties-of-mle.html"><span class="header-section-number">8</span> Asymptotic Properties of MLE</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></li>
<li><a class="" href="panel-data.html"><span class="header-section-number">10</span> Panel Data</a></li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">11</span> Endogeneity</a></li>
<li><a class="" href="generalized-method-of-moments.html"><span class="header-section-number">12</span> Generalized Method of Moments</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/zhentaoshi/Econ5121A">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="asymptotic-properties-of-mle" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Asymptotic Properties of MLE<a class="anchor" aria-label="anchor" href="#asymptotic-properties-of-mle"><i class="fas fa-link"></i></a>
</h1>
<div id="examples-of-mle" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Examples of MLE<a class="anchor" aria-label="anchor" href="#examples-of-mle"><i class="fas fa-link"></i></a>
</h2>
<p>Normal, Logistic, Probit, Poisson</p>
</div>
<div id="consistency-1" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Consistency<a class="anchor" aria-label="anchor" href="#consistency-1"><i class="fas fa-link"></i></a>
</h2>
<p>We specify a parametric distribution (pdf) <span class="math inline">\(f\left(x;\theta\right)\)</span> and
a parameter space <span class="math inline">\(\Theta\)</span>. Define
<span class="math inline">\(Q\left(\theta\right)=E\left[\log f\left(x;\theta\right)\right]\)</span>, and
<span class="math inline">\(\theta_{0}=\arg\max_{\theta\in\Theta}Q\left(\theta\right)\)</span> maximizes
the expected log-likelihood. Given a sample of <span class="math inline">\(n\)</span> observations, we
compute the average sample log-likelihood
<span class="math inline">\(\ell_{n}\left(\theta\right)=\frac{1}{n}\sum_{i=1}^{n}\log f\left(x;\theta\right)\)</span>.
The MLE estimator is
<span class="math inline">\(\widehat{\theta}=\arg\max_{\theta\in\Theta}\ell_{n}\left(\theta\right)\)</span>.</p>
<p>We say that <em>correctly specified</em> if the data
<span class="math inline">\(\left(x_{1},\ldots,x_{n}\right)\)</span> is generated from the pdf
<span class="math inline">\(f\left(x;\theta\right)\)</span> for some <span class="math inline">\(\theta\in\Theta\)</span>. Otherwise if the
data is not generated from any member in the class of distributions
<span class="math inline">\(\mathcal{M}^{*}:=\left\{ \theta\in\Theta:f\left(x;\theta\right)\right\}\)</span>,
we say it is <em>misspecified</em>. When the model is misspecified, strictly
speaking the log-likelihood function <span class="math inline">\(\ell_{n}\left(\theta\right)\)</span>
should be called quasi log-likelihood and the MLE estimator
<span class="math inline">\(\widehat{\theta}\)</span> should be called the <em>quasi MLE</em>.</p>
<p>We will discuss under what condition
<span class="math inline">\(\widehat{\theta}\stackrel{p}{\to}\theta_{0}\)</span>, that is, the maximizer of
the sample log-likelihood converges in probability to the maximizer of
the expected log-likelihood in population. Notice that unlike OLS, most
MLE estimators do not admit a closed-form. They are defined as a
maximizer and solved by numerical optimization.</p>
<p>The first requirement for the consistency of MLE is that <span class="math inline">\(\theta_{0}\)</span>
uniquely defined. Suppose <span class="math inline">\(\theta_{0}\in\mathrm{int}\left(\Theta\right)\)</span>
lies in the interior of <span class="math inline">\(\Theta\)</span>. Let
<span class="math inline">\(N\left(\theta_{0},\varepsilon\right)=\left\{ \theta\in\Theta:\left|\theta-\theta_{0}\right|&lt;\varepsilon\right\}\)</span>
is a neighborhood around <span class="math inline">\(\theta_{0}\)</span> with radius <span class="math inline">\(\varepsilon\)</span> for some
<span class="math inline">\(\varepsilon&gt;0\)</span>.</p>
<p>The value <span class="math inline">\(\theta_{0}\)</span> is identified if for any <span class="math inline">\(\varepsilon&gt;0\)</span>, there
exists a <span class="math inline">\(\delta=\delta\left(\varepsilon\right)&gt;0\)</span> such that
<span class="math inline">\(Q\left(\theta_{0}\right)&gt;\sup_{\theta\in\Theta\backslash N\left(\theta_{0},\varepsilon\right)}Q\left(\theta\right)+\delta\)</span>.</p>
<p>We know under suitable condition, LLN implies
<span class="math inline">\(\ell_{n}\left(\theta\right)\stackrel{p}{\to}Q\left(\theta\right)\)</span> for
each <span class="math inline">\(\theta\in\Theta\)</span>. This is a pointwise result, meaning <span class="math inline">\(\theta\)</span> is
taken as fixed as <span class="math inline">\(n\to\infty\)</span>. However, <span class="math inline">\(\widehat{\theta}\)</span> is random in
finite-sample, which makes <span class="math inline">\(\ell_{n}(\widehat{\theta})\)</span> a complicated
function of the data in particular when <span class="math inline">\(\widehat{\theta}\)</span> has no
closed-form solution. We therefore need to strengthen the pointwise LLN.</p>
<p>We say a <em>uniform law of large numbers</em> (ULLN) for
<span class="math inline">\(Q\left(\theta\right)\)</span> holds on <span class="math inline">\(\Theta\)</span> if
<span class="math display">\[P\left\{ \sup_{\theta\in\Theta}\left|\ell_{n}\left(\theta\right)-Q\left(\theta\right)\right|\geq\varepsilon\right\} \to0\label{eq:ULLN}\]</span>
for all <span class="math inline">\(\varepsilon&gt;0\)</span> as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>ULLN can be established under pointwise LLN plus some regularity
conditions, for example when <span class="math inline">\(\Theta\)</span> is a compact set, and
<span class="math inline">\(\log f\left(x;\cdot\right)\)</span> is continuous in <span class="math inline">\(\theta\)</span> almost everywhere
on the support of <span class="math inline">\(x\)</span>.</p>
<p>If <span class="math inline">\(\theta_{0}\)</span> is identified and ULLN
<a href="#eq:ULLN" reference-type="eqref" reference="eq:ULLN"><span class="math display">\[eq:ULLN\]</span></a>
hold, then <span class="math inline">\(\widehat{\theta}\stackrel{p}{\to}\theta_{0}\)</span>.</p>
<p>According to the definition of consistency, we can check
<span class="math display">\[\begin{aligned}
&amp; P\left\{ \left|\widehat{\theta}-\theta_{0}\right|&gt;\varepsilon\right\} \leq P\left\{ Q\left(\theta_{0}\right)-Q(\widehat{\theta})&gt;\delta\right\} \\
&amp; =P\left\{ Q\left(\theta_{0}\right)-\ell_{n}\left(\theta_{0}\right)+\ell_{n}\left(\theta_{0}\right)-\ell_{n}(\widehat{\theta})+\ell_{n}\left(\widehat{\theta}\right)-Q(\widehat{\theta})&gt;\delta\right\} \\
&amp; \leq P\left\{ \left|Q\left(\theta_{0}\right)-\ell_{n}\left(\theta_{0}\right)\right|+\ell_{n}\left(\theta_{0}\right)-\ell_{n}(\widehat{\theta})+\left|\ell_{n}\left(\widehat{\theta}\right)-Q(\widehat{\theta})\right|&gt;\delta\right\} \\
&amp; \leq P\left\{ \left|Q\left(\theta_{0}\right)-\ell_{n}\left(\theta_{0}\right)\right|+\left|\ell_{n}(\widehat{\theta})-Q(\widehat{\theta})\right|\geq\delta\right\} \\
&amp; \leq P\left\{ 2\sup_{\theta\in\Theta}\left|\ell_{n}\left(\theta\right)-Q\left(\theta\right)\right|\geq\delta\right\} =P\left\{ \sup_{\theta\in\Theta}\left|\ell_{n}\left(\theta\right)-Q\left(\theta\right)\right|\geq\frac{\delta}{2}\right\} \to0.\end{aligned}\]</span>
The first line holds because of identification, the third line by the
triangle inequality, the fourth line by the definition of MLE that
<span class="math inline">\(\ell_{n}(\widehat{\theta})\geq\ell_{n}\left(\theta_{0}\right)\)</span>, and the
last line by ULLN.</p>
<p>Identification is a necessary condition for consistent estimation.
Although <span class="math inline">\(\widehat{\theta}\)</span> has no closed-form solution in general, we
establish consistency via ULLN over all point <span class="math inline">\(\theta\in\Theta\)</span> under
consideration.</p>
</div>
<div id="asymptotic-normality" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Asymptotic Normality<a class="anchor" aria-label="anchor" href="#asymptotic-normality"><i class="fas fa-link"></i></a>
</h2>
<p>The next step is to derive the asymptotic distribution of the MLE
estimator. Let
<span class="math inline">\(s\left(x;\theta\right)=\partial\log f\left(x;\theta\right)/\partial\theta\)</span>
and
<span class="math inline">\(h\left(x;\theta\right)=\frac{\partial^{2}}{\partial\theta\partial\theta'}\log f\left(x;\theta\right)\)</span></p>
<p><span id="thm:mis-MLE" label="thm:mis-MLE"><span class="math display">\[thm:mis-MLE\]</span></span> Under suitable
regularity conditions, the MLE estimator
<span class="math display">\[\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\left(E\left[h\left(x;\theta_{0}\right)\right]\right)^{-1}\mathrm{var}\left[s\left(x;\theta_{0}\right)\right]\left(E\left[h\left(x;\theta_{0}\right)\right]\right)^{-1}\right).\]</span></p>
<p>The “suitable regularity conditions” will be spelled out later. Indeed,
those conditions can be observed in the proof.</p>
<p>That <span class="math inline">\(\widehat{\theta}\)</span> is a maximizer entails
<span class="math inline">\(\frac{\partial}{\partial\theta}\ell_{n}\left(\widehat{\theta}\right)=0\)</span>.
Take a Taylor expansion of
<span class="math inline">\(\frac{\partial}{\partial\theta}\ell_{n}\left(\widehat{\theta}\right)\)</span>
around <span class="math inline">\(\frac{\partial}{\partial\theta}\ell_{n}\left(\theta_{0}\right)\)</span>:
<span class="math display">\[0-\frac{\partial}{\partial\theta}\ell_{n}\left(\theta_{0}\right)=\frac{\partial}{\partial\theta}\ell_{n}\left(\widehat{\theta}\right)-\frac{\partial}{\partial\theta}\ell_{n}\left(\theta_{0}\right)=\frac{\partial}{\partial\theta\partial\theta'}\ell_{n}\left(\dot{\theta}\right)\left(\widehat{\theta}-\theta_{0}\right)\]</span>
where <span class="math inline">\(\dot{\theta}\)</span> is some point on the line segment connecting
<span class="math inline">\(\widehat{\theta}\)</span> and <span class="math inline">\(\theta_{0}.\)</span> Rearrange the above equation and
multiply both side by <span class="math inline">\(\sqrt{n}:\)</span>
<span class="math display">\[\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)=-\left(\frac{\partial}{\partial\theta\partial\theta'}\ell_{n}\left(\dot{\theta}\right)\right)^{-1}\sqrt{n}\frac{\partial}{\partial\theta}\ell_{n}\left(\theta_{0}\right).\label{eq:taylor1}\]</span></p>
<p>When <span class="math inline">\(Q\left(\theta\right)\)</span> is differentiable at <span class="math inline">\(\theta_{0}\)</span>, we have
<span class="math inline">\(\frac{\partial}{\partial\theta}Q\left(\theta_{0}\right)=0\)</span> by the first
condition of optimality of <span class="math inline">\(\theta_{0}\)</span> for <span class="math inline">\(Q\left(\theta\right)\)</span>.
Notice that
<span class="math inline">\(E\left[s\left(x;\theta_{0}\right)\right]=\frac{\partial}{\partial\theta}Q\left(\theta_{0}\right)=0\)</span>
if differentiation and integration are interchangeable. By CLT, the
second factor in <a href="#eq:taylor1" reference-type="eqref" reference="eq:taylor1"><span class="math display">\[eq:taylor1\]</span></a> follows
<span class="math display">\[\sqrt{n}\frac{\partial}{\partial\theta}\ell_{n}\left(\theta_{0}\right)\stackrel{d}{\to}N\left(0,\mathrm{var}\left[s\left(x;\theta_{0}\right)\right]\right).\]</span>
Suppose the second factor in
<a href="#eq:taylor1" reference-type="eqref" reference="eq:taylor1"><span class="math display">\[eq:taylor1\]</span></a> follows
<span class="math inline">\(\frac{\partial}{\partial\theta\partial\theta'}\ell_{n}\left(\dot{\theta}\right)\stackrel{p}{\to}E\left[h\left(x;\theta_{0}\right)\right]\)</span>
(sufficient if we assume
<span class="math inline">\(E\left[\frac{\partial^{3}}{\partial\theta_{i}\partial\theta_{j}\partial\theta_{l}}\log f\left(x;\theta_{0}\right)\right]\)</span>
is continuous in <span class="math inline">\(\theta\)</span> for all <span class="math inline">\(i,j,l\leq K\)</span>. Thus we have the
conclusion by Slutsky’s theorem.</p>
<p>When the model is misspecified, the asymptotic variance takes a
complicated sandwich form. When the parametric model is correctly
specified, then the asymptotic variance can be further simplified,
thanks to the following important result of information matrix equality.</p>
</div>
<div id="information-matrix-equality" class="section level2" number="8.4">
<h2>
<span class="header-section-number">8.4</span> Information Matrix Equality<a class="anchor" aria-label="anchor" href="#information-matrix-equality"><i class="fas fa-link"></i></a>
</h2>
<p>When the model is correctly specified, <span class="math inline">\(\theta_{0}\)</span> is the <em>true</em>
parameter value. The variance
<span class="math inline">\(\mathcal{I}\left(\theta_{0}\right):=\mathrm{var}_{f\left(x;\theta_{0}\right)}\left[\frac{\partial}{\partial\theta}\log f\left(x;\theta_{0}\right)\right]\)</span>
is called the <em>(Fisher) information matrix</em>, and
<span class="math inline">\(\mathcal{H}\left(\theta_{0}\right):=E_{f\left(x;\theta_{0}\right)}\left[h\left(x;\theta_{0}\right)\right]\)</span>
is called the <em>expected Hessian matrix</em>. Here we emphasize the true
underlying distribution <span class="math inline">\(f\left(x;\theta_{0}\right)\)</span> by writing it as
the subscript of the mathematical expectations.</p>
<p><span id="fact:Info" label="fact:Info"><span class="math display">\[fact:Info\]</span></span>Under suitable regularity
conditions, we have
<span class="math inline">\(\mathcal{I}\left(\theta_{0}\right)=-\mathcal{H}\left(\theta_{0}\right)\)</span></p>
<p>Because <span class="math inline">\(f\left(x;\theta_{0}\right)\)</span> a pdf,
<span class="math inline">\(\int f\left(x;\theta_{0}\right)dx=1\)</span>. Take partial derivative with
respect to <span class="math inline">\(\theta\)</span>, <span class="math display">\[\begin{aligned}
0 &amp; =\int\frac{\partial}{\partial\theta}f\left(x;\theta_{0}\right)dx=\int\frac{\partial f\left(x;\theta_{0}\right)/\partial\theta}{f\left(x;\theta_{0}\right)}f\left(x;\theta_{0}\right)dx\nonumber \\
&amp; =\int\left[s\left(x;\theta_{0}\right)\right]f\left(x;\theta_{0}\right)dx=E_{f\left(x;\theta_{0}\right)}\left[s\left(x;\theta_{0}\right)\right]\label{eq:info_eqn_1}\end{aligned}\]</span>
where the third equality holds as by the chain rule
<span class="math display">\[s\left(x;\theta_{0}\right)=\frac{\partial f\left(x;\theta_{0}\right)/\partial\theta}{f\left(x;\theta_{0}\right)}.\label{eq:ell_d}\]</span>
Take a second partial derivative of
(<a href="#eq:info_eqn_1" reference-type="ref" reference="eq:info_eqn_1"><span class="math display">\[eq:info\_eqn\_1\]</span></a>) with respective to <span class="math inline">\(\theta\)</span>, according to
the chain rule: <span class="math display">\[\begin{aligned}
0 &amp; =\int\left[h\left(x;\theta_{0}\right)\right]f\left(x;\theta_{0}\right)dx+\int\left[s\left(x;\theta_{0}\right)\right]\frac{\partial}{\partial\theta'}f\left(x;\theta_{0}\right)dx\\
&amp; =\int\left[h\left(x;\theta_{0}\right)\right]f\left(x;\theta_{0}\right)dx+\int s\left(x;\theta_{0}\right)\frac{\partial f\left(x;\theta_{0}\right)/\partial\theta}{f\left(x;\theta\right)}f\left(x;\theta_{0}\right)dx\\
&amp; =\int\left[h\left(x;\theta_{0}\right)\right]f\left(x;\theta_{0}\right)dx+\int\left[s\left(x;\theta_{0}\right)s\left(x;\theta_{0}\right)'\right]f\left(x;\theta_{0}\right)dx\\
&amp; =E_{f\left(x;\theta_{0}\right)}\left[h\left(x;\theta_{0}\right)\right]+E_{f\left(x;\theta_{0}\right)}\left[s\left(x;\theta_{0}\right)s\left(x;\theta_{0}\right)'\right]\\
&amp; =\mathcal{H}\left(\theta_{0}\right)+\mathcal{I}\left(\theta_{0}\right).\end{aligned}\]</span>
The second equality follows by
(<a href="#eq:ell_d" reference-type="ref" reference="eq:ell_d"><span class="math display">\[eq:ell\_d\]</span></a>).
The last equality by
<a href="#eq:info_eqn_1" reference-type="eqref" reference="eq:info_eqn_1"><span class="math display">\[eq:info\_eqn\_1\]</span></a> as the zero mean ensures the variance of
<span class="math inline">\(\frac{\partial}{\partial\theta}\log f\left(x;\theta_{0}\right)\)</span> is
equal to the expectation of its out-product.</p>
<p>Notice that a correct specification is essential for the information
matrix equality. If the true data generating distribution is
<span class="math inline">\(g\notin\mathcal{M}^{*}\)</span>, then
<a href="#eq:info_eqn_1" reference-type="eqref" reference="eq:info_eqn_1"><span class="math display">\[eq:info\_eqn\_1\]</span></a> breaks down because
<span class="math display">\[0=\int\frac{\partial}{\partial\theta}f\left(x;\theta_{0}\right)=\int\left[g^{-1}\frac{\partial}{\partial\theta}f\left(x;\theta_{0}\right)\right]g=E_{g}\left[g^{-1}\frac{\partial}{\partial\theta}f\left(x;\theta_{0}\right)\right]\]</span>
but
<span class="math inline">\(g^{-1}\frac{\partial}{\partial\theta}f\left(x;\theta_{0}\right)\neq\left(f\left(x;\theta_{0}\right)\right)^{-1}\frac{\partial}{\partial\theta}f\left(x;\theta_{0}\right)=\frac{\partial}{\partial\theta}\log f\left(\theta_{0}\right)\)</span>.
The asymptotic variance in Theorem
<a href="asymptotic-properties-of-mle.html#thm:mis-MLE" reference-type="ref" reference="thm:mis-MLE"><span class="math display">\[thm:mis-MLE\]</span></a>,
<span class="math display">\[\left(E_{g}\left[h\left(x;\theta_{0}\right)\right]\right)^{-1}\mathrm{var}_{g}\left[s\left(x;\theta_{0}\right)\right]\left(E_{g}\left[h\left(x;\theta_{0}\right)\right]\right)^{-1},\]</span>
written explicitly in <span class="math inline">\(E_{g}\left[\cdot\right]\)</span>, is still valid.</p>
<p>When the parametric model <span class="math inline">\(\mathcal{M}^{*}\)</span> is correctly specified, then
we can replace
<span class="math inline">\(E_{g}\left[\frac{\partial^{2}\ell_{n}}{\partial\theta\partial\theta'}\left(\theta_{0}\right)\right]\)</span>
by <span class="math inline">\(\mathcal{H}\left(\theta_{0}\right)\)</span> and replace
<span class="math inline">\(\mathrm{var}_{g}\left[\frac{\partial\ell_{n}}{\partial\theta}\left(\theta_{0}\right)\right]\)</span>
by <span class="math inline">\(\mathcal{I}\left(\theta_{0}\right)\)</span>, we simplify the asymptotic
variance as
<span class="math display">\[\left(\mathcal{H}\left(\theta_{0}\right)\right)^{-1}\mathcal{I}\left(\theta_{0}\right)\left(\mathcal{H}\left(\theta_{0}\right)\right)^{-1}=\left(-\mathcal{I}\left(\theta_{0}\right)\right)^{-1}\mathcal{I}\left(\theta_{0}\right)\left(-\mathcal{I}\left(\theta_{0}\right)\right)^{-1}=\left(\mathcal{I}\left(\theta_{0}\right)\right)^{-1}\]</span>
by the information matrix equality Fact
<a href="asymptotic-properties-of-mle.html#fact:Info" reference-type="ref" reference="fact:Info"><span class="math display">\[fact:Info\]</span></a>.</p>
<p>If the model is correctly specified, under the conditions for Theorem
<a href="#eq:info_eqn_1" reference-type="ref" reference="eq:info_eqn_1"><span class="math display">\[eq:info\_eqn\_1\]</span></a> and Fact
<a href="asymptotic-properties-of-mle.html#fact:Info" reference-type="ref" reference="fact:Info"><span class="math display">\[fact:Info\]</span></a>
the MLE estimator
<span class="math display">\[\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\left[\mathcal{I}\left(\theta_{0}\right)\right]^{-1}\right).\]</span></p>
<p>This is the classical asymptotic normality result of MLE.</p>
</div>
<div id="cramer-rao-lower-bound" class="section level2" number="8.5">
<h2>
<span class="header-section-number">8.5</span> Cramer-Rao Lower Bound<a class="anchor" aria-label="anchor" href="#cramer-rao-lower-bound"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="summary-6" class="section level2" number="8.6">
<h2>
<span class="header-section-number">8.6</span> Summary<a class="anchor" aria-label="anchor" href="#summary-6"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Further reading</strong>: <span class="citation">White (<a href="generalized-method-of-moments.html#ref-white1996estimation" role="doc-biblioref">1996</a>)</span>, <span class="citation">Newey and McFadden (<a href="generalized-method-of-moments.html#ref-newey1994large" role="doc-biblioref">1994</a>)</span>.</p>
<p><code>Zhentao Shi. Oct 29, 2020.</code></p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="asymptotic-properties-of-least-squares.html"><span class="header-section-number">7</span> Asymptotic Properties of Least Squares</a></div>
<div class="next"><a href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#asymptotic-properties-of-mle"><span class="header-section-number">8</span> Asymptotic Properties of MLE</a></li>
<li><a class="nav-link" href="#examples-of-mle"><span class="header-section-number">8.1</span> Examples of MLE</a></li>
<li><a class="nav-link" href="#consistency-1"><span class="header-section-number">8.2</span> Consistency</a></li>
<li><a class="nav-link" href="#asymptotic-normality"><span class="header-section-number">8.3</span> Asymptotic Normality</a></li>
<li><a class="nav-link" href="#information-matrix-equality"><span class="header-section-number">8.4</span> Information Matrix Equality</a></li>
<li><a class="nav-link" href="#cramer-rao-lower-bound"><span class="header-section-number">8.5</span> Cramer-Rao Lower Bound</a></li>
<li><a class="nav-link" href="#summary-6"><span class="header-section-number">8.6</span> Summary</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/zhentaoshi/Econ5121A/blob/master/07-lecture.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/zhentaoshi/Econ5121A/edit/master/07-lecture.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Econ5121</strong>" was written by Zhentao Shi. It was last built on 2022-06-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
