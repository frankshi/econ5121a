<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="0.2 Linear Projection | Regression, Projection and Causality" />
<meta property="og:type" content="book" />


<meta property="og:description" content="nothing" />


<meta name="author" content="Zhentao Shi" />

<meta name="date" content="2022-01-16" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="nothing">

<title>0.2 Linear Projection | Regression, Projection and Causality</title>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a>
<ul>
<li><a href="personal-reflection.html#personal-reflection">Personal Reflection</a></li>
<li><a href="prerequisite.html#prerequisite">Prerequisite</a></li>
<li><a href="structure.html#structure">Structure</a></li>
<li><a href="packages.html#packages">Packages</a></li>
<li><a href="0.1-conditional-expectation.html#conditional-expectation"><span class="toc-section-number">0.1</span> Conditional Expectation</a></li>
<li><a href="0.2-linear-projection.html#linear-projection"><span class="toc-section-number">0.2</span> Linear Projection</a>
<ul>
<li><a href="0.2-linear-projection.html#omitted-variable-bias"><span class="toc-section-number">0.2.1</span> Omitted Variable Bias</a></li>
</ul></li>
<li><a href="0.3-causality.html#causality"><span class="toc-section-number">0.3</span> Causality</a>
<ul>
<li><a href="0.3-causality.html#structure-and-identification"><span class="toc-section-number">0.3.1</span> Structure and Identification</a></li>
<li><a href="0.3-causality.html#treatment-effect"><span class="toc-section-number">0.3.2</span> Treatment Effect</a></li>
<li><a href="0.3-causality.html#ate-and-cef"><span class="toc-section-number">0.3.3</span> ATE and CEF</a></li>
</ul></li>
<li><a href="0.4-summary.html#summary"><span class="toc-section-number">0.4</span> Summary</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="linear-projection" class="section level2" number="0.2">
<h2><span class="header-section-number">0.2</span> Linear Projection</h2>
<p>The CEF <span class="math inline">\(m(x)\)</span> is the function that minimizes the MSE. However,
<span class="math inline">\(m\left(x\right)=E\left[y|x\right]\)</span> is a complex function of <span class="math inline">\(x\)</span>, for it
depends on the joint distribution of <span class="math inline">\(\left(y,x\right)\)</span>, which is mostly
unknown in practice. Now let us make the prediction task even simpler.
How about we minimize the MSE within all linear functions in the form of
<span class="math inline">\(h\left(x\right)=h\left(x;b\right)=x&#39;b\)</span> for <span class="math inline">\(b\in\mathbb{R}^{K}\)</span>? The
minimization problem is
<span class="math display">\[\min_{b\in\mathbb{R}^{K}}E\left[\left(y-x&#39;b\right)^{2}\right].\label{eq:linear_MSE}\]</span>
Take the first-order condition of the MSE
<span class="math display">\[\frac{\partial}{\partial b}E\left[\left(y-x&#39;b\right)^{2}\right]=E\left[\frac{\partial}{\partial b}\left(y-x&#39;b\right)^{2}\right]=-2E\left[x\left(y-x&#39;b\right)\right],\]</span>
where the first equality holds if
<span class="math inline">\(E\left[\left(y-x&#39;b\right)^{2}\right]&lt;\infty\)</span> so that the expectation
and partial differentiation is interchangeable, and the second equality
hods by the chain rule and the linearity of expectation. Set the first
order condition to 0 and we solve
<span class="math display">\[\beta=\arg\min_{b\in\mathbb{R}^{K}}E\left[\left(y-x&#39;b\right)^{2}\right]\]</span>
in the closed-form
<span class="math display">\[\beta=\left(E\left[xx&#39;\right]\right)^{-1}E\left[xy\right]\]</span> if
<span class="math inline">\(E\left[xx&#39;\right]\)</span> is invertible. Notice here that <span class="math inline">\(b\)</span> is an arbitrary
<span class="math inline">\(K\)</span>-vector, while <span class="math inline">\(\beta\)</span> is the optimizer. The function <span class="math inline">\(x&#39;\beta\)</span> is
called the <em>best linear projection</em> (BLP) of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>, and the vector
<span class="math inline">\(\beta\)</span> is called the <em>linear projection coefficient</em>.</p>
<p>The linear function is not as restrictive as one might thought. It can
be used to produce some nonlinear (in random variables) effect if we
re-define <span class="math inline">\(x\)</span>. For example, if
<span class="math display">\[y=x_{1}\beta_{1}+x_{2}\beta_{2}+x_{1}^{2}\beta_{3}+e,\]</span> then
<span class="math inline">\(\frac{\partial}{\partial x_{1}}m\left(x_{1},x_{2}\right)=\beta_{1}+2x_{1}\beta_{3}\)</span>,
which is nonlinear in <span class="math inline">\(x_{1}\)</span>, while it is still linear in the parameter
<span class="math inline">\(\beta=\left(\beta_{1},\beta_{2},\beta_{3}\right)\)</span> if we define a set of
new regressors as
<span class="math inline">\(\left(\tilde{x}_{1},\tilde{x}_{2},\tilde{x}_{3}\right)=\left(x_{1},x_{2},x_{1}^{2}\right)\)</span>.</p>
<p>If <span class="math inline">\(\left(y,x\right)\)</span> is jointly normal in the form <span class="math display">\[\begin{pmatrix}y\\
x
\end{pmatrix}\sim\mathrm{N}\left(\begin{pmatrix}\mu_{y}\\
\mu_{x}
\end{pmatrix},\begin{pmatrix}\sigma_{y}^{2} &amp; \rho\sigma_{y}\sigma_{x}\\
\rho\sigma_{y}\sigma_{x} &amp; \sigma_{x}^{2}
\end{pmatrix}\right)\]</span> where <span class="math inline">\(\rho\)</span> is the correlation coefficient, then
<span class="math display">\[E\left[y|x\right]=\mu_{y}+\rho\frac{\sigma_{y}}{\sigma_{x}}\left(x-\mu_{x}\right)=\left(\mu_{y}-\rho\frac{\sigma_{y}}{\sigma_{x}}\mu_{x}\right)+\rho\frac{\sigma_{y}}{\sigma_{x}}x,\]</span>
is a liner function of <span class="math inline">\(x\)</span>. In this example, the CEF is linear.</p>
<p>Even though in general <span class="math inline">\(m\left(x\right)\neq x&#39;\beta\)</span>, the linear form
<span class="math inline">\(x&#39;\beta\)</span> is still useful in approximating <span class="math inline">\(m\left(x\right)\)</span>. That is,
<span class="math inline">\(\beta=\arg\min\limits _{b\in\mathbb{R}^{K}}E\left[\left(m(x)-x&#39;b\right)^{2}\right]\)</span>.</p>
<p>The first-order condition gives
<span class="math inline">\(\frac{\partial}{\partial b}E\left[\left(m(x)-x&#39;b\right)^{2}\right]=-2E[x(m(x)-x&#39;b)]=0\)</span>.
Rearrange the terms and obtain <span class="math inline">\(E[x\cdot m(x)]=E[xx&#39;]b\)</span>. When <span class="math inline">\(E[xx&#39;]\)</span>
is invertible, we solve
<span class="math display">\[\left(E\left[xx&#39;\right]\right){}^{-1}E[x\cdot m(x)]=\left(E\left[xx&#39;\right]\right){}^{-1}E[E[xy|x]]=\left(E\left[xx&#39;\right]\right){}^{-1}E[xy]=\beta.\]</span>
Thus <span class="math inline">\(\beta\)</span> is also the best linear approximation to <span class="math inline">\(m\left(x\right)\)</span>
under MSE.</p>
<p>We may rewrite the linear regression model, or the <em>linear projection
model,</em> as <span class="math display">\[\begin{array}[t]{c}
y=x&#39;\beta+e\\
E[xe]=0,
\end{array}\]</span> where <span class="math inline">\(e=y-x&#39;\beta\)</span> is called the <em>linear projection
error</em>, to be distinguished from <span class="math inline">\(\epsilon=y-m(x).\)</span></p>
<p>Show (a) <span class="math inline">\(E\left[xe\right]=0\)</span>. (b) If <span class="math inline">\(x\)</span> contains a constant, then
<span class="math inline">\(E\left[e\right]=0\)</span>.</p>
<div id="omitted-variable-bias" class="section level3" number="0.2.1">
<h3><span class="header-section-number">0.2.1</span> Omitted Variable Bias</h3>
<p>We write the <em>long regression</em> as
<span class="math display">\[y=x_{1}&#39;\beta_{1}+x_{2}&#39;\beta_{2}+\beta_{3}+e_{\beta},\]</span> and the
<em>short regression</em> as <span class="math display">\[y=x_{1}&#39;\gamma_{1}+\gamma_{2}+e_{\gamma},\]</span>
where <span class="math inline">\(e_{\beta}\)</span> and <span class="math inline">\(e_{\gamma}\)</span> are the projection errors,
respectively. If <span class="math inline">\(\beta_{1}\)</span> in the long regression is the parameter of
interest, omitting <span class="math inline">\(x_{2}\)</span> as in the short regression will render
<em>omitted variable bias</em> (meaning <span class="math inline">\(\gamma_{1}\neq\beta_{1}\)</span>) unless
<span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span> are uncorrelated.</p>
<p>We first demean all the variables in the two regressions, which is
equivalent as if we project out the effect of the constant. The long
regression becomes
<span class="math display">\[\tilde{y}=\tilde{x}_{1}&#39;\beta_{1}+\tilde{x}_{2}&#39;\beta_{2}+\tilde{e}_{\beta},\]</span>
and the short regression becomes
<span class="math display">\[\tilde{y}=\tilde{x}_{1}&#39;\gamma_{1}+\tilde{e}_{\gamma},\]</span> where <em>tilde</em>
denotes the demeaned variable.</p>
<p>Show <span class="math inline">\(\tilde{e}_{\beta}=e_{\beta}\)</span> and <span class="math inline">\(\tilde{e}_{\gamma}=e_{\gamma}\)</span>.</p>
<p>After demeaning, the cross-moment equals to the covariance. The short
regression coefficient
<span class="math display">\[\begin{aligned}\gamma_{1} &amp; =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}&#39;\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{y}\right]\\
 &amp; =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}&#39;\right]\right)^{-1}E\left[\tilde{x}_{1}\left(\tilde{x}_{1}&#39;\beta_{1}+\tilde{x}_{2}&#39;\beta_{2}+\tilde{e}_{\beta}\right)\right]\\
 &amp; =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}&#39;\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{1}&#39;\right]\beta_{1}+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}&#39;\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}&#39;\right]\beta_{2}\\
 &amp; =\beta_{1}+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}&#39;\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}&#39;\right]\beta_{2},
\end{aligned}\]</span> where the third line holds as
<span class="math inline">\(E\left[\tilde{x}_{1}\tilde{e}_{\beta}\right]=0\)</span>. Therefore,
<span class="math inline">\(\gamma_{1}=\beta_{1}\)</span> if and only if
<span class="math inline">\(E\left[\tilde{x}_{1}\tilde{x}_{2}&#39;\right]\beta_{2}=0\)</span>, which demands
either <span class="math inline">\(E\left[\tilde{x}_{1}\tilde{x}_{2}&#39;\right]=0\)</span> or <span class="math inline">\(\beta_{2}=0\)</span>.</p>
<p>Show that
<span class="math inline">\(E\left[\left(y-x_{1}&#39;\beta_{1}-x_{2}&#39;\beta_{2}-\beta_{3}\right)^{2}\right]\leq E\left[\left(y-x_{1}&#39;\gamma_{1}-\gamma_{2}\right)^{2}\right]\)</span>.</p>
<p>Obviously we prefer to run the long regression to attain <span class="math inline">\(\beta_{1}\)</span> if
possible, for it is a more general model than the short regression and
achieves no larger variance in the projection error. However, sometimes
<span class="math inline">\(x_{2}\)</span> is unobservable so the long regression is unavailable. This
example of omitted variable bias is ubiquitous in applied econometrics.
Ideally we would like to directly observe some regressors but in reality
we do not have them at hand. We should be aware of the potential
consequence when the data are not as ideal as we have wished. When only
the short regression is available, in some cases we are able to sign the
bias, meaning that we can argue whether <span class="math inline">\(\gamma_{1}\)</span> is bigger or
smaller than <span class="math inline">\(\beta_{1}\)</span> based on our knowledge.</p>
</div>
</div>
<p style="text-align: center;">
<a href="0.1-conditional-expectation.html"><button class="btn btn-default">Previous</button></a>
<a href="0.3-causality.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
