<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Panel Data | Econ5121</title>
  <meta name="description" content="nothing" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Panel Data | Econ5121" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="nothing" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Panel Data | Econ5121" />
  
  <meta name="twitter:description" content="nothing" />
  

<meta name="author" content="Zhentao Shi" />


<meta name="date" content="2022-01-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hypothesis-testing.html"/>
<link rel="next" href="endogeneity.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="probability.html"><a href="probability.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="probability.html"><a href="probability.html#axiomatic-probability"><i class="fa fa-check"></i><b>1.2</b> Axiomatic Probability</a><ul>
<li class="chapter" data-level="1.2.1" data-path="probability.html"><a href="probability.html#probability-space"><i class="fa fa-check"></i><b>1.2.1</b> Probability Space</a></li>
<li class="chapter" data-level="1.2.2" data-path="probability.html"><a href="probability.html#random-variable"><i class="fa fa-check"></i><b>1.2.2</b> Random Variable</a></li>
<li class="chapter" data-level="1.2.3" data-path="probability.html"><a href="probability.html#distribution-function"><i class="fa fa-check"></i><b>1.2.3</b> Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability.html"><a href="probability.html#expected-value"><i class="fa fa-check"></i><b>1.3</b> Expected Value</a><ul>
<li class="chapter" data-level="1.3.1" data-path="probability.html"><a href="probability.html#integration"><i class="fa fa-check"></i><b>1.3.1</b> Integration</a></li>
<li class="chapter" data-level="1.3.2" data-path="probability.html"><a href="probability.html#properties-of-expectations"><i class="fa fa-check"></i><b>1.3.2</b> Properties of Expectations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability.html"><a href="probability.html#multivariate-random-variable"><i class="fa fa-check"></i><b>1.4</b> Multivariate Random Variable</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probability.html"><a href="probability.html#conditional-probability-and-bayes-theorem"><i class="fa fa-check"></i><b>1.4.1</b> Conditional Probability and Bayes’ Theorem</a></li>
<li class="chapter" data-level="1.4.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>1.4.2</b> Independence</a></li>
<li class="chapter" data-level="1.4.3" data-path="probability.html"><a href="probability.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>1.4.3</b> Law of Iterated Expectations</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>1.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="conditional-expectation.html"><a href="conditional-expectation.html"><i class="fa fa-check"></i><b>2</b> Conditional Expectation</a><ul>
<li class="chapter" data-level="2.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#linear-projection"><i class="fa fa-check"></i><b>2.1</b> Linear Projection</a><ul>
<li class="chapter" data-level="2.1.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#omitted-variable-bias"><i class="fa fa-check"></i><b>2.1.1</b> Omitted Variable Bias</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="conditional-expectation.html"><a href="conditional-expectation.html#causality"><i class="fa fa-check"></i><b>2.2</b> Causality</a><ul>
<li class="chapter" data-level="2.2.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#structure-and-identification"><i class="fa fa-check"></i><b>2.2.1</b> Structure and Identification</a></li>
<li class="chapter" data-level="2.2.2" data-path="conditional-expectation.html"><a href="conditional-expectation.html#treatment-effect"><i class="fa fa-check"></i><b>2.2.2</b> Treatment Effect</a></li>
<li class="chapter" data-level="2.2.3" data-path="conditional-expectation.html"><a href="conditional-expectation.html#ate-and-cef"><i class="fa fa-check"></i><b>2.2.3</b> ATE and CEF</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html"><i class="fa fa-check"></i><b>3</b> Least Squares: Linear Algebra</a><ul>
<li class="chapter" data-level="3.1" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html#estimator"><i class="fa fa-check"></i><b>3.1</b> Estimator</a></li>
<li class="chapter" data-level="3.2" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html#subvector"><i class="fa fa-check"></i><b>3.2</b> Subvector</a></li>
<li class="chapter" data-level="3.3" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html#goodness-of-fit"><i class="fa fa-check"></i><b>3.3</b> Goodness of Fit</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>3.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html"><i class="fa fa-check"></i><b>4</b> Least Squares: Finite Sample Theory</a><ul>
<li class="chapter" data-level="4.1" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#maximum-likelihood"><i class="fa fa-check"></i><b>4.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="4.2" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#likelihood-estimation-for-regression"><i class="fa fa-check"></i><b>4.2</b> Likelihood Estimation for Regression</a></li>
<li class="chapter" data-level="4.3" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#finite-sample-distribution"><i class="fa fa-check"></i><b>4.3</b> Finite Sample Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#mean-and-variancemean-and-variance"><i class="fa fa-check"></i><b>4.4</b> Mean and Variance<span id="mean-and-variance" label="mean-and-variance"><span class="math display">\[mean-and-variance\]</span></span></a></li>
<li class="chapter" data-level="4.5" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>4.5</b> Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="4.6" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
<li class="chapter" data-level="4.7" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#appendix"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#joint-normal-distribution"><i class="fa fa-check"></i><b>4.7.1</b> Joint Normal Distribution</a></li>
<li class="chapter" data-level="4.7.2" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#basus-theorem-subsecbasus-theoremsubsecbasus-theorem-labelsubsecbasus-theorem"><i class="fa fa-check"></i><b>4.7.2</b> Basu’s Theorem* [<span class="math display">\[subsec:Basu\&#39;s-Theorem\]</span>]{#subsec:Basu’s-Theorem label=“subsec:Basu’s-Theorem”}</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html"><i class="fa fa-check"></i><b>5</b> Basic Asymptotic Theory</a><ul>
<li class="chapter" data-level="5.1" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#modes-of-convergence"><i class="fa fa-check"></i><b>5.1</b> Modes of Convergence</a></li>
<li class="chapter" data-level="5.2" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#law-of-large-numbers"><i class="fa fa-check"></i><b>5.2</b> Law of Large Numbers</a><ul>
<li class="chapter" data-level="5.2.1" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#cherbyshev-lln"><i class="fa fa-check"></i><b>5.2.1</b> Cherbyshev LLN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.3</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="5.4" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#tools-for-transformations"><i class="fa fa-check"></i><b>5.4</b> Tools for Transformations</a></li>
<li class="chapter" data-level="5.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html"><i class="fa fa-check"></i><b>6</b> Asymptotic Properties of Least Squares</a><ul>
<li class="chapter" data-level="6.1" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#consistency"><i class="fa fa-check"></i><b>6.1</b> Consistency</a></li>
<li class="chapter" data-level="6.2" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#asymptotic-distribution"><i class="fa fa-check"></i><b>6.2</b> Asymptotic Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#asymptotic-inference"><i class="fa fa-check"></i><b>6.3</b> Asymptotic Inference</a></li>
<li class="chapter" data-level="6.4" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#consistency-of-feasible-variance-estimator"><i class="fa fa-check"></i><b>6.4</b> Consistency of Feasible Variance Estimator</a><ul>
<li class="chapter" data-level="6.4.1" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#homoskedasticity"><i class="fa fa-check"></i><b>6.4.1</b> Homoskedasticity</a></li>
<li class="chapter" data-level="6.4.2" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#heteroskedasticity"><i class="fa fa-check"></i><b>6.4.2</b> Heteroskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#appendix"><i class="fa fa-check"></i><b>6.6</b> Appendix</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html"><i class="fa fa-check"></i><b>7</b> Asymptotic Properties of MLE</a><ul>
<li class="chapter" data-level="7.1" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#examples-of-mle"><i class="fa fa-check"></i><b>7.1</b> Examples of MLE</a></li>
<li class="chapter" data-level="7.2" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#consistency"><i class="fa fa-check"></i><b>7.2</b> Consistency</a></li>
<li class="chapter" data-level="7.3" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#asymptotic-normality"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="7.4" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#information-matrix-equality"><i class="fa fa-check"></i><b>7.4</b> Information Matrix Equality</a></li>
<li class="chapter" data-level="7.5" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#cramer-rao-lower-bound"><i class="fa fa-check"></i><b>7.5</b> Cramer-Rao Lower Bound</a></li>
<li class="chapter" data-level="7.6" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>8</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="8.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#testing"><i class="fa fa-check"></i><b>8.1</b> Testing</a><ul>
<li class="chapter" data-level="8.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#decision-rule-and-errors"><i class="fa fa-check"></i><b>8.1.1</b> Decision Rule and Errors</a></li>
<li class="chapter" data-level="8.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#optimality"><i class="fa fa-check"></i><b>8.1.2</b> Optimality</a></li>
<li class="chapter" data-level="8.1.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#likelihood-ratio-test-and-wilks-theorem"><i class="fa fa-check"></i><b>8.1.3</b> Likelihood-Ratio Test and Wilks’ theorem</a></li>
<li class="chapter" data-level="8.1.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#score-test"><i class="fa fa-check"></i><b>8.1.4</b> Score Test</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#confidence-intervalconfidence-interval"><i class="fa fa-check"></i><b>8.2</b> Confidence Interval<span id="confidence-interval" label="confidence-interval"><span class="math display">\[confidence-interval\]</span></span></a></li>
<li class="chapter" data-level="8.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#bayesian-credible-set"><i class="fa fa-check"></i><b>8.3</b> Bayesian Credible Set</a></li>
<li class="chapter" data-level="8.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#applications-in-ols"><i class="fa fa-check"></i><b>8.4</b> Applications in OLS</a><ul>
<li class="chapter" data-level="8.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wald-test"><i class="fa fa-check"></i><b>8.4.1</b> Wald Test</a></li>
<li class="chapter" data-level="8.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#lagrangian-multiplier-test"><i class="fa fa-check"></i><b>8.4.2</b> Lagrangian Multiplier Test</a></li>
<li class="chapter" data-level="8.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#likelihood-ratio-test-for-regression"><i class="fa fa-check"></i><b>8.4.3</b> Likelihood-Ratio Test for Regression</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#appendix"><i class="fa fa-check"></i><b>8.6</b> Appendix</a><ul>
<li class="chapter" data-level="8.6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#neyman-pearson-lemma"><i class="fa fa-check"></i><b>8.6.1</b> Neyman-Pearson Lemma</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>9</b> Panel Data</a><ul>
<li class="chapter" data-level="9.1" data-path="panel-data.html"><a href="panel-data.html#fixed-effect"><i class="fa fa-check"></i><b>9.1</b> Fixed Effect</a></li>
<li class="chapter" data-level="9.2" data-path="panel-data.html"><a href="panel-data.html#random-effect"><i class="fa fa-check"></i><b>9.2</b> Random Effect</a></li>
<li class="chapter" data-level="9.3" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>10</b> Endogeneity</a><ul>
<li class="chapter" data-level="10.1" data-path="endogeneity.html"><a href="endogeneity.html#identification"><i class="fa fa-check"></i><b>10.1</b> Identification</a></li>
<li class="chapter" data-level="10.2" data-path="endogeneity.html"><a href="endogeneity.html#instruments"><i class="fa fa-check"></i><b>10.2</b> Instruments</a></li>
<li class="chapter" data-level="10.3" data-path="endogeneity.html"><a href="endogeneity.html#sources-of-endogeneity"><i class="fa fa-check"></i><b>10.3</b> Sources of Endogeneity</a></li>
<li class="chapter" data-level="10.4" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html"><i class="fa fa-check"></i><b>11</b> Generalized Method of Moments</a><ul>
<li class="chapter" data-level="11.1" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#instrumental-regression"><i class="fa fa-check"></i><b>11.1</b> Instrumental Regression</a><ul>
<li class="chapter" data-level="11.1.1" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#just-identification"><i class="fa fa-check"></i><b>11.1.1</b> Just-identification</a></li>
<li class="chapter" data-level="11.1.2" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#over-identification"><i class="fa fa-check"></i><b>11.1.2</b> Over-identification</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#gmm-estimator"><i class="fa fa-check"></i><b>11.2</b> GMM Estimator</a><ul>
<li class="chapter" data-level="11.2.1" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#efficient-gmm"><i class="fa fa-check"></i><b>11.2.1</b> Efficient GMM</a></li>
<li class="chapter" data-level="11.2.2" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#two-step-gmm"><i class="fa fa-check"></i><b>11.2.2</b> Two-Step GMM</a></li>
<li class="chapter" data-level="11.2.3" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#two-stage-least-squares"><i class="fa fa-check"></i><b>11.2.3</b> Two Stage Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#gmm-in-nonlinear-model"><i class="fa fa-check"></i><b>11.3</b> GMM in Nonlinear Model</a></li>
<li class="chapter" data-level="11.4" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econ5121</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="panel-data" class="section level1">
<h1><span class="header-section-number">9</span> Panel Data</h1>
<p>Economists mostly work with observational data. The data generation
process is out of the control of researchers. If we only have a cross
sectional dataset at hand, it is difficult to deal with heterogeneity
among the individuals. On the other hand, panel data offers a chance to
handle heterogeneity of some particular forms.</p>
<p>A panel dataset tracks the same individuals across time <span class="math inline">\(t=1,\ldots,T\)</span>.
We assume the observations are independent across <span class="math inline">\(i=1,\ldots,n\)</span>, while
we allow some form of dependence within a group across <span class="math inline">\(t=1,\ldots,T\)</span>
for the same <span class="math inline">\(i\)</span>. We maintain the linear equation
<span class="math display">\[y_{it}=\beta_{1}+x_{it}&#39;\beta_{2}+u_{it},\ i=1,\ldots,n;t=1,\ldots,T\label{eq:basic_eq}\]</span>
where <span class="math inline">\(u_{it}=\alpha_{i}+\epsilon_{it}\)</span> is called the <em>composite error</em>.
Note that <span class="math inline">\(\alpha_{i}\)</span> is the time-invariant unobserved heterogeneity,
while <span class="math inline">\(\epsilon_{it}\)</span> varies across individuals and time periods.</p>
<p>The most important techniques of panel data estimation are the fixed
effect regression and the random effect regression. The asymptotic
distributions of both estimators can be derived from knowledge about the
OLS regression. In this sense, panel data estimation becomes applied
examples of the theory that we have covered in this course. It
highlights the fundamental role of linear regression theory in
econometrics.</p>
<div id="fixed-effect" class="section level2">
<h2><span class="header-section-number">9.1</span> Fixed Effect</h2>
<p>The unobservable individual-specific heterogeneity <span class="math inline">\(\alpha_{i}\)</span> is
absorbed into the composite error <span class="math inline">\(u_{it}=\alpha_{i}+\epsilon_{it}\)</span>. If
<span class="math inline">\(\mathrm{cov}\left(\alpha_{i},x_{it}\right)=0\)</span>, the OLS is consistent;
otherwise the consistency breaks down. The fixed effect model allows
<span class="math inline">\(\alpha_{i}\)</span> and <span class="math inline">\(x_{it}\)</span> to be arbitrarily correlated. Let us rewrite
(<a href="#eq:basic_eq" reference-type="ref" reference="eq:basic_eq"><span class="math display">\[eq:basic\_eq\]</span></a>) as
<span class="math display">\[y_{it}=w_{it}&#39;\boldsymbol{\beta}+u_{it},\label{eq:basic_eq2}\]</span> where
<span class="math inline">\(\boldsymbol{\beta}=\left(\beta_{1},\beta_{2}&#39;\right)&#39;\)</span> and
<span class="math inline">\(w_{it}=\left(1,x_{it}&#39;\right)&#39;\)</span> are <span class="math inline">\(K+1\)</span> vectors, i.e.,
<span class="math inline">\(\boldsymbol{\beta}\)</span> is the parameter including the intercept, and
<span class="math inline">\(w_{it}\)</span> is the explanatory variables including the constant.</p>
<p>Show that <span class="math inline">\(\mathrm{cov}\left(\alpha_{i},x_{it}\right)\neq0\)</span>, running OLS
in (<a href="#eq:basic_eq2" reference-type="ref" reference="eq:basic_eq2"><span class="math display">\[eq:basic\_eq2\]</span></a>) will deliver an inconsistent estimator.</p>
<p>While naively run OLS in the fixed effect model is inconsistent, the
trick to regain consistency is to eliminate <span class="math inline">\(\alpha_{i}\)</span>. This section
develops the consistency and asymptotic distribution of the <em>within
estimator</em>, the default fixed-effect (FE) estimator. The within
estimator transforms the data by subtracting all the observable
variables by the corresponding group means. Averaging the <span class="math inline">\(T\)</span> equations
of the original regression for the same <span class="math inline">\(i\)</span>, we have
<span class="math display">\[\overline{y}_{i}=\beta_{1}+\overline{x}_{i}&#39;\beta_{2}+\bar{u}_{it}=\beta_{1}+\overline{x}_{i}&#39;\beta_{2}+\alpha_{i}+\bar{\epsilon}_{it}.\label{eq:group_mean}\]</span>
where <span class="math inline">\(\overline{y}_{i}=\frac{1}{T}\sum_{t=1}^{T}y_{it}\)</span>. Subtracting
the averaged equation from the original equation gives
<span class="math display">\[\tilde{y}_{it}=\tilde{x}_{it}&#39;\beta_{2}+\tilde{\epsilon}_{it}\label{eq:FE_demean}\]</span>
where <span class="math inline">\(\tilde{y}_{it}=y_{it}-\overline{y}_{i}\)</span>. We then run OLS with the
demeaned data, and obtain the within estimator
<span class="math display">\[\widehat{\beta}_{2}^{FE}=\left(\tilde{X}&#39;\tilde{X}\right)^{-1}\tilde{X}&#39;\tilde{y},\]</span>
where <span class="math inline">\(\tilde{y}=\left(y_{it}\right)_{i,t}\)</span> stacks all the <span class="math inline">\(nT\)</span>
observations into a vector, and similarly defined is <span class="math inline">\(\tilde{X}\)</span> as an
<span class="math inline">\(nT\times K\)</span> matrix, where <span class="math inline">\(K\)</span> is the dimension of <span class="math inline">\(\beta_{2}\)</span>.</p>
<p>We know that OLS would be consistent if
<span class="math inline">\(E\left[\tilde{\epsilon}_{it}|\tilde{x}_{it}\right]=0\)</span>. Below we provide
a sufficient condition, which is often called <em>strict exogeneity</em>.</p>
<p><strong>Assumption FE.1</strong>
<span class="math inline">\(E\left[\epsilon_{it}|\alpha_{i},\mathbf{x}_{i}\right]=0\)</span> where
<span class="math inline">\(\mathbf{x}_{i}=\left(x_{i1},\ldots,x_{iT}\right)\)</span>.</p>
<p>Its strictness is relative to the contemporary exogeneity
<span class="math inline">\(E\left[\epsilon_{it}|\alpha_{i},x_{it}\right]=0\)</span>. FE.1 is more
restrictive as it assumes that the error <span class="math inline">\(\epsilon_{it}\)</span> is mean
independent of the past, present and future explanatory variables.</p>
<p>When we talk about the consistency in panel data, typically we are
considering <span class="math inline">\(n\to\infty\)</span> while <span class="math inline">\(T\)</span> stays fixed. This asymptotic
framework is appropriate for panel datasets with many individuals but
only a few time periods.</p>
<p><strong>Proposition</strong> If FE.1 is satisfied, then <span class="math inline">\(\widehat{\beta}_{2}^{FE}\)</span> is
consistent.</p>
<p>The variance estimation for the FE estimator is a little bit tricky. We
assume a homoskedasitcity condition to simplify the calculation.
Violation of this assumption changes the form of the asymptotic
variance, but does not jeopardize the asymptotic normality.</p>
<p><strong>Assumption FE.2</strong>
<span class="math inline">\(\mathrm{var}\left(\epsilon_{i}|\alpha_{i},\mathbf{x}_{i}\right)=\sigma_{\epsilon}^{2}I_{T}\)</span>
for all <span class="math inline">\(i\)</span>.</p>
<p>Under FE.1 and FE.2,
<span class="math inline">\(\widehat{\sigma}_{\epsilon}^{2}=\frac{1}{n\left(T-1\right)}\sum_{i=1}^{n}\sum_{t=1}^{T}\widehat{\tilde{\epsilon}}_{it}^{2}\)</span>
is a consistent estimator of <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span>, where
<span class="math inline">\(\widehat{\tilde{\epsilon}}=\tilde{y}_{it}-\tilde{x}_{it}&#39;\widehat{\beta}_{2}^{FE}\)</span>.
Note that the denominator is <span class="math inline">\(n\left(T-1\right)\)</span>, not <span class="math inline">\(nT\)</span>. The
necessity of adjusting the degree of freedom can be easily seen from the
FWL theorem: the FE estimator for the slope coefficient is numerical the
same as its counterpart in the full regression with a dummy variable for
each cross sectional unit.</p>
<p>If FE.1 and FE.2 are satisfied, then
<span class="math display">\[\left(\widehat{\sigma}_{\epsilon}^{2}\left(\tilde{X}&#39;\tilde{X}\right)^{-1}\right)^{-1/2}\left(\widehat{\beta}_{2}^{FE}-\beta_{2}^{0}\right)\stackrel{d}{\to}N\left(0,I_{K}\right).\]</span></p>
<p>Let <span class="math inline">\(M_{\iota}=I_{T}-\frac{1}{T}\iota_{T}\iota_{T}&#39;\)</span> be the within-group
demeaner, and <span class="math inline">\(M=I_{n}\otimes M_{\iota}\)</span> (“<span class="math inline">\(\otimes\)</span>” denotes the
Kronecker product). The FE estimator can be explicitly written as
<span class="math display">\[\widehat{\beta}_{2}^{FE}=\left(\tilde{X}&#39;\tilde{X}\right)^{-1}\tilde{X}&#39;\tilde{Y}=\left(X&#39;MX\right)^{-1}X&#39;MY.\]</span>
So
<span class="math display">\[\sqrt{nT}\left(\widehat{\beta}_{2}^{FE}-\beta_{2}^{0}\right)=\left(\frac{X&#39;MX}{nT}\right)^{-1}\frac{X&#39;M\epsilon}{\sqrt{nT}}=\left(\frac{\tilde{X}&#39;\tilde{X}}{nT}\right)^{-1}\frac{\tilde{X}&#39;\epsilon}{\sqrt{nT}}\]</span>
Since <span class="math display">\[\begin{aligned}
\mathrm{var}\left(\frac{\tilde{X}&#39;\epsilon}{\sqrt{nT}}|X\right) &amp; =\frac{1}{nT}E\left(X&#39;M\epsilon\epsilon&#39;MX|X\right)=\frac{1}{nT}X&#39;ME\left(\epsilon\epsilon&#39;|X\right)MX=\left(\frac{\tilde{X}&#39;\tilde{X}}{nT}\right)\sigma^{2},\end{aligned}\]</span>
We apply a law of large numbers and conclude
<span class="math display">\[\left(\tilde{X}&#39;\tilde{X}\right)^{1/2}\left(\widehat{\beta}_{2}^{FE}-\beta_{2}^{0}\right)\stackrel{d}{\to}N\left(0,\sigma_{\epsilon}^{2}I_{K}\right).\]</span>
For simplicity, suppose we can direct observe <span class="math inline">\(\tilde{\epsilon}_{it}\)</span>.
Then <span class="math display">\[\begin{aligned}
\frac{1}{n\left(T-1\right)}E\left[\sum_{i=1}^{n}\sum_{t=1}^{T}\tilde{\epsilon}_{it}^{2}\right] &amp; =\frac{1}{n}\sum_{i=1}^{n}\frac{1}{T-1}E\left[\epsilon_{i}&#39;M_{\iota}\epsilon_{i}\right]\\
 &amp; =\frac{1}{n}\sum_{i=1}^{n}\frac{1}{T-1}\mathrm{tr}\left(E\left[M_{\iota}E\left[\epsilon_{i}\epsilon_{i}&#39;|\mathbf{x}_{i}\right]\right]\right)\\
 &amp; =\frac{\sigma_{\epsilon}^{2}}{n}\sum_{i=1}^{n}\frac{1}{T-1}\mathrm{tr}\left(M_{\iota}\right)=\sigma_{\epsilon}^{2}.\end{aligned}\]</span>
Although in reality we only observe <span class="math inline">\(\widehat{\tilde{\epsilon}}_{it}\)</span>,
we can show that the estimation error between
<span class="math inline">\(\widehat{\tilde{\epsilon}}_{it}\)</span> and <span class="math inline">\(\tilde{\epsilon}_{it}\)</span> is
negligible. Thus by the law of large numbers
<span class="math display">\[\widehat{\sigma}_{\epsilon}^{2}=\frac{1}{n\left(T-1\right)}\sum_{i=1}^{n}\sum_{t=1}^{T}\widehat{\tilde{\epsilon}}_{it}^{2}\stackrel{d}{\to}\frac{1}{n\left(T-1\right)}E\left[\sum_{i=1}^{n}\sum_{t=1}^{T}\tilde{\epsilon}_{it}^{2}\right]=\sigma_{\epsilon}^{2}\]</span>
is a consistent estimator of the variance. The stated conclusion
follows.</p>
<p>We implicitly assume regularity conditions that allow us to invoke a law
of large numbers and a central limit theorem. We ignore those technical
details here.</p>
<p>It is important to notice that the within-group demean in FE eliminates
all time-invariant explanatory variables, including the intercept.
Therefore from FE we cannot obtain the coefficient estimates of these
time-invariant variables.</p>
</div>
<div id="random-effect" class="section level2">
<h2><span class="header-section-number">9.2</span> Random Effect</h2>
<p>The random effect estimator pursues efficiency at a knife-edge special
case <span class="math inline">\(\mathrm{cov}\left(\alpha_{i},x_{it}\right)=0\)</span>. As mentioned above,
FE is consistent when <span class="math inline">\(\alpha_{i}\)</span> and <span class="math inline">\(x_{it}\)</span> are uncorrelated.
However, an inspection of the covariance matrix reveals that OLS is
inefficient.</p>
<p>The starting point is again the original model, while we assume</p>
<p><strong>Assumption RE.1</strong>
<span class="math inline">\(E\left[\epsilon_{it}|\alpha_{i},\mathbf{x}_{i}\right]=0\)</span> and
<span class="math inline">\(E\left[\alpha_{i}|\mathbf{x}_{i}\right]=0\)</span>.</p>
<p>RE.1 obviously implies <span class="math inline">\(\mathrm{cov}\left(\alpha_{i},x_{it}\right)=0\)</span>,
so
<span class="math display">\[S=\mathrm{var}\left(u_{i}|\mathbf{x}_{i}\right)=\sigma_{\alpha}^{2}\mathbf{1}_{T}\mathbf{1}_{T}&#39;+\sigma_{\epsilon}^{2}I_{T},\ \mbox{for all }i=1,\ldots,n.\]</span>
Because the covariance matrix is not a scalar multiplication of the
identity matrix, OLS is inefficient.</p>
<p>As mentioned before, FE estimation kills all time-invariant regressors.
In contrast, RE allows time-invariant explanatory variables. The
infeasible GLS estimator is
<span class="math display">\[\widehat{\boldsymbol{\beta}}_{\mathrm{infeasible}}^{RE}=\left(\sum_{i=1}^{n}\mathbf{w}_{i}&#39;S^{-1}\mathbf{w}_{i}\right)^{-1}\sum_{i=1}^{n}\mathbf{w}_{i}&#39;S^{-1}\mathbf{y}_{i}=\left(W&#39;\mathbf{S}^{-1}W\right)^{-1}W&#39;\mathbf{S}^{-1}y\]</span>
where <span class="math inline">\(\mathbf{S}=I_{T}\otimes S\)</span>. In practice, <span class="math inline">\(\sigma_{\alpha}^{2}\)</span>
and <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span> in <span class="math inline">\(S\)</span> are unknown, so we seek consistent
estimators. Again, we impose a simplifying assumption parallel to FE.2.</p>
<p><strong>Assumption RE.2</strong>
<span class="math inline">\(\mathrm{var}\left(\epsilon_{i}|\mathbf{x}_{i},\alpha_{i}\right)=\sigma_{\epsilon}^{2}I_{T}\)</span>
and
<span class="math inline">\(\mathrm{var}\left(\alpha_{i}|\mathbf{x}_{i}\right)=\sigma_{\alpha}^{2}.\)</span></p>
<p>Under this assumption, we can consistently estimate the variances from
the residuals
<span class="math inline">\(\widehat{u}_{it}=y_{it}-x_{it}&#39;\widehat{\boldsymbol{\beta}}^{RE}\)</span>. That
is
<span class="math display">\[\begin{aligned}\widehat{\sigma}_{u}^{2} &amp; =\frac{1}{nT}\sum_{i=1}^{n}\sum_{t=1}^{T}\widehat{u}_{it}^{2}\\
\widehat{\sigma}_{\alpha}^{2} &amp; =\frac{1}{n}\sum_{i=1}^{n}\frac{1}{T\left(T-1\right)}\sum_{t=1}^{T}\sum_{r=1}^{T}\sum_{r\neq t}\widehat{u}_{it}\widehat{u}_{ir}.
\end{aligned}\]</span> Given the estimated variance and covariance, we can
construct
<span class="math inline">\(\widehat{\mathbf{S}}=\left(\widehat{\sigma}_{u}^{2}-\widehat{\sigma}_{\epsilon}^{2}\right)\cdot I_{T}+\widehat{\sigma}_{\epsilon}^{2}\cdot\boldsymbol{1}_{T}\boldsymbol{1}_{T}&#39;\)</span>
and then follows the feasible GLS (FGLS)
<span class="math display">\[\widehat{\boldsymbol{\beta}}^{RE}=\left(W&#39;\mathbf{\widehat{S}}^{-1}W\right)^{-1}W&#39;\widehat{\mathbf{S}}^{-1}y\]</span></p>
<p>Show that if RE.1 and RE.2 are satisfied, then
<span class="math display">\[\left(\widehat{\sigma}_{u}^{2}\left(W&#39;\widehat{\mathbf{S}}^{-1}W\right)^{-1}\right)^{-1/2}\left(\widehat{\boldsymbol{\beta}}^{RE}-\boldsymbol{\beta}_{0}\right)\stackrel{d}{\to}N\left(0,I_{K+1}\right).\]</span></p>
<p>In econometrics practice, the FE estimator is more popular than the RE
estimator as the former is consistent in more general conditions.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">9.3</span> Summary</h2>
<p>The formula of the FE estimator or the RE estimators is not important
because the estimation and inference are automatically handled by
econometric packages. What is important is the conceptual difference of
FE and RE on their treatment of the unobservable individual
heterogeneity.</p>
<p>Panel data is the first generation of economic “big data”, as the number
of observations of a cross section is multiplied by the number of time
periods. It reflected econometrician’s pursuit of controlling
heterogeneity, so that the OLS estimate is more credible for causal
interpretation.</p>
<p><strong>Further reading</strong>: <span class="citation">Hsiao (<a href="#ref-hsiao2014analysis" role="doc-biblioref">2014</a>)</span> is a comprehensive monograph on
the topic of panel data. <span class="citation">Su, Shi, and Phillips (<a href="#ref-su2016identifying" role="doc-biblioref">2016</a>)</span> extends fixed effect models
to incorporate group heterogeneity.</p>

<p><code>Zhentao Shi. Nov 8, 2020.</code></p>


</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-hsiao2014analysis">
<p>Hsiao, Cheng. 2014. <em>Analysis of Panel Data</em>. 54. Cambridge University Press.</p>
</div>
<div id="ref-su2016identifying">
<p>Su, Liangjun, Zhentao Shi, and Peter CB Phillips. 2016. “Identifying Latent Structures in Panel Data.” <em>Econometrica</em> 84 (6): 2215–64.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="endogeneity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
