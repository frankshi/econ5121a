[
["index.html", "A Primer on Economic Data Science 1 Preface 1.1 Personal Reflection 1.2 Prerequisite 1.3 Structure 1.4 Packages", " A Primer on Economic Data Science Zhentao Shi 2022-01-19 1 Preface This course came into being after postgraduate students repeatedly requested for training in coding skills. Programming is an essential skill for researchers in economic analysis. I believe the best way to learn programming is via examples. These notes contain many executable examples that illustrate R usage and econometric computational ideas. to be developed: econometrics is interdisciplinary study involving economics, statistics, operational research, and computational science. Reading materials are for peruse. They are not too long. Usually written by experts in the particular topic and offers a big picture. Survey papers, or at least with a survey. Open-source attitude. Stand on the shoulder of giants. Do not tend to be comprehensive. Because many topics have excellent writings. We refer to them. Do not reinvent the wheel. 1.1 Personal Reflection Thirty years ago aspiring young econometricians picked up GAUSS. Twenty years ago the new blood began with MATLAB. R raised ten years ago when the time came to my generation. I have been using R since I started my postgraduate study in Peking University in 2005. R helps me with my daily research and teaching. There are other candidates in statistical programming, for example Matlab, Python, Julia and Fortran. Each language has its own pros and cons. R has many advantages. First, it inherits the standard program syntax from C. It is quick to learn for anyone with prior programming experience. Moreover, once you master R, it is easy to switch to other language, if not R, in your workplace in the future. Second, backed by a vast statistician community, R enjoys a large selection of packages, including the most recent ones. When they publish a paper, often times statisticians write and upload a companion R package to facilitate user adoption. Third, R is free. It was the primary reason that I chose it at the very beginning. In the era of cloud computing, an algorithm written in R is easier to share, test, and improve. Anyone can run R code on any platform, free of license and fee headache. R is not without limitations. For example, speed is a concern when running big and complex jobs. However, it will not be an issue in the problems that we will encounter in the first-year postgraduate study. Lastly, learning a language is a non-trivial investment of our precious time. It is much more important to master one language with fluency than to know several languages. R is not the only language available for computing. Why not Python? Python is a general purpose language, not a scientific computing language. We need to import external modules even for basic numerical operations. For example, I personally don’t like np.mean, np.var and np.log, and its index from 0. For basic matrix manipulation, the default behavior of numpy is different from R. Why not Julia? Julia is too young to have a big community. We would wait until it grows into more stable status. Moreover, the speed advantage does not help much in interactive usage in empirical research. Over the years, I have had a taste of both Python and Julia. In my opinion, R so far best suits our purpose of learning a computing language for statistics and econometric analysis. 1.2 Prerequisite For this course, please install R or Microsoft Open R. An fully functional integrated development environment (IDE) is also highly desirable. It makes programming user-friendly and enjoyable. RStudio is recommended. 1.3 Structure The book version can be partitioned into three parts: R, Econometrics, and Machine Learning. 1.4 Packages install.packages(c(&quot;plyr&quot;, &quot;foreach&quot;, &quot;doParallel&quot;, &quot;sampleSelection&quot;, &quot;AER&quot;, &quot;mcmc&quot;, &quot;randomForest&quot;)) "],
["probability.html", "2 Probability 2.1 Introduction 2.2 Axiomatic Probability 2.3 Expected Value 2.4 Multivariate Random Variable 2.5 Summary", " 2 Probability For the convenience of online teaching in the fall semester of 2020, the layout is modified with wide margins and line space for note taking. 2.1 Introduction With the advent of big data, computer scientists have come up with a plethora of new algorithms that are aimed at revealing patterns from data. Machine learning and artificial intelligence become buzz words that attract public attention. They defeated best human Go players, automated manufacturers, powered self-driving vehicles, recognized human faces, and recommended online purchases. Some of these industrial successes are based on statistical theory, and statistical theory is based on probability theory. Although this probabilistic approach is not the only perspective to understand the behavior of machine learning and artificial intelligence, it offers one of the most promising paradigms to rationalize existing algorithms and engineer new ones. Economics has been an empirical social science since Adam Smith (1723–1790). Many numerical observations and anecdotes were scattered in his Wealth of Nations published in 1776. Ragnar Frisch (1895–1973) and Jan Tinbergen (1903--1994), two pioneer econometricians, were awarded in 1969 the first Nobel Prize in economics. Econometrics provides quantitative insights about economic data. It flourishes in real-world management practices, from households and firms up to governance at the global level. Today, the big data revolution is pumping fresh energy into research and exercises of econometric methods. The mathematical foundation of econometric theory is built on probability theory as well. 2.2 Axiomatic Probability Human beings are awed by uncertainty in daily life. In the old days, Egyptians consulted oracles, Hebrews inquired prophets, and Chinese counted on diviners to interpret tortoise shell or bone cracks. Fortunetellers are abundant in today’s Hong Kong. Probability theory is a philosophy about uncertainty. Over centuries, mathematicians strove to contribute to the understanding of randomness. As measure theory matured in the early 20th century, Andrey Kolmogorov (1903-1987) built the edifice of modern probability theory in his monograph published in 1933. The formal mathematical language is a system that allows rigorous explorations which have made fruitful advancements, and is now widely accepted in academic and industrial research. In this lecture, we will briefly introduce the axiomatic probability theory along with familiar results covered in undergraduate probability and statistics. This lecture note is at the level Hansen (2020): Introduction to Econometrics, or Stachurski (2016): A Primer in Econometric Theory, or Casella and Berger (2002): Statistical Inference (second edition) Interested readers may want to read this textbook for more examples. 2.2.1 Probability Space A sample space \\(\\Omega\\) is a collection of all possible outcomes. It is a set of things. An event \\(A\\) is a subset of \\(\\Omega\\). It is something of interest on the sample space. A \\(\\sigma\\)-field, denoted by \\(\\mathcal{F}\\), is a collection of events such that \\(\\emptyset\\in\\mathcal{F}\\); if an event \\(A\\in\\mathcal{F}\\), then \\(A^{c}\\in\\mathcal{F}\\); if \\(A_{i}\\in\\mathcal{F}\\) for \\(i\\in\\mathbb{N}\\), then \\(\\bigcup_{i\\in\\mathbb{N}}A_{i}\\in\\mathcal{F}\\). Implications: (a) Since \\(\\Omega=\\emptyset^{c}\\in\\mathcal{F}\\), we have \\(\\Omega\\in\\mathcal{F}\\). (b) If \\(A_{i}\\in\\mathcal{F}\\) for \\(i\\in\\mathbb{N}\\), then \\(A_{i}^{c}\\in\\mathcal{F}\\) for \\(i\\in\\mathbb{N}\\). Thus, if \\(\\bigcup_{i\\in\\mathbb{N}}A_{i}^{c}\\in\\mathcal{F}\\) , then \\(\\bigcap_{i\\in\\mathbb{N}}A_{i}=(\\bigcup_{i\\in\\mathbb{N}}A_{i}^{c})^{c}\\in\\mathcal{F}\\). 1.1*. Intuitively, a \\(\\sigma\\)-field is a pool which is closed for countable sets to conduct union, difference, and intersection operations. These are algebraic operations of sets. \\(\\sigma\\)-field is also called \\(\\sigma\\)-algebra. ** 1.1**. Let \\(\\Omega=\\{1,2,3,4,5,6\\}\\). Some examples of \\(\\sigma\\)-fields include \\(\\mathcal{F}_{1}=\\{\\emptyset,\\{1,2,3\\},\\{4,5,6\\},\\Omega\\}\\); \\(\\mathcal{F}_{2}=\\{\\emptyset,\\{1,3\\},\\{2,4,5,6\\},\\Omega\\}\\). Counterexample: \\(\\mathcal{F}_{3}=\\{\\emptyset,\\{1,2\\},\\{4,6\\},\\Omega\\}\\) is not a \\(\\sigma\\)-field since \\(\\{1,2,4,6\\}=\\{1,2\\}\\bigcup\\{4,6\\}\\) does not belong to \\(\\mathcal{F}_{3}\\). The \\(\\sigma\\)-field can be viewed as a well-organized structure built on the ground of the sample space. The pair \\(\\left(\\Omega,\\mathcal{F}\\right)\\) is called a measure space. Let \\(\\mathcal{G}=\\{B_{1},B_{2},\\ldots\\}\\) be an arbitrary collection of sets, not necessarily a \\(\\sigma\\)-field. We say \\(\\mathcal{F}\\) is the smallest \\(\\sigma\\)-field generated by \\(\\mathcal{G}\\) if \\(\\mathcal{G}\\subseteq\\mathcal{F}\\), and \\(\\mathcal{F}\\subseteq\\mathcal{\\tilde{F}}\\) for any \\(\\mathcal{\\tilde{F}}\\) such that \\(\\mathcal{G}\\subseteq\\mathcal{\\tilde{F}}\\). A Borel \\(\\sigma\\)-field \\(\\mathcal{R}\\) is the smallest \\(\\sigma\\)-field generated by the open sets on the real line \\(\\mathbb{R}\\). ** 1.2**. Let \\(\\Omega=\\{1,2,3,4,5,6\\}\\) and \\(A=\\{\\{1\\},\\{1,3\\}\\}\\). Then the smallest \\(\\sigma\\)-field generated by \\(A\\) is \\[\\sigma(A)=\\{\\emptyset,\\{1\\},\\{1,3\\},\\{3\\},\\{2,4,5,6\\},\\{2,3,4,5,6\\},\\{1,2,4,5,6\\},\\Omega\\}.\\] A function \\(\\mu:(\\Omega,\\mathcal{F})\\mapsto\\left[0,\\infty\\right]\\) is called a measure if it satisfies (positiveness) \\(\\mu\\left(A\\right)\\geq0\\) for all \\(A\\in\\mathcal{F}\\); (countable additivity) if \\(A_{i}\\in\\mathcal{F}\\), \\(i\\in\\mathbb{N}\\), are mutually disjoint, then \\[\\mu\\left(\\bigcup_{i\\in\\mathbb{N}}A_{i}\\right)=\\sum_{i\\in\\mathbb{N}}\\mu\\left(A_{i}\\right).\\] Measure can be understand as weight or length. In particular, we call \\(\\mu\\) a probability measure if \\(\\mu\\left(\\Omega\\right)=1\\). A probability measure is often denoted as \\(P\\). The triple \\(\\left(\\Omega,\\mathcal{F},P\\right)\\) is called a probability space. So far we have answered the question: “What is a mathematically well-defined probability?”, but we have not yet answered “How to assign the probability?” There are two major schools of thinking on probability assignment. One is frequentist, who considers probability as the average chance of occurrence if a large number of experiments are carried out. The other is Bayesian, who deems probability as a subjective brief. The principles of these two schools are largely incompatible, while each school has merits and difficulties, which will be elaborated when discussing hypothesis testing. 2.2.2 Random Variable The terminology random variable is a historic relic which belies its modern definition of a deterministic mapping. It is a link between two measurable spaces such that any event in the \\(\\sigma\\)-field installed on the range can be traced back to an event in the \\(\\sigma\\)-field installed on the domain. Formally, a function \\(X:\\Omega\\mapsto\\mathbb{R}\\) is \\(\\left(\\Omega,\\mathcal{F}\\right)\\backslash\\left(\\mathbb{R},\\mathcal{R}\\right)\\) measurable if \\[X^{-1}\\left(B\\right)=\\left\\{ \\omega\\in\\Omega:X\\left(\\omega\\right)\\in B\\right\\} \\in\\mathcal{F}\\] for any \\(B\\in\\mathcal{R}.\\) Random variable is an alternative, and somewhat romantic, name for a measurable function. The \\(\\sigma\\)-field generated by the random variable \\(X\\) is defined as \\(\\sigma\\left(X\\right)=\\left\\{ X^{-1}\\left(B\\right):B\\in\\mathcal{R}\\right\\}\\). We say a measurable is a discrete random variable if the set \\(\\left\\{ X\\left(\\omega\\right):\\omega\\in\\Omega\\right\\}\\) is finite or countable. We say it is a continuous random variable if the set \\(\\left\\{ X\\left(\\omega\\right):\\omega\\in\\Omega\\right\\}\\) is uncountable. A measurable function connects two measurable spaces. No probability is involved in its definition yet. While if a probability measure \\(P\\) is installed on \\((\\Omega,\\mathcal{F})\\), the measurable function \\(X\\) will induce a probability measure on \\((\\mathbb{R},\\mathcal{R})\\). It is easy to verify that \\(P_{X}:(\\mathbb{R},\\mathcal{R})\\mapsto\\left[0,1\\right]\\) is also a probability measure if defined as \\[P_{X}\\left(B\\right)=P\\left(X^{-1}\\left(B\\right)\\right)\\] for any \\(B\\in\\mathcal{R}\\). This \\(P_{X}\\) is called the probability measure induced by the measurable function \\(X\\). The induced probability measure \\(P_{X}\\) is an offspring of the parent probability measure \\(P\\) though the channel of \\(X\\). 2.2.3 Distribution Function We go back to some terms that we have learned in a undergraduate probability course. A (cumulative) distribution function \\(F:\\mathbb{R}\\mapsto[0,1]\\) is defined as \\[F\\left(x\\right)=P\\left(X\\leq x\\right)=P\\left(\\{X\\leq x\\}\\right)=P\\left(\\left\\{ \\omega\\in\\Omega:X\\left(\\omega\\right)\\leq x\\right\\} \\right).\\] It is often abbreviated as CDF, and it has the following properties. \\(\\lim_{x\\to-\\infty}F\\left(x\\right)=0\\), \\(\\lim_{x\\to\\infty}F\\left(x\\right)=1\\), non-decreasing, right-continuous \\(\\lim_{y\\to x^{+}}F\\left(y\\right)=F\\left(x\\right).\\) ** 1.1**. Draw the CDF of a binary distribution; that is, \\(X=1\\) with probability \\(p\\in\\left(0,1\\right)\\) and \\(X=0\\) with probability \\(1-p\\). For continuous distribution, if there exists a function \\(f\\) such that for all \\(x\\), \\[F\\left(x\\right)=\\int_{-\\infty}^{x}f\\left(y\\right)\\mathrm{d}y,\\] then \\(f\\) is called the probability density function of \\(X\\), often abbreviated as PDF. It is easy to show that \\(f\\left(x\\right)\\geq0\\) and \\(\\int_{a}^{b}f\\left(x\\right)dx=F\\left(b\\right)-F\\left(a\\right)\\). ** 1.3**. We have learned many parametric distributions like the binary distribution, the Poisson distribution, the uniform distribution, the exponential distribution, the normal distribution, \\(\\chi^{2}\\), \\(t\\), \\(F\\) distributions and so on. They are parametric distributions, meaning that the CDF or PDF can be completely characterized by very few parameters. 2.3 Expected Value 2.3.1 Integration Integration is one of the most fundamental operations in mathematical analysis. We have studied Riemann’s integral in the undergraduate calculus. Riemann’s integral is intuitive, but Lebesgue integral is a more general approach to defining integration. Lebesgue integral is constructed by the following steps. \\(X\\) is called a simple function on a measurable space \\(\\left(\\Omega,\\mathcal{F}\\right)\\) if \\(X=\\sum_{i}a_{i}\\cdot1\\left\\{ A_{i}\\right\\}\\) and this summation is finite, where \\(a_{i}\\in\\mathbb{R}\\) and \\(\\{A_{i}\\in\\mathcal{F}\\}_{i\\in\\mathbb{N}}\\) is a partition of \\(\\Omega\\). A simple function is measurable. Let \\(\\left(\\Omega,\\mathcal{F},\\mu\\right)\\) be a measure space. The integral of the simple function \\(X\\) with respect to \\(\\mu\\) is \\[\\int X\\mathrm{d}\\mu=\\sum_{i}a_{i}\\mu\\left(A_{i}\\right).\\] Unlike the Rieman integral, this definition of integration does not partition the domain into splines of equal length. Instead, it tracks the distinctive values of the function and the corresponding measure. Let \\(X\\) be a non-negative measurable function. The integral of \\(X\\) with respect to \\(\\mu\\) is \\[\\int X\\mathrm{d}\\mu=\\sup\\left\\{ \\int Y\\mathrm{d}\\mu:0\\leq Y\\leq X,\\text{ }Y\\text{ is simple}\\right\\} .\\] Let \\(X\\) be a measurable function. Define \\(X^{+}=\\max\\left\\{ X,0\\right\\}\\) and \\(X^{-}=-\\min\\left\\{ X,0\\right\\}\\). Both \\(X^{+}\\) and \\(X^{-}\\) are non-negative functions. The integral of \\(X\\) with respect to \\(\\mu\\) is \\[\\int X\\mathrm{d}\\mu=\\int X^{+}\\mathrm{d}\\mu-\\int X^{-}\\mathrm{d}\\mu.\\] The Step 1 above defines the integral of a simple function. Step 2 defines the integral of a non-negative function as the approximation of steps functions from below. Step 3 defines the integral of a general function as the difference of the integral of two non-negative parts. 1.2*. The integrand that highlights the difference between the Lebesgue integral and Riemann integral is the Dirichelet function on the unit interval \\(1\\left\\{ x\\in\\mathbb{Q}\\cap[0,1]\\right\\}\\). It is not Riemann-integrable whereas its Lebesgue integral. is well defined and \\(\\int1\\left\\{ x\\in\\mathbb{Q}\\cap[0,1]\\right\\} dx=0\\). If the measure \\(\\mu\\) is a probability measure \\(P\\), then the integral \\(\\int X\\mathrm{d}P\\) is called the expected value, or expectation, of \\(X\\). We often use the notation \\(E\\left[X\\right]\\), instead of \\(\\int X\\mathrm{d}P\\), for convenience. Expectation provides the average of a random variable, despite that we cannot foresee the realization of a random variable in a particular trial (otherwise the study of uncertainty is trivial). In the frequentist’s view, the expectation is the average outcome if we carry out a large number of independent trials. If we know the probability mass function of a discrete random variable, its expectation is calculated as \\(E\\left[X\\right]=\\sum_{x}xP\\left(X=x\\right)\\), which is the integral of a simple function. If a continuous random variable has a PDF \\(f(x)\\), its expectation can be computed as \\(E\\left[X\\right]=\\int xf\\left(x\\right)\\mathrm{d}x\\). These two expressions are unified as \\(E[X]=\\int X\\mathrm{d}P\\) by the Lebesgue integral. 2.3.2 Properties of Expectations Here are some properties of mathematical expectations. The probability of an event \\(A\\) is the expectation of an indicator function. \\(E\\left[1\\left\\{ A\\right\\} \\right]=1\\times P(A)+0\\times P(A^{c})=P\\left(A\\right)\\). \\(E\\left[X^{r}\\right]\\) is call the \\(r\\)-moment of \\(X\\). The mean of a random variable is the first moment \\(\\mu=E\\left[X\\right]\\), and the second centered moment is called the variance \\(\\mathrm{var}\\left[X\\right]=E\\left[\\left(X-\\mu\\right)^{2}\\right]\\). The third centered moment \\(E\\left[\\left(X-\\mu\\right)^{3}\\right]\\), called skewness, is a measurement of the symmetry of a random variable, and the fourth centered moment \\(E\\left[\\left(X-\\mu\\right)^{4}\\right]\\), called kurtosis, is a measurement of the tail thickness. Moments do not always exist. For example, the mean of the Cauchy distribution does not exist, and the variance of the \\(t(2)\\) distribution does not exist. \\(E[\\cdot]\\) is a linear operation. If \\(\\phi(\\cdot)\\) is a linear function, then \\(E[\\phi(X)]=\\phi(E[X]).\\) Jensen’s inequality is an important fact. A function \\(\\varphi(\\cdot)\\) is convex if \\(\\varphi(ax_{1}+(1-a)x_{2})\\leq a\\varphi(x_{1})+(1-a)\\varphi(x_{2})\\) for all \\(x_{1},x_{2}\\) in the domain and \\(a\\in[0,1]\\). For instance, \\(x^{2}\\) is a convex function. Jensen’s inequality says that if \\(\\varphi\\left(\\cdot\\right)\\) is a convex function, then \\(\\varphi\\left(E\\left[X\\right]\\right)\\leq E\\left[\\varphi\\left(X\\right)\\right].\\) Markov inequality is another simple but important fact. If \\(E\\left[\\left|X\\right|^{r}\\right]\\) exists, then \\(P\\left(\\left|X\\right|&gt;\\epsilon\\right)\\leq E\\left[\\left|X\\right|^{r}\\right]/\\epsilon^{r}\\) for all \\(r\\geq1\\). Chebyshev inequality \\(P\\left(\\left|X\\right|&gt;\\epsilon\\right)\\leq E\\left[X^{2}\\right]/\\epsilon^{2}\\) is a special case of the Markov inequality when \\(r=2\\). The distribution of a random variable is completely characterized by its CDF or PDF. A moment is a function of the distribution. To back out the underlying distribution from moments, we need to know the moment-generating function (mgf) \\(M_{X}(t)=E[e^{tX}]\\) for \\(t\\in\\mathbb{R}\\) whenever the expectation exists. The \\(r\\)th moment can be computed from mgf as \\[E[X^{r}]=\\frac{\\mathrm{d}^{r}M_{X}(t)}{\\mathrm{d}t^{r}}\\big\\vert_{t=0}.\\] Just like moments, mgf does not always exist. 2.4 Multivariate Random Variable A bivariate random variable is a measurable function \\(X:\\Omega\\mapsto\\mathbb{R}^{2}\\), and more generally a multivariate random variable is a measurable function \\(X:\\Omega\\mapsto\\mathbb{R}^{n}\\). We can define the joint CDF as \\(F\\left(x_{1},\\ldots,x_{n}\\right)=P\\left(X_{1}\\leq x_{1},\\ldots,X_{n}\\leq x_{n}\\right)\\). Joint PDF is defined similarly. It is sufficient to introduce the joint distribution, conditional distribution and marginal distribution in the simple bivariate case, and these definitions can be extended to multivariate distributions. Suppose a bivariate random variable \\((X,Y)\\) has a joint density \\(f(\\cdot,\\cdot)\\). The conditional density can be roughly written as \\(f\\left(y|x\\right)=f\\left(x,y\\right)/f\\left(x\\right)\\) if we do not formally deal with the case \\(f(x)=0\\). The marginal density \\(f\\left(y\\right)=\\int f\\left(x,y\\right)dx\\) integrates out the coordinate that is not interested. 2.4.1 Conditional Probability and Bayes’ Theorem In a probability space \\((\\Omega,\\mathcal{F},P)\\), for two events \\(A_{1},A_{2}\\in\\mathcal{F}\\) the conditional probability is \\[P\\left(A_{1}|A_{2}\\right)=\\frac{P\\left(A_{1}A_{2}\\right)}{P\\left(A_{2}\\right)}\\] if \\(P(A_{2})&gt;0\\). In the definition of conditional probability, \\(A_{2}\\) plays the role of the outcome space so that \\(P(A_{1}A_{2})\\) is standardized by the total mass \\(P(A_{2})\\). If \\(P(A_{2})=0\\), the conditional probability can still be valid in some cases, but we need to introduce the dominance between two measures, which we do not elaborate here. Since \\(A_{1}\\) and \\(A_{2}\\) are symmetric, we also have \\(P(A_{1}A_{2})=P(A_{2}|A_{1})P(A_{1})\\). It implies \\[P(A_{1}|A_{2})=\\frac{P\\left(A_{2}|A_{1}\\right)P\\left(A_{1}\\right)}{P\\left(A_{2}\\right)}\\] This formula is the Bayes’ Theorem. 2.4.2 Independence We say two events \\(A_{1}\\) and \\(A_{2}\\) are independent if \\(P(A_{1}A_{2})=P(A_{1})P(A_{2})\\). If \\(P(A_{2})\\neq0\\), it is equivalent to \\(P(A_{1}|A_{2})=P(A_{1})\\). In words, knowing \\(A_{2}\\) does not change the probability of \\(A_{1}\\). Regarding the independence of two random variables, \\(X\\) and \\(Y\\) are independent if \\(P\\left(X\\in B_{1},Y\\in B_{2}\\right)=P\\left(X\\in B_{1}\\right)P\\left(Y\\in B_{2}\\right)\\) for any two Borel sets \\(B_{1}\\) and \\(B_{2}\\). If \\(X\\) and \\(Y\\) are independent, then \\(E[XY]=E[X]E[Y]\\). The expectation of their product is the product of their expectations. 2.4.3 Law of Iterated Expectations Given a probability space \\(\\left(\\Omega,\\mathcal{F},P\\right)\\), a sub \\(\\sigma\\)-algebra \\(\\mathcal{G}\\subseteq\\mathcal{F}\\) and a \\(\\mathcal{F}\\)-measurable function \\(Y\\) with \\(E\\left|Y\\right|&lt;\\infty\\), the conditional expectation \\(E\\left[Y|\\mathcal{G}\\right]\\) is defined as a \\(\\mathcal{G}\\)-measurable function such that \\[\\int_{A}Y\\mathrm{d}P=\\int_{A}E\\left[Y|\\mathcal{G}\\right]\\mathrm{d}P\\] for all \\(A\\in\\mathcal{G}\\). Here \\(\\mathcal{G}\\) is a coarse \\(\\sigma\\)-field and \\(\\mathcal{F}\\) is a finer \\(\\sigma\\)-field. Taking \\(A=\\Omega\\), we have \\(E\\left[Y\\right]=\\int Y\\mathrm{d}P=\\int E\\left[Y|\\mathcal{G}\\right]\\mathrm{d}P=E\\left[E\\left[Y|\\mathcal{G}\\right]\\right]\\). The law of iterated expectation \\[E\\left[Y\\right]=E\\left[E\\left[Y|\\mathcal{G}\\right]\\right]\\] is a trivial fact which follows this definition of the conditional expectation. In the bivariate case, if the conditional density exists, the conditional expectation can be computed as \\(E\\left[Y|X\\right]=\\int yf\\left(y|X\\right)\\mathrm{d}y\\), where the conditioning variable \\(E\\left[\\cdot|X\\right]=E\\left[\\cdot|\\sigma\\left(X\\right)\\right]\\) is a concise notation for the smallest \\(\\sigma\\)-field generated by \\(X\\). The law of iterated expectation implies \\(E\\left[E\\left[Y|X\\right]\\right]=E\\left[Y\\right]\\). Below are some properties of conditional expectations \\(E\\left[E\\left[Y|X_{1},X_{2}\\right]|X_{1}\\right]=E\\left[Y|X_{1}\\right];\\) \\(E\\left[E\\left[Y|X_{1}\\right]|X_{1},X_{2}\\right]=E\\left[Y|X_{1}\\right];\\) \\(E\\left[h\\left(X\\right)Y|X\\right]=h\\left(X\\right)E\\left[Y|X\\right].\\) 2.5 Summary If it is your first encounter of measure theory, the new definitions here may seem overwhelmingly abstract. A natural question is that: “I earned high grade in my undergraduate probability and statistics; do I really need the fancy mathematics in this lecture to do well in econometrics?” The answer is yes and no. No is in the sense that if you want to use econometric methods, instead of grasp the underlying theory, then the axiomatic probability does not add much to your weaponry. You can be an excellent economist or applied econometrician without knowing measure theoretic probability. Yes is in the sense that without measure theory, we cannot even formally define conditional expectation, which will be the subject of our next lecture and is a core concept of econometrics. Moreover, the pillars of asymptotic theory — law of large numbers and central limit theorem — can only be made accurate with this foundation. If you are aspired to work on econometric theory, you will meet and use measure theory so often in your future study and finally it becomes part of your muscle memory. In this course, we try to keep a balance manner. On the one hand, many econometrics topics can be presented with elementary mathematics. Whenever possible, econometrics should reach wider audience with a plain appearance, instead of intimidating people by arcane languages. On the other hand, we introduce these concepts in this lecture and will invoke them in the discussion of asymptotic theory later. Your investment in advanced mathematics will not be wasted in vain. Historical notes: Measure theory was established in the early 20th century by a constellation of French/German mathematicians, represented by Émile Borel, Henri Lebesgue, Johann Radon, etc. Generations of Russian mathematicians such as Andrey Markov and Andrey Kolmogorov made fundamental contributions in mathematizing seemingly abstract concepts of uncertainty and randomness. Their names are immortalized by the Borel set, the Lebesgue integral, the Radon measure, Markov chain, Kolmogorov’s zero–one law and many other terminologies named after them. Fascinating questions about probability attracted great economists. Francis Edgeworth (1845–1926) wrote extensively on probability and statistics. John Maynard Keynes (1883–1946) published A Treatise on Probability in 1921 which mixed probability and philosophy, although this piece of work was not as influential as his General Theory of Employment, Interest and Money in 1936 which later revolutionized economics. Today, the technology of collecting data and the processing data is unbelievably cheaper than that 100 years ago. Unfortunately, the cost of learning mathematics and developing mathematics has not been significantly lowered over one century. Only a small handful of talents, like you, enjoy the privilege and luxury to appreciate the ideas of these great minds. Further reading: Doob (1996) summarized the development of axiomatic probability in the first half of the 20th century. Zhentao Shi. Sep 12, 2020. References "],
["conditional-expectation.html", "3 Conditional Expectation 3.1 Linear Projection 3.2 Causality 3.3 Summary", " 3 Conditional Expectation Notation: In this note, \\(y\\) is a scale random variable, and \\(x=\\left(x_{1},\\ldots,x_{K}\\right)&#39;\\) is a \\(K\\times1\\) random vector. Throughout this course, a vector is a column vector, i.e. a one-column matrix. Machine learning is a big basket that contains the regression models. We motivate the conditional expectation model from the perspective of prediction. We view a regression as supervised learning. Supervised learning uses a function of \\(x\\), say, \\(g\\left(x\\right)\\), to predict \\(y\\). \\(x\\) cannot perfectly predict \\(y\\); otherwise their relationship is deterministic. The prediction error \\(y-g\\left(x\\right)\\) depends on the choice of \\(g\\). There are numerous possible choices of \\(g\\). Which one is the best? Notice that this question is not concerned about the underlying data generating process (DGP) of the joint distribution of \\(\\left(y,x\\right)\\). We want to find a general rule to achieve accurate prediction of \\(y\\) given \\(x\\), no matter how this pair of variables is generated. To answer this question, we need to decide a criterion to compare different \\(g\\). Such a criterion is called the loss function \\(L\\left(y,g\\left(x\\right)\\right)\\). A particularly convenient one is the quadratic loss, defined as \\[L\\left(y,g\\left(x\\right)\\right)=\\left(y-g\\left(x\\right)\\right)^{2}.\\] Since the data are random, \\(L\\left(y,g\\left(x\\right)\\right)\\) is also random. “Random” means uncertainty: sometimes this happens, and sometimes that happens. To get rid of the uncertainty, we average the loss function with respect to the joint distribution of \\(\\left(y,x\\right)\\) as \\(R\\left(y,g\\left(x\\right)\\right)=E\\left[L\\left(y,g\\left(x\\right)\\right)\\right]\\), which is called risk. Risk is a deterministic quality. For the quadratic loss function, the corresponding risk is \\[R\\left(y,g\\left(x\\right)\\right)=E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right],\\] is called the mean squared error (MSE). MSE is the most widely used risk measure, although there exist many alternative measures, for example the mean absolute error (MAE) \\(E\\left[\\left|y-g\\left(x\\right)\\right|\\right]\\). The popularity of MSE comes from its convenience for analysis in closed-form, which MAE does not enjoy due to its nondifferentiability. This is similar to the choice of utility functions in economics. There are only a few functional forms for the utility, for example CRRA, CARA, and so on. They are popular because they lead to close-form solutions that are easy to handle. Now our quest is narrowed to: What is the optimal choice of \\(g\\) if we minimize the MSE? \\[prop:CEF\\] The conditional mean function (CEF) \\(m\\left(x\\right)=E\\left[y|x\\right]=\\int yf\\left(y|x\\right)\\mathrm{d}y\\) minimizes MSE. Before we prove the above proposition, we first discuss some properties of the conditional mean function. Obviously \\[y=m\\left(x\\right)+\\left(y-m\\left(x\\right)\\right)=m\\left(x\\right)+\\epsilon,\\] where \\(\\epsilon:=y-m\\left(x\\right)\\) is called the regression error. This equation holds for \\(\\left(y,x\\right)\\) following any joint distribution, as long as \\(E\\left[y|x\\right]\\) exists. The error term \\(\\epsilon\\) satisfies these properties: \\(E\\left[\\epsilon|x\\right]=E\\left[y-m\\left(x\\right)|x\\right]=E\\left[y|x\\right]-m(x)=0\\), \\(E\\left[\\epsilon\\right]=E\\left[E\\left[\\epsilon|x\\right]\\right]=E\\left[0\\right]=0\\), For any function \\(h\\left(x\\right)\\), we have \\[E\\left[h\\left(x\\right)\\epsilon\\right]=E\\left[E\\left[h\\left(x\\right)\\epsilon|x\\right]\\right]=E\\left[h(x)E\\left[\\epsilon|x\\right]\\right]=0.\\label{eq:uncorr}\\] The last property implies that \\(\\epsilon\\) is uncorrelated with any function of \\(x\\). In particular, when \\(h\\) is the identity function \\(h\\left(x\\right)=x\\), we have \\(E\\left[x\\epsilon\\right]=\\mathrm{cov}\\left(x,\\epsilon\\right)=0\\). The optimality of the CEF can be confirmed by “guess-and-verify.” For an arbitrary \\(g\\left(x\\right)\\), the MSE can be decomposed into three terms \\[\\begin{aligned} &amp; &amp; E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right]\\\\ &amp; = &amp; E\\left[\\left(y-m(x)+m(x)-g(x)\\right)^{2}\\right]\\\\ &amp; = &amp; E\\left[\\left(y-m\\left(x\\right)\\right)^{2}\\right]+2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]+E\\left[\\left(m\\left(x\\right)-g\\left(x\\right)\\right)^{2}\\right].\\end{aligned}\\] The first term is irrelevant to \\(g\\left(x\\right)\\). The second term \\[\\begin{aligned}2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right] &amp; =2E\\left[\\epsilon\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]=0\\end{aligned}\\] by invoking (\\[eq:uncorr\\]) with \\(h\\left(x\\right)=m\\left(x\\right)-g\\left(x\\right)\\). The second term is again irrelevant of \\(g\\left(x\\right)\\). The third term, obviously, is minimized at \\(g\\left(x\\right)=m\\left(x\\right)\\). Our perspective so far deviates from many econometric textbooks that assume that the dependent variable \\(y\\) is generated as \\(g\\left(x\\right)+\\epsilon\\) for some unknown function \\(g\\left(\\cdot\\right)\\) and error term \\(\\epsilon\\) such that \\(E\\left[\\epsilon|x\\right]=0\\). Instead, we take a predictive approach regardless the DGP. What we observe are \\(y\\) and \\(x\\) and we are solely interested in seeking a function \\(g\\left(x\\right)\\) to predict \\(y\\) as accurately as possible under the MSE criterion. 3.1 Linear Projection The CEF \\(m(x)\\) is the function that minimizes the MSE. However, \\(m\\left(x\\right)=E\\left[y|x\\right]\\) is a complex function of \\(x\\), for it depends on the joint distribution of \\(\\left(y,x\\right)\\), which is mostly unknown in practice. Now let us make the prediction task even simpler. How about we minimize the MSE within all linear functions in the form of \\(h\\left(x\\right)=h\\left(x;b\\right)=x&#39;b\\) for \\(b\\in\\mathbb{R}^{K}\\)? The minimization problem is \\[\\min_{b\\in\\mathbb{R}^{K}}E\\left[\\left(y-x&#39;b\\right)^{2}\\right].\\label{eq:linear_MSE}\\] Take the first-order condition of the MSE \\[\\frac{\\partial}{\\partial b}E\\left[\\left(y-x&#39;b\\right)^{2}\\right]=E\\left[\\frac{\\partial}{\\partial b}\\left(y-x&#39;b\\right)^{2}\\right]=-2E\\left[x\\left(y-x&#39;b\\right)\\right],\\] where the first equality holds if \\(E\\left[\\left(y-x&#39;b\\right)^{2}\\right]&lt;\\infty\\) so that the expectation and partial differentiation is interchangeable, and the second equality hods by the chain rule and the linearity of expectation. Set the first order condition to 0 and we solve \\[\\beta=\\arg\\min_{b\\in\\mathbb{R}^{K}}E\\left[\\left(y-x&#39;b\\right)^{2}\\right]\\] in the closed-form \\[\\beta=\\left(E\\left[xx&#39;\\right]\\right)^{-1}E\\left[xy\\right]\\] if \\(E\\left[xx&#39;\\right]\\) is invertible. Notice here that \\(b\\) is an arbitrary \\(K\\)-vector, while \\(\\beta\\) is the optimizer. The function \\(x&#39;\\beta\\) is called the best linear projection (BLP) of \\(y\\) on \\(x\\), and the vector \\(\\beta\\) is called the linear projection coefficient. The linear function is not as restrictive as one might thought. It can be used to produce some nonlinear (in random variables) effect if we re-define \\(x\\). For example, if \\[y=x_{1}\\beta_{1}+x_{2}\\beta_{2}+x_{1}^{2}\\beta_{3}+e,\\] then \\(\\frac{\\partial}{\\partial x_{1}}m\\left(x_{1},x_{2}\\right)=\\beta_{1}+2x_{1}\\beta_{3}\\), which is nonlinear in \\(x_{1}\\), while it is still linear in the parameter \\(\\beta=\\left(\\beta_{1},\\beta_{2},\\beta_{3}\\right)\\) if we define a set of new regressors as \\(\\left(\\tilde{x}_{1},\\tilde{x}_{2},\\tilde{x}_{3}\\right)=\\left(x_{1},x_{2},x_{1}^{2}\\right)\\). If \\(\\left(y,x\\right)\\) is jointly normal in the form \\[\\begin{pmatrix}y\\\\ x \\end{pmatrix}\\sim\\mathrm{N}\\left(\\begin{pmatrix}\\mu_{y}\\\\ \\mu_{x} \\end{pmatrix},\\begin{pmatrix}\\sigma_{y}^{2} &amp; \\rho\\sigma_{y}\\sigma_{x}\\\\ \\rho\\sigma_{y}\\sigma_{x} &amp; \\sigma_{x}^{2} \\end{pmatrix}\\right)\\] where \\(\\rho\\) is the correlation coefficient, then \\[E\\left[y|x\\right]=\\mu_{y}+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\left(x-\\mu_{x}\\right)=\\left(\\mu_{y}-\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\mu_{x}\\right)+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}x,\\] is a liner function of \\(x\\). In this example, the CEF is linear. Even though in general \\(m\\left(x\\right)\\neq x&#39;\\beta\\), the linear form \\(x&#39;\\beta\\) is still useful in approximating \\(m\\left(x\\right)\\). That is, \\(\\beta=\\arg\\min\\limits _{b\\in\\mathbb{R}^{K}}E\\left[\\left(m(x)-x&#39;b\\right)^{2}\\right]\\). The first-order condition gives \\(\\frac{\\partial}{\\partial b}E\\left[\\left(m(x)-x&#39;b\\right)^{2}\\right]=-2E[x(m(x)-x&#39;b)]=0\\). Rearrange the terms and obtain \\(E[x\\cdot m(x)]=E[xx&#39;]b\\). When \\(E[xx&#39;]\\) is invertible, we solve \\[\\left(E\\left[xx&#39;\\right]\\right){}^{-1}E[x\\cdot m(x)]=\\left(E\\left[xx&#39;\\right]\\right){}^{-1}E[E[xy|x]]=\\left(E\\left[xx&#39;\\right]\\right){}^{-1}E[xy]=\\beta.\\] Thus \\(\\beta\\) is also the best linear approximation to \\(m\\left(x\\right)\\) under MSE. We may rewrite the linear regression model, or the linear projection model, as \\[\\begin{array}[t]{c} y=x&#39;\\beta+e\\\\ E[xe]=0, \\end{array}\\] where \\(e=y-x&#39;\\beta\\) is called the linear projection error, to be distinguished from \\(\\epsilon=y-m(x).\\) Show (a) \\(E\\left[xe\\right]=0\\). (b) If \\(x\\) contains a constant, then \\(E\\left[e\\right]=0\\). 3.1.1 Omitted Variable Bias We write the long regression as \\[y=x_{1}&#39;\\beta_{1}+x_{2}&#39;\\beta_{2}+\\beta_{3}+e_{\\beta},\\] and the short regression as \\[y=x_{1}&#39;\\gamma_{1}+\\gamma_{2}+e_{\\gamma},\\] where \\(e_{\\beta}\\) and \\(e_{\\gamma}\\) are the projection errors, respectively. If \\(\\beta_{1}\\) in the long regression is the parameter of interest, omitting \\(x_{2}\\) as in the short regression will render omitted variable bias (meaning \\(\\gamma_{1}\\neq\\beta_{1}\\)) unless \\(x_{1}\\) and \\(x_{2}\\) are uncorrelated. We first demean all the variables in the two regressions, which is equivalent as if we project out the effect of the constant. The long regression becomes \\[\\tilde{y}=\\tilde{x}_{1}&#39;\\beta_{1}+\\tilde{x}_{2}&#39;\\beta_{2}+\\tilde{e}_{\\beta},\\] and the short regression becomes \\[\\tilde{y}=\\tilde{x}_{1}&#39;\\gamma_{1}+\\tilde{e}_{\\gamma},\\] where tilde denotes the demeaned variable. Show \\(\\tilde{e}_{\\beta}=e_{\\beta}\\) and \\(\\tilde{e}_{\\gamma}=e_{\\gamma}\\). After demeaning, the cross-moment equals to the covariance. The short regression coefficient \\[\\begin{aligned}\\gamma_{1} &amp; =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{y}\\right]\\\\ &amp; =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\left(\\tilde{x}_{1}&#39;\\beta_{1}+\\tilde{x}_{2}&#39;\\beta_{2}+\\tilde{e}_{\\beta}\\right)\\right]\\\\ &amp; =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]\\beta_{2}\\\\ &amp; =\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]\\beta_{2}, \\end{aligned}\\] where the third line holds as \\(E\\left[\\tilde{x}_{1}\\tilde{e}_{\\beta}\\right]=0\\). Therefore, \\(\\gamma_{1}=\\beta_{1}\\) if and only if \\(E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]\\beta_{2}=0\\), which demands either \\(E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]=0\\) or \\(\\beta_{2}=0\\). Show that \\(E\\left[\\left(y-x_{1}&#39;\\beta_{1}-x_{2}&#39;\\beta_{2}-\\beta_{3}\\right)^{2}\\right]\\leq E\\left[\\left(y-x_{1}&#39;\\gamma_{1}-\\gamma_{2}\\right)^{2}\\right]\\). Obviously we prefer to run the long regression to attain \\(\\beta_{1}\\) if possible, for it is a more general model than the short regression and achieves no larger variance in the projection error. However, sometimes \\(x_{2}\\) is unobservable so the long regression is unavailable. This example of omitted variable bias is ubiquitous in applied econometrics. Ideally we would like to directly observe some regressors but in reality we do not have them at hand. We should be aware of the potential consequence when the data are not as ideal as we have wished. When only the short regression is available, in some cases we are able to sign the bias, meaning that we can argue whether \\(\\gamma_{1}\\) is bigger or smaller than \\(\\beta_{1}\\) based on our knowledge. 3.2 Causality 3.2.1 Structure and Identification Unlike physical laws such as Einstein’s mass–energy equivalence \\(E=mc^{2}\\) and Newton’s universal gravitation \\(F=Gm_{1}m_{2}/r^{2}\\), economic phenomena can rarely be summarized in such a minimalistic style. When using experiments to verify physical laws, scientists often manage to come up with smart design in which signal-to-noise ratio is so high that small disturbances are kept at a negligible level. On the contrary, economic laws do not fit a laboratory for experimentation. What is worse, the subjects in economic studies — human beings — are heterogeneous and with many features that are hard to control. People from distinctive cultural and family backgrounds respond to the same issue differently and researchers can do little to homogenize them. The signal-to-noise ratios in economic laws are often significantly lower than those of physical laws, mainly due to the lack of laboratory setting and the heterogeneous nature of the subjects. Educational return and the demand-supply system are two classical topics in econometrics. A person’s incomes is determined by too many random factors in the academic and career path that is impossible to exhaustively observe and control. The observable prices and quantities are outcomes of equilibrium so the demand and supply affect each other. Generations of thinkers have been debating the definitions of causality. In economics, an accepted definition is structural causality. Structural causality is a thought experiment. It assumes that there is a DGP that produces the observational data. If we can use data to recover the DGP or some features of the DGP, then we have learned causality or some implications of causality. A key issue to resolve before looking at the realized sample is identification. We say a model or DGP is identified if the each possible parameter of the model under consideration generates distinctive features of the observable data. A model is under-identified if more than one parameter in the model can generate exact the same features of the observable data. In other words, a model is under-identified if from the observable data we cannot trace back to a unique parameter in the model. A correctly specified model is the prerequisite for any discussion of identification. In reality, all models are wrong. Thus when talking about identification, we are indulged in an imaginary world. If in such a thought experiment we still cannot unique distinguish the true parameter of the data generating process, then identification fails. We cannot determine what is the true model no matter how large the sample is. 3.2.2 Treatment Effect We narrow down to the framework of the relationship between \\(y\\) and \\(x\\). One question of particular interest is treatment effect. The treatment effect is how much \\(y\\) will change if we change a variable of interest, say \\(d\\), by one unit while keeping all other variables (including the unobservable variables) the same. The Latin phrase ceteris paribus means “keep all other things constant.” During the 2020 covid-19 pandemic, Hong Kong’s unemployment rate rose to a high-level and consumption collapsed. In order to boost the economy, some Hong Kong residents were qualified in receiving 10,000 HKD cash allowance from the government. We are interested in learning how much does the 10,000 HKD allowance increase people’s consumption. For an individual, we imagine two parallel worlds: one with the cash allowance and one without. The difference of the consumption in the world with the allowance, denoted \\(Y\\left(1\\right)\\), and that in the world without the allowance, denoted \\(Y\\left(0\\right)\\), is the treatment effect of that particular person. This thought experiment is called the potential outcome framework. However, in reality one and only one scenario happens, which echoes the saying of ancient Greek philosopher Heraclitus (553 BC--475 BC) “You cannot step into the same river twice.” The individual treatment effect is not operational (operational means it can be computed from data at the population level), because one and only one outcome is realized. With many people available, we can define average treatment effect (ATE) as \\[ATE=E\\left[Y\\left(1\\right)-Y\\left(0\\right)\\right]=E\\left[Y\\left(1\\right)\\right]-E\\left[Y\\left(0\\right)\\right].\\] Notice that \\(E\\left[Y\\left(1\\right)\\right]\\) and \\(E\\left[Y\\left(0\\right)\\right]\\) are still not operational before we observe a companion variable \\[D=1\\left\\{ \\mbox{treatment received}\\right\\} .\\] Once each individual’s treatment status is observable, \\(E\\left[Y\\left(1\\right)|D=1\\right]\\) and \\(E\\left[Y\\left(0\\right)|D=0\\right]\\) are operational from the data. If the two potential outcomes \\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\) are independent of the assignment \\(D\\), then \\(E\\left[Y\\left(1\\right)\\right]=E\\left[Y\\left(1\\right)|D=1\\right]\\) and \\(E\\left[Y\\left(0\\right)\\right]=E\\left[Y\\left(0\\right)|D=0\\right]\\) so that ATE can be estimated from the data in an operational way as \\[ATE=E\\left[Y\\left(1\\right)|D=1\\right]-E\\left[Y\\left(0\\right)|D=0\\right].\\] Therefore, to evaluate ATE ideally we would like use a lottery to randomly decide that some people receive the treatment (treatment group, with \\(D=1\\)) and the others do not (control group, with \\(D=0\\)). When we have other control variables, we can also define a finer treatment effect conditional on \\(x\\): \\[ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|x\\right]-E\\left[Y\\left(0\\right)|x\\right].\\] ATE is the average effect in the population of individuals when we hypothetical give them the treatment, keeping all other factors \\(x\\) constant. If conditioning on \\(x\\), the treatment \\(D\\) is independent of \\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\), then ATE becomes operational: \\[ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|D=1,x\\right]-E\\left[Y\\left(0\\right)|D=0,x\\right]\\] The important condition \\(\\left(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\perp D\\right)|x\\) is called the conditional independence assumption (CIA). CIA is more plausible than full independence. Consider the example \\(Y\\left(1\\right)=x+u\\left(1\\right)\\), \\(Y\\left(0\\right)=x+u\\left(0\\right)\\) and \\(D=1\\left\\{ x+u_{d}\\geq0\\right\\}\\). If \\(\\left(\\left(u\\left(0\\right),u\\left(1\\right)\\right)\\perp u_{d}\\right)|x\\), then CIA is satisfied. Nevertheless \\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\) and \\(D\\) are statistically dependent, since \\(x\\) is involved in all random variables. 3.2.3 ATE and CEF In the previous section the treatment \\(D\\) is binary. Now we consider a continuous treatment \\(D\\). Suppose the DGP, or the structural model, is \\(Y=h\\left(D,x,u\\right)\\) where \\(D\\) and \\(x\\) are observable and \\(u\\) is unobservable. It is natural to define ATE with the continuous treatment (Hansen’s book Chapter 2.30 calls it average causal effect) as \\[ATE\\left(d,x\\right)=E\\left[\\lim_{\\Delta\\to0}\\frac{h\\left(d+\\Delta,x,u\\right)-h\\left(d,x,u\\right)}{\\Delta}\\right]=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right],\\] where the continuous differentiability of \\(h\\left(d,x,u\\right)\\) at \\(d\\) is implicitly assumed. Unlike the binary treatment case, here \\(d\\) explicitly shows up in \\(ATE\\left(d,x\\right)\\) because the effect can vary at different values of \\(d\\). ATE here is the average effect in the population of individuals if we hypothetical move \\(D\\) a tiny bit around \\(d\\), keeping all other factors \\(x\\) constant. In the previous sections, we focused on the CEF \\(m\\left(d,x\\right)\\), where \\(d\\) is added to \\(x\\) as an additional variable of interest. We did not intend to model the underlying economic mechanism \\(h\\left(D,x,u\\right)\\), which may be very complex. Can we learn the \\(ATE\\left(d,x\\right)\\) which bears the structural causal interpretation, from the mechanical \\(m\\left(d,x\\right)\\) which merely cares about best prediction? The answer is positive under CIA: \\(\\left(u\\perp D\\right)|x\\). \\[\\begin{aligned} \\frac{\\partial}{\\partial d}m\\left(d,x\\right) &amp; =\\frac{\\partial}{\\partial d}E\\left[y|d,x\\right]=\\frac{\\partial}{\\partial d}E\\left[h\\left(d,x,u\\right)|d,x\\right]=\\frac{\\partial}{\\partial d}\\int h\\left(d,x,u\\right)f\\left(u|d,x\\right)du\\\\ &amp; =\\int\\frac{\\partial}{\\partial d}\\left[h\\left(d,x,u\\right)f\\left(u|d,x\\right)\\right]du\\\\ &amp; =\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du+\\int h\\left(d,x,u\\right)\\left[\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)\\right]du,\\end{aligned}\\] where the second line implicitly assumes interchangeability between the integral and the partial derivative. Under CIA, \\(\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)=0\\) and the second term drops out. Thus \\[\\frac{\\partial}{\\partial d}m\\left(d,x\\right)=\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]=ATE\\left(d,x\\right).\\] This is an important result. It says that if CIA holds, we can learn the causal effect of \\(d\\) on \\(y\\) by the partial derivative of CEF conditional on \\(x\\). In particular, if we further assume a linear CEF \\(m\\left(d,x\\right)=\\beta_{d}d+\\beta_{x}&#39;x\\), then the causal effect is the coefficient \\(\\beta_{d}\\). CIA is the key condition that links the CEF and the causal effect. CIA is not an innocuous assumption. In applications, our causal results are credible only when we can convincing defend CIA. Let factories’ output be a Cobb-Douglas function \\(Y=AK^{\\alpha}L^{\\beta}\\), where the capital level \\(K\\) and labor \\(L\\) as well as the output \\(Y\\) is observable, while the “technology” \\(A\\) is unobservable. Take logarithm on both sides of the equation: \\[y=u+\\alpha k+\\beta l\\label{eq:causal}\\] where \\(y=\\log Y\\), \\(u=\\log A\\), \\(k=\\log K\\) and \\(l=\\log L\\). Suppose \\(\\begin{pmatrix}u\\\\ k\\\\ l \\end{pmatrix}\\sim N\\left(\\begin{pmatrix}1\\\\ 1\\\\ 1 \\end{pmatrix},\\begin{pmatrix}1 &amp; 0.5 &amp; 0\\\\ 0.5 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\right)\\) and \\(\\alpha=\\beta=1/2\\) make the true DGP. Here \\(u\\) and \\(k\\) are correlated, because factories of larger scale can afford robots to facilitate automation. What is the partial derivative of CEF when we use \\(k\\) as a treatment variable for a fixed labor level \\(l\\)? (Hint: the CEF is a linear function thanks to the joint normality.) Does it coincide with \\(\\alpha=1/2\\), the coefficient in the causal model (\\[eq:causal\\])? (Hint: No, because CIA is violated.) Sometimes applied researchers assume by brute force that \\(y=m\\left(d,x\\right)+u\\) is the DGP and \\(E\\left[u|d,x\\right]=0\\), where \\(d\\) is the variable of interest and \\(x\\) is the vector of other control variables. Under these assumptions, \\[ATE\\left(d,x\\right)=E\\left[\\frac{\\partial}{\\partial d}\\left(m\\left(d,x\\right)+u\\right)|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d}+\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d},\\] where the second equality holds if \\(\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=E\\left[\\frac{\\partial}{\\partial d}u|d,x\\right]\\). At a first glance, it seems that the mean independence assumption \\(E\\left[u|d,x\\right]=0\\), which is weaker than CIA, implies the equivalence between \\(ATE\\left(d,x\\right)\\) and \\(\\partial m\\left(d,x\\right)/\\partial d\\) here. However, such slight weakening is achieved by a very strong assumption that the DGP \\(h\\left(d,x,u\\right)\\) follows the additive separable form \\(m\\left(d,x\\right)+u\\). Without economic theory to defend the choice of the assumed DGP \\(y=m\\left(d,x\\right)+u\\), this is at best the reduced-form approach. The structural approach here models the economic mechanism, guided by economic theory. The reduced-form approach is convenient and can document stylized facts when suitable economic theory is not immediately available. There are constant debates about the pros and cons of the two approaches; see Journal of Economic Perspectives Vol. 24, No. 2 Spring 2010. In macroeconomics, the so-called Phillips curve, attributed to A.W. Phillips about the negative correlation between inflation and unemployment, is a stylized fact learned from the reduced-form approach. The Lucas critique (Lucas 1976) exposed its lack of microfoundation and advocated modeling deep parameters that are invariant to policy changes. The latter is a structural approach. Ironically, more than 40 years has passed since the Lucas critique, equations with little microfoundation still dominate the analytical apparatus of central bankers. 3.3 Summary In this lecture, we cover the conditional mean function and causality. When we are faced with a pair of random variable \\(\\left(y,x\\right)\\) drawn from some joint distribution, the CEF is the best predictor. When we go further into the structural causality about some treatment \\(d\\) to the dependent variable \\(y\\), under CIA we can find equivalence between ATE and the partial derivative of CEF. All analyses are conducted in population. We have not touched the sample yet. Historical notes: Regressions and conditional expectations are concepts from statistics and they are imported to econometrics in early time. Researchers at the Cowles Commission (now Cowles Foundation for Research in Economics) — Jacob Marschak (1898–1977), Tjalling Koopmans (1910–1985, Nobel Prize 1975), Trygve Haavelmo (1911–1999, Nobel Prize 1989) and their colleagues — were trailblazers of the econometric structural approach. The potential outcome framework is not peculiar to economics. It is widely used in other fields such as biostatistics and medical studies. It was initiated by Jerzy Neyman (1894–1981) and extended by Donald B. Rubin (1943– ), Professor of Statistics at Tsinghua University. Further reading: Lewbel (2019) offers a comprehensive summary of identification in econometrics. Accounting is an applied field with many claimed causal inference drawn from simple regressions; it is encouraging to hear Gow, Larcker, and Reiss (2016) to reflect causality in their practices. Zhentao Shi. Sep 17, 2020 "]
]
