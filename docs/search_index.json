[
["probability.html", "Econ5121 1 Probability 1.1 Introduction 1.2 Axiomatic Probability 1.3 Expected Value 1.4 Multivariate Random Variable 1.5 Summary", " Econ5121 Zhentao Shi 2022-01-22 1 Probability For the convenience of online teaching in the fall semester of 2020, the layout is modified with wide margins and line space for note taking. 1.1 Introduction With the advent of big data, computer scientists have come up with a plethora of new algorithms that are aimed at revealing patterns from data. Machine learning and artificial intelligence become buzz words that attract public attention. They defeated best human Go players, automated manufacturers, powered self-driving vehicles, recognized human faces, and recommended online purchases. Some of these industrial successes are based on statistical theory, and statistical theory is based on probability theory. Although this probabilistic approach is not the only perspective to understand the behavior of machine learning and artificial intelligence, it offers one of the most promising paradigms to rationalize existing algorithms and engineer new ones. Economics has been an empirical social science since Adam Smith (1723–1790). Many numerical observations and anecdotes were scattered in his Wealth of Nations published in 1776. Ragnar Frisch (1895–1973) and Jan Tinbergen (1903--1994), two pioneer econometricians, were awarded in 1969 the first Nobel Prize in economics. Econometrics provides quantitative insights about economic data. It flourishes in real-world management practices, from households and firms up to governance at the global level. Today, the big data revolution is pumping fresh energy into research and exercises of econometric methods. The mathematical foundation of econometric theory is built on probability theory as well. 1.2 Axiomatic Probability Human beings are awed by uncertainty in daily life. In the old days, Egyptians consulted oracles, Hebrews inquired prophets, and Chinese counted on diviners to interpret tortoise shell or bone cracks. Fortunetellers are abundant in today’s Hong Kong. Probability theory is a philosophy about uncertainty. Over centuries, mathematicians strove to contribute to the understanding of randomness. As measure theory matured in the early 20th century, Andrey Kolmogorov (1903-1987) built the edifice of modern probability theory in his monograph published in 1933. The formal mathematical language is a system that allows rigorous explorations which have made fruitful advancements, and is now widely accepted in academic and industrial research. In this lecture, we will briefly introduce the axiomatic probability theory along with familiar results covered in undergraduate probability and statistics. This lecture note is at the level Hansen (2020): Introduction to Econometrics, or Stachurski (2016): A Primer in Econometric Theory, or Casella and Berger (2002): Statistical Inference (second edition) Interested readers may want to read this textbook for more examples. 1.2.1 Probability Space A sample space \\(\\Omega\\) is a collection of all possible outcomes. It is a set of things. An event \\(A\\) is a subset of \\(\\Omega\\). It is something of interest on the sample space. A \\(\\sigma\\)-field, denoted by \\(\\mathcal{F}\\), is a collection of events such that \\(\\emptyset\\in\\mathcal{F}\\); if an event \\(A\\in\\mathcal{F}\\), then \\(A^{c}\\in\\mathcal{F}\\); if \\(A_{i}\\in\\mathcal{F}\\) for \\(i\\in\\mathbb{N}\\), then \\(\\bigcup_{i\\in\\mathbb{N}}A_{i}\\in\\mathcal{F}\\). Implications: (a) Since \\(\\Omega=\\emptyset^{c}\\in\\mathcal{F}\\), we have \\(\\Omega\\in\\mathcal{F}\\). (b) If \\(A_{i}\\in\\mathcal{F}\\) for \\(i\\in\\mathbb{N}\\), then \\(A_{i}^{c}\\in\\mathcal{F}\\) for \\(i\\in\\mathbb{N}\\). Thus, if \\(\\bigcup_{i\\in\\mathbb{N}}A_{i}^{c}\\in\\mathcal{F}\\) , then \\(\\bigcap_{i\\in\\mathbb{N}}A_{i}=(\\bigcup_{i\\in\\mathbb{N}}A_{i}^{c})^{c}\\in\\mathcal{F}\\). 1.1*. Intuitively, a \\(\\sigma\\)-field is a pool which is closed for countable sets to conduct union, difference, and intersection operations. These are algebraic operations of sets. \\(\\sigma\\)-field is also called \\(\\sigma\\)-algebra. ** 1.1**. Let \\(\\Omega=\\{1,2,3,4,5,6\\}\\). Some examples of \\(\\sigma\\)-fields include \\(\\mathcal{F}_{1}=\\{\\emptyset,\\{1,2,3\\},\\{4,5,6\\},\\Omega\\}\\); \\(\\mathcal{F}_{2}=\\{\\emptyset,\\{1,3\\},\\{2,4,5,6\\},\\Omega\\}\\). Counterexample: \\(\\mathcal{F}_{3}=\\{\\emptyset,\\{1,2\\},\\{4,6\\},\\Omega\\}\\) is not a \\(\\sigma\\)-field since \\(\\{1,2,4,6\\}=\\{1,2\\}\\bigcup\\{4,6\\}\\) does not belong to \\(\\mathcal{F}_{3}\\). The \\(\\sigma\\)-field can be viewed as a well-organized structure built on the ground of the sample space. The pair \\(\\left(\\Omega,\\mathcal{F}\\right)\\) is called a measure space. Let \\(\\mathcal{G}=\\{B_{1},B_{2},\\ldots\\}\\) be an arbitrary collection of sets, not necessarily a \\(\\sigma\\)-field. We say \\(\\mathcal{F}\\) is the smallest \\(\\sigma\\)-field generated by \\(\\mathcal{G}\\) if \\(\\mathcal{G}\\subseteq\\mathcal{F}\\), and \\(\\mathcal{F}\\subseteq\\mathcal{\\tilde{F}}\\) for any \\(\\mathcal{\\tilde{F}}\\) such that \\(\\mathcal{G}\\subseteq\\mathcal{\\tilde{F}}\\). A Borel \\(\\sigma\\)-field \\(\\mathcal{R}\\) is the smallest \\(\\sigma\\)-field generated by the open sets on the real line \\(\\mathbb{R}\\). ** 1.2**. Let \\(\\Omega=\\{1,2,3,4,5,6\\}\\) and \\(A=\\{\\{1\\},\\{1,3\\}\\}\\). Then the smallest \\(\\sigma\\)-field generated by \\(A\\) is \\[\\sigma(A)=\\{\\emptyset,\\{1\\},\\{1,3\\},\\{3\\},\\{2,4,5,6\\},\\{2,3,4,5,6\\},\\{1,2,4,5,6\\},\\Omega\\}.\\] A function \\(\\mu:(\\Omega,\\mathcal{F})\\mapsto\\left[0,\\infty\\right]\\) is called a measure if it satisfies (positiveness) \\(\\mu\\left(A\\right)\\geq0\\) for all \\(A\\in\\mathcal{F}\\); (countable additivity) if \\(A_{i}\\in\\mathcal{F}\\), \\(i\\in\\mathbb{N}\\), are mutually disjoint, then \\[\\mu\\left(\\bigcup_{i\\in\\mathbb{N}}A_{i}\\right)=\\sum_{i\\in\\mathbb{N}}\\mu\\left(A_{i}\\right).\\] Measure can be understand as weight or length. In particular, we call \\(\\mu\\) a probability measure if \\(\\mu\\left(\\Omega\\right)=1\\). A probability measure is often denoted as \\(P\\). The triple \\(\\left(\\Omega,\\mathcal{F},P\\right)\\) is called a probability space. So far we have answered the question: “What is a mathematically well-defined probability?”, but we have not yet answered “How to assign the probability?” There are two major schools of thinking on probability assignment. One is frequentist, who considers probability as the average chance of occurrence if a large number of experiments are carried out. The other is Bayesian, who deems probability as a subjective brief. The principles of these two schools are largely incompatible, while each school has merits and difficulties, which will be elaborated when discussing hypothesis testing. 1.2.2 Random Variable The terminology random variable is a historic relic which belies its modern definition of a deterministic mapping. It is a link between two measurable spaces such that any event in the \\(\\sigma\\)-field installed on the range can be traced back to an event in the \\(\\sigma\\)-field installed on the domain. Formally, a function \\(X:\\Omega\\mapsto\\mathbb{R}\\) is \\(\\left(\\Omega,\\mathcal{F}\\right)\\backslash\\left(\\mathbb{R},\\mathcal{R}\\right)\\) measurable if \\[X^{-1}\\left(B\\right)=\\left\\{ \\omega\\in\\Omega:X\\left(\\omega\\right)\\in B\\right\\} \\in\\mathcal{F}\\] for any \\(B\\in\\mathcal{R}.\\) Random variable is an alternative, and somewhat romantic, name for a measurable function. The \\(\\sigma\\)-field generated by the random variable \\(X\\) is defined as \\(\\sigma\\left(X\\right)=\\left\\{ X^{-1}\\left(B\\right):B\\in\\mathcal{R}\\right\\}\\). We say a measurable is a discrete random variable if the set \\(\\left\\{ X\\left(\\omega\\right):\\omega\\in\\Omega\\right\\}\\) is finite or countable. We say it is a continuous random variable if the set \\(\\left\\{ X\\left(\\omega\\right):\\omega\\in\\Omega\\right\\}\\) is uncountable. A measurable function connects two measurable spaces. No probability is involved in its definition yet. While if a probability measure \\(P\\) is installed on \\((\\Omega,\\mathcal{F})\\), the measurable function \\(X\\) will induce a probability measure on \\((\\mathbb{R},\\mathcal{R})\\). It is easy to verify that \\(P_{X}:(\\mathbb{R},\\mathcal{R})\\mapsto\\left[0,1\\right]\\) is also a probability measure if defined as \\[P_{X}\\left(B\\right)=P\\left(X^{-1}\\left(B\\right)\\right)\\] for any \\(B\\in\\mathcal{R}\\). This \\(P_{X}\\) is called the probability measure induced by the measurable function \\(X\\). The induced probability measure \\(P_{X}\\) is an offspring of the parent probability measure \\(P\\) though the channel of \\(X\\). 1.2.3 Distribution Function We go back to some terms that we have learned in a undergraduate probability course. A (cumulative) distribution function \\(F:\\mathbb{R}\\mapsto[0,1]\\) is defined as \\[F\\left(x\\right)=P\\left(X\\leq x\\right)=P\\left(\\{X\\leq x\\}\\right)=P\\left(\\left\\{ \\omega\\in\\Omega:X\\left(\\omega\\right)\\leq x\\right\\} \\right).\\] It is often abbreviated as CDF, and it has the following properties. \\(\\lim_{x\\to-\\infty}F\\left(x\\right)=0\\), \\(\\lim_{x\\to\\infty}F\\left(x\\right)=1\\), non-decreasing, right-continuous \\(\\lim_{y\\to x^{+}}F\\left(y\\right)=F\\left(x\\right).\\) ** 1.1**. Draw the CDF of a binary distribution; that is, \\(X=1\\) with probability \\(p\\in\\left(0,1\\right)\\) and \\(X=0\\) with probability \\(1-p\\). For continuous distribution, if there exists a function \\(f\\) such that for all \\(x\\), \\[F\\left(x\\right)=\\int_{-\\infty}^{x}f\\left(y\\right)\\mathrm{d}y,\\] then \\(f\\) is called the probability density function of \\(X\\), often abbreviated as PDF. It is easy to show that \\(f\\left(x\\right)\\geq0\\) and \\(\\int_{a}^{b}f\\left(x\\right)dx=F\\left(b\\right)-F\\left(a\\right)\\). ** 1.3**. We have learned many parametric distributions like the binary distribution, the Poisson distribution, the uniform distribution, the exponential distribution, the normal distribution, \\(\\chi^{2}\\), \\(t\\), \\(F\\) distributions and so on. They are parametric distributions, meaning that the CDF or PDF can be completely characterized by very few parameters. 1.3 Expected Value 1.3.1 Integration Integration is one of the most fundamental operations in mathematical analysis. We have studied Riemann’s integral in the undergraduate calculus. Riemann’s integral is intuitive, but Lebesgue integral is a more general approach to defining integration. Lebesgue integral is constructed by the following steps. \\(X\\) is called a simple function on a measurable space \\(\\left(\\Omega,\\mathcal{F}\\right)\\) if \\(X=\\sum_{i}a_{i}\\cdot1\\left\\{ A_{i}\\right\\}\\) and this summation is finite, where \\(a_{i}\\in\\mathbb{R}\\) and \\(\\{A_{i}\\in\\mathcal{F}\\}_{i\\in\\mathbb{N}}\\) is a partition of \\(\\Omega\\). A simple function is measurable. Let \\(\\left(\\Omega,\\mathcal{F},\\mu\\right)\\) be a measure space. The integral of the simple function \\(X\\) with respect to \\(\\mu\\) is \\[\\int X\\mathrm{d}\\mu=\\sum_{i}a_{i}\\mu\\left(A_{i}\\right).\\] Unlike the Rieman integral, this definition of integration does not partition the domain into splines of equal length. Instead, it tracks the distinctive values of the function and the corresponding measure. Let \\(X\\) be a non-negative measurable function. The integral of \\(X\\) with respect to \\(\\mu\\) is \\[\\int X\\mathrm{d}\\mu=\\sup\\left\\{ \\int Y\\mathrm{d}\\mu:0\\leq Y\\leq X,\\text{ }Y\\text{ is simple}\\right\\} .\\] Let \\(X\\) be a measurable function. Define \\(X^{+}=\\max\\left\\{ X,0\\right\\}\\) and \\(X^{-}=-\\min\\left\\{ X,0\\right\\}\\). Both \\(X^{+}\\) and \\(X^{-}\\) are non-negative functions. The integral of \\(X\\) with respect to \\(\\mu\\) is \\[\\int X\\mathrm{d}\\mu=\\int X^{+}\\mathrm{d}\\mu-\\int X^{-}\\mathrm{d}\\mu.\\] The Step 1 above defines the integral of a simple function. Step 2 defines the integral of a non-negative function as the approximation of steps functions from below. Step 3 defines the integral of a general function as the difference of the integral of two non-negative parts. 1.2*. The integrand that highlights the difference between the Lebesgue integral and Riemann integral is the Dirichelet function on the unit interval \\(1\\left\\{ x\\in\\mathbb{Q}\\cap[0,1]\\right\\}\\). It is not Riemann-integrable whereas its Lebesgue integral. is well defined and \\(\\int1\\left\\{ x\\in\\mathbb{Q}\\cap[0,1]\\right\\} dx=0\\). If the measure \\(\\mu\\) is a probability measure \\(P\\), then the integral \\(\\int X\\mathrm{d}P\\) is called the expected value, or expectation, of \\(X\\). We often use the notation \\(E\\left[X\\right]\\), instead of \\(\\int X\\mathrm{d}P\\), for convenience. Expectation provides the average of a random variable, despite that we cannot foresee the realization of a random variable in a particular trial (otherwise the study of uncertainty is trivial). In the frequentist’s view, the expectation is the average outcome if we carry out a large number of independent trials. If we know the probability mass function of a discrete random variable, its expectation is calculated as \\(E\\left[X\\right]=\\sum_{x}xP\\left(X=x\\right)\\), which is the integral of a simple function. If a continuous random variable has a PDF \\(f(x)\\), its expectation can be computed as \\(E\\left[X\\right]=\\int xf\\left(x\\right)\\mathrm{d}x\\). These two expressions are unified as \\(E[X]=\\int X\\mathrm{d}P\\) by the Lebesgue integral. 1.3.2 Properties of Expectations Here are some properties of mathematical expectations. The probability of an event \\(A\\) is the expectation of an indicator function. \\(E\\left[1\\left\\{ A\\right\\} \\right]=1\\times P(A)+0\\times P(A^{c})=P\\left(A\\right)\\). \\(E\\left[X^{r}\\right]\\) is call the \\(r\\)-moment of \\(X\\). The mean of a random variable is the first moment \\(\\mu=E\\left[X\\right]\\), and the second centered moment is called the variance \\(\\mathrm{var}\\left[X\\right]=E\\left[\\left(X-\\mu\\right)^{2}\\right]\\). The third centered moment \\(E\\left[\\left(X-\\mu\\right)^{3}\\right]\\), called skewness, is a measurement of the symmetry of a random variable, and the fourth centered moment \\(E\\left[\\left(X-\\mu\\right)^{4}\\right]\\), called kurtosis, is a measurement of the tail thickness. Moments do not always exist. For example, the mean of the Cauchy distribution does not exist, and the variance of the \\(t(2)\\) distribution does not exist. \\(E[\\cdot]\\) is a linear operation. If \\(\\phi(\\cdot)\\) is a linear function, then \\(E[\\phi(X)]=\\phi(E[X]).\\) Jensen’s inequality is an important fact. A function \\(\\varphi(\\cdot)\\) is convex if \\(\\varphi(ax_{1}+(1-a)x_{2})\\leq a\\varphi(x_{1})+(1-a)\\varphi(x_{2})\\) for all \\(x_{1},x_{2}\\) in the domain and \\(a\\in[0,1]\\). For instance, \\(x^{2}\\) is a convex function. Jensen’s inequality says that if \\(\\varphi\\left(\\cdot\\right)\\) is a convex function, then \\(\\varphi\\left(E\\left[X\\right]\\right)\\leq E\\left[\\varphi\\left(X\\right)\\right].\\) Markov inequality is another simple but important fact. If \\(E\\left[\\left|X\\right|^{r}\\right]\\) exists, then \\(P\\left(\\left|X\\right|&gt;\\epsilon\\right)\\leq E\\left[\\left|X\\right|^{r}\\right]/\\epsilon^{r}\\) for all \\(r\\geq1\\). Chebyshev inequality \\(P\\left(\\left|X\\right|&gt;\\epsilon\\right)\\leq E\\left[X^{2}\\right]/\\epsilon^{2}\\) is a special case of the Markov inequality when \\(r=2\\). The distribution of a random variable is completely characterized by its CDF or PDF. A moment is a function of the distribution. To back out the underlying distribution from moments, we need to know the moment-generating function (mgf) \\(M_{X}(t)=E[e^{tX}]\\) for \\(t\\in\\mathbb{R}\\) whenever the expectation exists. The \\(r\\)th moment can be computed from mgf as \\[E[X^{r}]=\\frac{\\mathrm{d}^{r}M_{X}(t)}{\\mathrm{d}t^{r}}\\big\\vert_{t=0}.\\] Just like moments, mgf does not always exist. 1.4 Multivariate Random Variable A bivariate random variable is a measurable function \\(X:\\Omega\\mapsto\\mathbb{R}^{2}\\), and more generally a multivariate random variable is a measurable function \\(X:\\Omega\\mapsto\\mathbb{R}^{n}\\). We can define the joint CDF as \\(F\\left(x_{1},\\ldots,x_{n}\\right)=P\\left(X_{1}\\leq x_{1},\\ldots,X_{n}\\leq x_{n}\\right)\\). Joint PDF is defined similarly. It is sufficient to introduce the joint distribution, conditional distribution and marginal distribution in the simple bivariate case, and these definitions can be extended to multivariate distributions. Suppose a bivariate random variable \\((X,Y)\\) has a joint density \\(f(\\cdot,\\cdot)\\). The conditional density can be roughly written as \\(f\\left(y|x\\right)=f\\left(x,y\\right)/f\\left(x\\right)\\) if we do not formally deal with the case \\(f(x)=0\\). The marginal density \\(f\\left(y\\right)=\\int f\\left(x,y\\right)dx\\) integrates out the coordinate that is not interested. 1.4.1 Conditional Probability and Bayes’ Theorem In a probability space \\((\\Omega,\\mathcal{F},P)\\), for two events \\(A_{1},A_{2}\\in\\mathcal{F}\\) the conditional probability is \\[P\\left(A_{1}|A_{2}\\right)=\\frac{P\\left(A_{1}A_{2}\\right)}{P\\left(A_{2}\\right)}\\] if \\(P(A_{2})&gt;0\\). In the definition of conditional probability, \\(A_{2}\\) plays the role of the outcome space so that \\(P(A_{1}A_{2})\\) is standardized by the total mass \\(P(A_{2})\\). If \\(P(A_{2})=0\\), the conditional probability can still be valid in some cases, but we need to introduce the dominance between two measures, which we do not elaborate here. Since \\(A_{1}\\) and \\(A_{2}\\) are symmetric, we also have \\(P(A_{1}A_{2})=P(A_{2}|A_{1})P(A_{1})\\). It implies \\[P(A_{1}|A_{2})=\\frac{P\\left(A_{2}|A_{1}\\right)P\\left(A_{1}\\right)}{P\\left(A_{2}\\right)}\\] This formula is the Bayes’ Theorem. 1.4.2 Independence We say two events \\(A_{1}\\) and \\(A_{2}\\) are independent if \\(P(A_{1}A_{2})=P(A_{1})P(A_{2})\\). If \\(P(A_{2})\\neq0\\), it is equivalent to \\(P(A_{1}|A_{2})=P(A_{1})\\). In words, knowing \\(A_{2}\\) does not change the probability of \\(A_{1}\\). Regarding the independence of two random variables, \\(X\\) and \\(Y\\) are independent if \\(P\\left(X\\in B_{1},Y\\in B_{2}\\right)=P\\left(X\\in B_{1}\\right)P\\left(Y\\in B_{2}\\right)\\) for any two Borel sets \\(B_{1}\\) and \\(B_{2}\\). If \\(X\\) and \\(Y\\) are independent, then \\(E[XY]=E[X]E[Y]\\). The expectation of their product is the product of their expectations. 1.4.3 Law of Iterated Expectations Given a probability space \\(\\left(\\Omega,\\mathcal{F},P\\right)\\), a sub \\(\\sigma\\)-algebra \\(\\mathcal{G}\\subseteq\\mathcal{F}\\) and a \\(\\mathcal{F}\\)-measurable function \\(Y\\) with \\(E\\left|Y\\right|&lt;\\infty\\), the conditional expectation \\(E\\left[Y|\\mathcal{G}\\right]\\) is defined as a \\(\\mathcal{G}\\)-measurable function such that \\[\\int_{A}Y\\mathrm{d}P=\\int_{A}E\\left[Y|\\mathcal{G}\\right]\\mathrm{d}P\\] for all \\(A\\in\\mathcal{G}\\). Here \\(\\mathcal{G}\\) is a coarse \\(\\sigma\\)-field and \\(\\mathcal{F}\\) is a finer \\(\\sigma\\)-field. Taking \\(A=\\Omega\\), we have \\(E\\left[Y\\right]=\\int Y\\mathrm{d}P=\\int E\\left[Y|\\mathcal{G}\\right]\\mathrm{d}P=E\\left[E\\left[Y|\\mathcal{G}\\right]\\right]\\). The law of iterated expectation \\[E\\left[Y\\right]=E\\left[E\\left[Y|\\mathcal{G}\\right]\\right]\\] is a trivial fact which follows this definition of the conditional expectation. In the bivariate case, if the conditional density exists, the conditional expectation can be computed as \\(E\\left[Y|X\\right]=\\int yf\\left(y|X\\right)\\mathrm{d}y\\), where the conditioning variable \\(E\\left[\\cdot|X\\right]=E\\left[\\cdot|\\sigma\\left(X\\right)\\right]\\) is a concise notation for the smallest \\(\\sigma\\)-field generated by \\(X\\). The law of iterated expectation implies \\(E\\left[E\\left[Y|X\\right]\\right]=E\\left[Y\\right]\\). Below are some properties of conditional expectations \\(E\\left[E\\left[Y|X_{1},X_{2}\\right]|X_{1}\\right]=E\\left[Y|X_{1}\\right];\\) \\(E\\left[E\\left[Y|X_{1}\\right]|X_{1},X_{2}\\right]=E\\left[Y|X_{1}\\right];\\) \\(E\\left[h\\left(X\\right)Y|X\\right]=h\\left(X\\right)E\\left[Y|X\\right].\\) 1.5 Summary If it is your first encounter of measure theory, the new definitions here may seem overwhelmingly abstract. A natural question is that: “I earned high grade in my undergraduate probability and statistics; do I really need the fancy mathematics in this lecture to do well in econometrics?” The answer is yes and no. No is in the sense that if you want to use econometric methods, instead of grasp the underlying theory, then the axiomatic probability does not add much to your weaponry. You can be an excellent economist or applied econometrician without knowing measure theoretic probability. Yes is in the sense that without measure theory, we cannot even formally define conditional expectation, which will be the subject of our next lecture and is a core concept of econometrics. Moreover, the pillars of asymptotic theory — law of large numbers and central limit theorem — can only be made accurate with this foundation. If you are aspired to work on econometric theory, you will meet and use measure theory so often in your future study and finally it becomes part of your muscle memory. In this course, we try to keep a balance manner. On the one hand, many econometrics topics can be presented with elementary mathematics. Whenever possible, econometrics should reach wider audience with a plain appearance, instead of intimidating people by arcane languages. On the other hand, we introduce these concepts in this lecture and will invoke them in the discussion of asymptotic theory later. Your investment in advanced mathematics will not be wasted in vain. Historical notes: Measure theory was established in the early 20th century by a constellation of French/German mathematicians, represented by Émile Borel, Henri Lebesgue, Johann Radon, etc. Generations of Russian mathematicians such as Andrey Markov and Andrey Kolmogorov made fundamental contributions in mathematizing seemingly abstract concepts of uncertainty and randomness. Their names are immortalized by the Borel set, the Lebesgue integral, the Radon measure, Markov chain, Kolmogorov’s zero–one law and many other terminologies named after them. Fascinating questions about probability attracted great economists. Francis Edgeworth (1845–1926) wrote extensively on probability and statistics. John Maynard Keynes (1883–1946) published A Treatise on Probability in 1921 which mixed probability and philosophy, although this piece of work was not as influential as his General Theory of Employment, Interest and Money in 1936 which later revolutionized economics. Today, the technology of collecting data and the processing data is unbelievably cheaper than that 100 years ago. Unfortunately, the cost of learning mathematics and developing mathematics has not been significantly lowered over one century. Only a small handful of talents, like you, enjoy the privilege and luxury to appreciate the ideas of these great minds. Further reading: Doob (1996) summarized the development of axiomatic probability in the first half of the 20th century. Zhentao Shi. Sep 12, 2020. References "],
["conditional-expectation.html", "2 Conditional Expectation 2.1 Linear Projection 2.2 Causality 2.3 Summary", " 2 Conditional Expectation Notation: In this note, \\(y\\) is a scale random variable, and \\(x=\\left(x_{1},\\ldots,x_{K}\\right)&#39;\\) is a \\(K\\times1\\) random vector. Throughout this course, a vector is a column vector, i.e. a one-column matrix. Machine learning is a big basket that contains the regression models. We motivate the conditional expectation model from the perspective of prediction. We view a regression as supervised learning. Supervised learning uses a function of \\(x\\), say, \\(g\\left(x\\right)\\), to predict \\(y\\). \\(x\\) cannot perfectly predict \\(y\\); otherwise their relationship is deterministic. The prediction error \\(y-g\\left(x\\right)\\) depends on the choice of \\(g\\). There are numerous possible choices of \\(g\\). Which one is the best? Notice that this question is not concerned about the underlying data generating process (DGP) of the joint distribution of \\(\\left(y,x\\right)\\). We want to find a general rule to achieve accurate prediction of \\(y\\) given \\(x\\), no matter how this pair of variables is generated. To answer this question, we need to decide a criterion to compare different \\(g\\). Such a criterion is called the loss function \\(L\\left(y,g\\left(x\\right)\\right)\\). A particularly convenient one is the quadratic loss, defined as \\[L\\left(y,g\\left(x\\right)\\right)=\\left(y-g\\left(x\\right)\\right)^{2}.\\] Since the data are random, \\(L\\left(y,g\\left(x\\right)\\right)\\) is also random. “Random” means uncertainty: sometimes this happens, and sometimes that happens. To get rid of the uncertainty, we average the loss function with respect to the joint distribution of \\(\\left(y,x\\right)\\) as \\(R\\left(y,g\\left(x\\right)\\right)=E\\left[L\\left(y,g\\left(x\\right)\\right)\\right]\\), which is called risk. Risk is a deterministic quality. For the quadratic loss function, the corresponding risk is \\[R\\left(y,g\\left(x\\right)\\right)=E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right],\\] is called the mean squared error (MSE). MSE is the most widely used risk measure, although there exist many alternative measures, for example the mean absolute error (MAE) \\(E\\left[\\left|y-g\\left(x\\right)\\right|\\right]\\). The popularity of MSE comes from its convenience for analysis in closed-form, which MAE does not enjoy due to its nondifferentiability. This is similar to the choice of utility functions in economics. There are only a few functional forms for the utility, for example CRRA, CARA, and so on. They are popular because they lead to close-form solutions that are easy to handle. Now our quest is narrowed to: What is the optimal choice of \\(g\\) if we minimize the MSE? \\[prop:CEF\\] The conditional mean function (CEF) \\(m\\left(x\\right)=E\\left[y|x\\right]=\\int yf\\left(y|x\\right)\\mathrm{d}y\\) minimizes MSE. Before we prove the above proposition, we first discuss some properties of the conditional mean function. Obviously \\[y=m\\left(x\\right)+\\left(y-m\\left(x\\right)\\right)=m\\left(x\\right)+\\epsilon,\\] where \\(\\epsilon:=y-m\\left(x\\right)\\) is called the regression error. This equation holds for \\(\\left(y,x\\right)\\) following any joint distribution, as long as \\(E\\left[y|x\\right]\\) exists. The error term \\(\\epsilon\\) satisfies these properties: \\(E\\left[\\epsilon|x\\right]=E\\left[y-m\\left(x\\right)|x\\right]=E\\left[y|x\\right]-m(x)=0\\), \\(E\\left[\\epsilon\\right]=E\\left[E\\left[\\epsilon|x\\right]\\right]=E\\left[0\\right]=0\\), For any function \\(h\\left(x\\right)\\), we have \\[E\\left[h\\left(x\\right)\\epsilon\\right]=E\\left[E\\left[h\\left(x\\right)\\epsilon|x\\right]\\right]=E\\left[h(x)E\\left[\\epsilon|x\\right]\\right]=0.\\label{eq:uncorr}\\] The last property implies that \\(\\epsilon\\) is uncorrelated with any function of \\(x\\). In particular, when \\(h\\) is the identity function \\(h\\left(x\\right)=x\\), we have \\(E\\left[x\\epsilon\\right]=\\mathrm{cov}\\left(x,\\epsilon\\right)=0\\). The optimality of the CEF can be confirmed by “guess-and-verify.” For an arbitrary \\(g\\left(x\\right)\\), the MSE can be decomposed into three terms \\[\\begin{aligned} &amp; &amp; E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right]\\\\ &amp; = &amp; E\\left[\\left(y-m(x)+m(x)-g(x)\\right)^{2}\\right]\\\\ &amp; = &amp; E\\left[\\left(y-m\\left(x\\right)\\right)^{2}\\right]+2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]+E\\left[\\left(m\\left(x\\right)-g\\left(x\\right)\\right)^{2}\\right].\\end{aligned}\\] The first term is irrelevant to \\(g\\left(x\\right)\\). The second term \\[\\begin{aligned}2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right] &amp; =2E\\left[\\epsilon\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]=0\\end{aligned}\\] by invoking (\\[eq:uncorr\\]) with \\(h\\left(x\\right)=m\\left(x\\right)-g\\left(x\\right)\\). The second term is again irrelevant of \\(g\\left(x\\right)\\). The third term, obviously, is minimized at \\(g\\left(x\\right)=m\\left(x\\right)\\). Our perspective so far deviates from many econometric textbooks that assume that the dependent variable \\(y\\) is generated as \\(g\\left(x\\right)+\\epsilon\\) for some unknown function \\(g\\left(\\cdot\\right)\\) and error term \\(\\epsilon\\) such that \\(E\\left[\\epsilon|x\\right]=0\\). Instead, we take a predictive approach regardless the DGP. What we observe are \\(y\\) and \\(x\\) and we are solely interested in seeking a function \\(g\\left(x\\right)\\) to predict \\(y\\) as accurately as possible under the MSE criterion. 2.1 Linear Projection The CEF \\(m(x)\\) is the function that minimizes the MSE. However, \\(m\\left(x\\right)=E\\left[y|x\\right]\\) is a complex function of \\(x\\), for it depends on the joint distribution of \\(\\left(y,x\\right)\\), which is mostly unknown in practice. Now let us make the prediction task even simpler. How about we minimize the MSE within all linear functions in the form of \\(h\\left(x\\right)=h\\left(x;b\\right)=x&#39;b\\) for \\(b\\in\\mathbb{R}^{K}\\)? The minimization problem is \\[\\min_{b\\in\\mathbb{R}^{K}}E\\left[\\left(y-x&#39;b\\right)^{2}\\right].\\label{eq:linear_MSE}\\] Take the first-order condition of the MSE \\[\\frac{\\partial}{\\partial b}E\\left[\\left(y-x&#39;b\\right)^{2}\\right]=E\\left[\\frac{\\partial}{\\partial b}\\left(y-x&#39;b\\right)^{2}\\right]=-2E\\left[x\\left(y-x&#39;b\\right)\\right],\\] where the first equality holds if \\(E\\left[\\left(y-x&#39;b\\right)^{2}\\right]&lt;\\infty\\) so that the expectation and partial differentiation is interchangeable, and the second equality hods by the chain rule and the linearity of expectation. Set the first order condition to 0 and we solve \\[\\beta=\\arg\\min_{b\\in\\mathbb{R}^{K}}E\\left[\\left(y-x&#39;b\\right)^{2}\\right]\\] in the closed-form \\[\\beta=\\left(E\\left[xx&#39;\\right]\\right)^{-1}E\\left[xy\\right]\\] if \\(E\\left[xx&#39;\\right]\\) is invertible. Notice here that \\(b\\) is an arbitrary \\(K\\)-vector, while \\(\\beta\\) is the optimizer. The function \\(x&#39;\\beta\\) is called the best linear projection (BLP) of \\(y\\) on \\(x\\), and the vector \\(\\beta\\) is called the linear projection coefficient. The linear function is not as restrictive as one might thought. It can be used to produce some nonlinear (in random variables) effect if we re-define \\(x\\). For example, if \\[y=x_{1}\\beta_{1}+x_{2}\\beta_{2}+x_{1}^{2}\\beta_{3}+e,\\] then \\(\\frac{\\partial}{\\partial x_{1}}m\\left(x_{1},x_{2}\\right)=\\beta_{1}+2x_{1}\\beta_{3}\\), which is nonlinear in \\(x_{1}\\), while it is still linear in the parameter \\(\\beta=\\left(\\beta_{1},\\beta_{2},\\beta_{3}\\right)\\) if we define a set of new regressors as \\(\\left(\\tilde{x}_{1},\\tilde{x}_{2},\\tilde{x}_{3}\\right)=\\left(x_{1},x_{2},x_{1}^{2}\\right)\\). If \\(\\left(y,x\\right)\\) is jointly normal in the form \\[\\begin{pmatrix}y\\\\ x \\end{pmatrix}\\sim\\mathrm{N}\\left(\\begin{pmatrix}\\mu_{y}\\\\ \\mu_{x} \\end{pmatrix},\\begin{pmatrix}\\sigma_{y}^{2} &amp; \\rho\\sigma_{y}\\sigma_{x}\\\\ \\rho\\sigma_{y}\\sigma_{x} &amp; \\sigma_{x}^{2} \\end{pmatrix}\\right)\\] where \\(\\rho\\) is the correlation coefficient, then \\[E\\left[y|x\\right]=\\mu_{y}+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\left(x-\\mu_{x}\\right)=\\left(\\mu_{y}-\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\mu_{x}\\right)+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}x,\\] is a liner function of \\(x\\). In this example, the CEF is linear. Even though in general \\(m\\left(x\\right)\\neq x&#39;\\beta\\), the linear form \\(x&#39;\\beta\\) is still useful in approximating \\(m\\left(x\\right)\\). That is, \\(\\beta=\\arg\\min\\limits _{b\\in\\mathbb{R}^{K}}E\\left[\\left(m(x)-x&#39;b\\right)^{2}\\right]\\). The first-order condition gives \\(\\frac{\\partial}{\\partial b}E\\left[\\left(m(x)-x&#39;b\\right)^{2}\\right]=-2E[x(m(x)-x&#39;b)]=0\\). Rearrange the terms and obtain \\(E[x\\cdot m(x)]=E[xx&#39;]b\\). When \\(E[xx&#39;]\\) is invertible, we solve \\[\\left(E\\left[xx&#39;\\right]\\right){}^{-1}E[x\\cdot m(x)]=\\left(E\\left[xx&#39;\\right]\\right){}^{-1}E[E[xy|x]]=\\left(E\\left[xx&#39;\\right]\\right){}^{-1}E[xy]=\\beta.\\] Thus \\(\\beta\\) is also the best linear approximation to \\(m\\left(x\\right)\\) under MSE. We may rewrite the linear regression model, or the linear projection model, as \\[\\begin{array}[t]{c} y=x&#39;\\beta+e\\\\ E[xe]=0, \\end{array}\\] where \\(e=y-x&#39;\\beta\\) is called the linear projection error, to be distinguished from \\(\\epsilon=y-m(x).\\) Show (a) \\(E\\left[xe\\right]=0\\). (b) If \\(x\\) contains a constant, then \\(E\\left[e\\right]=0\\). 2.1.1 Omitted Variable Bias We write the long regression as \\[y=x_{1}&#39;\\beta_{1}+x_{2}&#39;\\beta_{2}+\\beta_{3}+e_{\\beta},\\] and the short regression as \\[y=x_{1}&#39;\\gamma_{1}+\\gamma_{2}+e_{\\gamma},\\] where \\(e_{\\beta}\\) and \\(e_{\\gamma}\\) are the projection errors, respectively. If \\(\\beta_{1}\\) in the long regression is the parameter of interest, omitting \\(x_{2}\\) as in the short regression will render omitted variable bias (meaning \\(\\gamma_{1}\\neq\\beta_{1}\\)) unless \\(x_{1}\\) and \\(x_{2}\\) are uncorrelated. We first demean all the variables in the two regressions, which is equivalent as if we project out the effect of the constant. The long regression becomes \\[\\tilde{y}=\\tilde{x}_{1}&#39;\\beta_{1}+\\tilde{x}_{2}&#39;\\beta_{2}+\\tilde{e}_{\\beta},\\] and the short regression becomes \\[\\tilde{y}=\\tilde{x}_{1}&#39;\\gamma_{1}+\\tilde{e}_{\\gamma},\\] where tilde denotes the demeaned variable. Show \\(\\tilde{e}_{\\beta}=e_{\\beta}\\) and \\(\\tilde{e}_{\\gamma}=e_{\\gamma}\\). After demeaning, the cross-moment equals to the covariance. The short regression coefficient \\[\\begin{aligned}\\gamma_{1} &amp; =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{y}\\right]\\\\ &amp; =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\left(\\tilde{x}_{1}&#39;\\beta_{1}+\\tilde{x}_{2}&#39;\\beta_{2}+\\tilde{e}_{\\beta}\\right)\\right]\\\\ &amp; =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]\\beta_{2}\\\\ &amp; =\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}&#39;\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]\\beta_{2}, \\end{aligned}\\] where the third line holds as \\(E\\left[\\tilde{x}_{1}\\tilde{e}_{\\beta}\\right]=0\\). Therefore, \\(\\gamma_{1}=\\beta_{1}\\) if and only if \\(E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]\\beta_{2}=0\\), which demands either \\(E\\left[\\tilde{x}_{1}\\tilde{x}_{2}&#39;\\right]=0\\) or \\(\\beta_{2}=0\\). Show that \\(E\\left[\\left(y-x_{1}&#39;\\beta_{1}-x_{2}&#39;\\beta_{2}-\\beta_{3}\\right)^{2}\\right]\\leq E\\left[\\left(y-x_{1}&#39;\\gamma_{1}-\\gamma_{2}\\right)^{2}\\right]\\). Obviously we prefer to run the long regression to attain \\(\\beta_{1}\\) if possible, for it is a more general model than the short regression and achieves no larger variance in the projection error. However, sometimes \\(x_{2}\\) is unobservable so the long regression is unavailable. This example of omitted variable bias is ubiquitous in applied econometrics. Ideally we would like to directly observe some regressors but in reality we do not have them at hand. We should be aware of the potential consequence when the data are not as ideal as we have wished. When only the short regression is available, in some cases we are able to sign the bias, meaning that we can argue whether \\(\\gamma_{1}\\) is bigger or smaller than \\(\\beta_{1}\\) based on our knowledge. 2.2 Causality 2.2.1 Structure and Identification Unlike physical laws such as Einstein’s mass–energy equivalence \\(E=mc^{2}\\) and Newton’s universal gravitation \\(F=Gm_{1}m_{2}/r^{2}\\), economic phenomena can rarely be summarized in such a minimalistic style. When using experiments to verify physical laws, scientists often manage to come up with smart design in which signal-to-noise ratio is so high that small disturbances are kept at a negligible level. On the contrary, economic laws do not fit a laboratory for experimentation. What is worse, the subjects in economic studies — human beings — are heterogeneous and with many features that are hard to control. People from distinctive cultural and family backgrounds respond to the same issue differently and researchers can do little to homogenize them. The signal-to-noise ratios in economic laws are often significantly lower than those of physical laws, mainly due to the lack of laboratory setting and the heterogeneous nature of the subjects. Educational return and the demand-supply system are two classical topics in econometrics. A person’s incomes is determined by too many random factors in the academic and career path that is impossible to exhaustively observe and control. The observable prices and quantities are outcomes of equilibrium so the demand and supply affect each other. Generations of thinkers have been debating the definitions of causality. In economics, an accepted definition is structural causality. Structural causality is a thought experiment. It assumes that there is a DGP that produces the observational data. If we can use data to recover the DGP or some features of the DGP, then we have learned causality or some implications of causality. A key issue to resolve before looking at the realized sample is identification. We say a model or DGP is identified if the each possible parameter of the model under consideration generates distinctive features of the observable data. A model is under-identified if more than one parameter in the model can generate exact the same features of the observable data. In other words, a model is under-identified if from the observable data we cannot trace back to a unique parameter in the model. A correctly specified model is the prerequisite for any discussion of identification. In reality, all models are wrong. Thus when talking about identification, we are indulged in an imaginary world. If in such a thought experiment we still cannot unique distinguish the true parameter of the data generating process, then identification fails. We cannot determine what is the true model no matter how large the sample is. 2.2.2 Treatment Effect We narrow down to the framework of the relationship between \\(y\\) and \\(x\\). One question of particular interest is treatment effect. The treatment effect is how much \\(y\\) will change if we change a variable of interest, say \\(d\\), by one unit while keeping all other variables (including the unobservable variables) the same. The Latin phrase ceteris paribus means “keep all other things constant.” During the 2020 covid-19 pandemic, Hong Kong’s unemployment rate rose to a high-level and consumption collapsed. In order to boost the economy, some Hong Kong residents were qualified in receiving 10,000 HKD cash allowance from the government. We are interested in learning how much does the 10,000 HKD allowance increase people’s consumption. For an individual, we imagine two parallel worlds: one with the cash allowance and one without. The difference of the consumption in the world with the allowance, denoted \\(Y\\left(1\\right)\\), and that in the world without the allowance, denoted \\(Y\\left(0\\right)\\), is the treatment effect of that particular person. This thought experiment is called the potential outcome framework. However, in reality one and only one scenario happens, which echoes the saying of ancient Greek philosopher Heraclitus (553 BC--475 BC) “You cannot step into the same river twice.” The individual treatment effect is not operational (operational means it can be computed from data at the population level), because one and only one outcome is realized. With many people available, we can define average treatment effect (ATE) as \\[ATE=E\\left[Y\\left(1\\right)-Y\\left(0\\right)\\right]=E\\left[Y\\left(1\\right)\\right]-E\\left[Y\\left(0\\right)\\right].\\] Notice that \\(E\\left[Y\\left(1\\right)\\right]\\) and \\(E\\left[Y\\left(0\\right)\\right]\\) are still not operational before we observe a companion variable \\[D=1\\left\\{ \\mbox{treatment received}\\right\\} .\\] Once each individual’s treatment status is observable, \\(E\\left[Y\\left(1\\right)|D=1\\right]\\) and \\(E\\left[Y\\left(0\\right)|D=0\\right]\\) are operational from the data. If the two potential outcomes \\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\) are independent of the assignment \\(D\\), then \\(E\\left[Y\\left(1\\right)\\right]=E\\left[Y\\left(1\\right)|D=1\\right]\\) and \\(E\\left[Y\\left(0\\right)\\right]=E\\left[Y\\left(0\\right)|D=0\\right]\\) so that ATE can be estimated from the data in an operational way as \\[ATE=E\\left[Y\\left(1\\right)|D=1\\right]-E\\left[Y\\left(0\\right)|D=0\\right].\\] Therefore, to evaluate ATE ideally we would like use a lottery to randomly decide that some people receive the treatment (treatment group, with \\(D=1\\)) and the others do not (control group, with \\(D=0\\)). When we have other control variables, we can also define a finer treatment effect conditional on \\(x\\): \\[ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|x\\right]-E\\left[Y\\left(0\\right)|x\\right].\\] ATE is the average effect in the population of individuals when we hypothetical give them the treatment, keeping all other factors \\(x\\) constant. If conditioning on \\(x\\), the treatment \\(D\\) is independent of \\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\), then ATE becomes operational: \\[ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|D=1,x\\right]-E\\left[Y\\left(0\\right)|D=0,x\\right]\\] The important condition \\(\\left(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\perp D\\right)|x\\) is called the conditional independence assumption (CIA). CIA is more plausible than full independence. Consider the example \\(Y\\left(1\\right)=x+u\\left(1\\right)\\), \\(Y\\left(0\\right)=x+u\\left(0\\right)\\) and \\(D=1\\left\\{ x+u_{d}\\geq0\\right\\}\\). If \\(\\left(\\left(u\\left(0\\right),u\\left(1\\right)\\right)\\perp u_{d}\\right)|x\\), then CIA is satisfied. Nevertheless \\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\) and \\(D\\) are statistically dependent, since \\(x\\) is involved in all random variables. 2.2.3 ATE and CEF In the previous section the treatment \\(D\\) is binary. Now we consider a continuous treatment \\(D\\). Suppose the DGP, or the structural model, is \\(Y=h\\left(D,x,u\\right)\\) where \\(D\\) and \\(x\\) are observable and \\(u\\) is unobservable. It is natural to define ATE with the continuous treatment (Hansen’s book Chapter 2.30 calls it average causal effect) as \\[ATE\\left(d,x\\right)=E\\left[\\lim_{\\Delta\\to0}\\frac{h\\left(d+\\Delta,x,u\\right)-h\\left(d,x,u\\right)}{\\Delta}\\right]=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right],\\] where the continuous differentiability of \\(h\\left(d,x,u\\right)\\) at \\(d\\) is implicitly assumed. Unlike the binary treatment case, here \\(d\\) explicitly shows up in \\(ATE\\left(d,x\\right)\\) because the effect can vary at different values of \\(d\\). ATE here is the average effect in the population of individuals if we hypothetical move \\(D\\) a tiny bit around \\(d\\), keeping all other factors \\(x\\) constant. In the previous sections, we focused on the CEF \\(m\\left(d,x\\right)\\), where \\(d\\) is added to \\(x\\) as an additional variable of interest. We did not intend to model the underlying economic mechanism \\(h\\left(D,x,u\\right)\\), which may be very complex. Can we learn the \\(ATE\\left(d,x\\right)\\) which bears the structural causal interpretation, from the mechanical \\(m\\left(d,x\\right)\\) which merely cares about best prediction? The answer is positive under CIA: \\(\\left(u\\perp D\\right)|x\\). \\[\\begin{aligned} \\frac{\\partial}{\\partial d}m\\left(d,x\\right) &amp; =\\frac{\\partial}{\\partial d}E\\left[y|d,x\\right]=\\frac{\\partial}{\\partial d}E\\left[h\\left(d,x,u\\right)|d,x\\right]=\\frac{\\partial}{\\partial d}\\int h\\left(d,x,u\\right)f\\left(u|d,x\\right)du\\\\ &amp; =\\int\\frac{\\partial}{\\partial d}\\left[h\\left(d,x,u\\right)f\\left(u|d,x\\right)\\right]du\\\\ &amp; =\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du+\\int h\\left(d,x,u\\right)\\left[\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)\\right]du,\\end{aligned}\\] where the second line implicitly assumes interchangeability between the integral and the partial derivative. Under CIA, \\(\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)=0\\) and the second term drops out. Thus \\[\\frac{\\partial}{\\partial d}m\\left(d,x\\right)=\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]=ATE\\left(d,x\\right).\\] This is an important result. It says that if CIA holds, we can learn the causal effect of \\(d\\) on \\(y\\) by the partial derivative of CEF conditional on \\(x\\). In particular, if we further assume a linear CEF \\(m\\left(d,x\\right)=\\beta_{d}d+\\beta_{x}&#39;x\\), then the causal effect is the coefficient \\(\\beta_{d}\\). CIA is the key condition that links the CEF and the causal effect. CIA is not an innocuous assumption. In applications, our causal results are credible only when we can convincing defend CIA. Let factories’ output be a Cobb-Douglas function \\(Y=AK^{\\alpha}L^{\\beta}\\), where the capital level \\(K\\) and labor \\(L\\) as well as the output \\(Y\\) is observable, while the “technology” \\(A\\) is unobservable. Take logarithm on both sides of the equation: \\[y=u+\\alpha k+\\beta l\\label{eq:causal}\\] where \\(y=\\log Y\\), \\(u=\\log A\\), \\(k=\\log K\\) and \\(l=\\log L\\). Suppose \\(\\begin{pmatrix}u\\\\ k\\\\ l \\end{pmatrix}\\sim N\\left(\\begin{pmatrix}1\\\\ 1\\\\ 1 \\end{pmatrix},\\begin{pmatrix}1 &amp; 0.5 &amp; 0\\\\ 0.5 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\right)\\) and \\(\\alpha=\\beta=1/2\\) make the true DGP. Here \\(u\\) and \\(k\\) are correlated, because factories of larger scale can afford robots to facilitate automation. What is the partial derivative of CEF when we use \\(k\\) as a treatment variable for a fixed labor level \\(l\\)? (Hint: the CEF is a linear function thanks to the joint normality.) Does it coincide with \\(\\alpha=1/2\\), the coefficient in the causal model (\\[eq:causal\\])? (Hint: No, because CIA is violated.) Sometimes applied researchers assume by brute force that \\(y=m\\left(d,x\\right)+u\\) is the DGP and \\(E\\left[u|d,x\\right]=0\\), where \\(d\\) is the variable of interest and \\(x\\) is the vector of other control variables. Under these assumptions, \\[ATE\\left(d,x\\right)=E\\left[\\frac{\\partial}{\\partial d}\\left(m\\left(d,x\\right)+u\\right)|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d}+\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d},\\] where the second equality holds if \\(\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=E\\left[\\frac{\\partial}{\\partial d}u|d,x\\right]\\). At a first glance, it seems that the mean independence assumption \\(E\\left[u|d,x\\right]=0\\), which is weaker than CIA, implies the equivalence between \\(ATE\\left(d,x\\right)\\) and \\(\\partial m\\left(d,x\\right)/\\partial d\\) here. However, such slight weakening is achieved by a very strong assumption that the DGP \\(h\\left(d,x,u\\right)\\) follows the additive separable form \\(m\\left(d,x\\right)+u\\). Without economic theory to defend the choice of the assumed DGP \\(y=m\\left(d,x\\right)+u\\), this is at best the reduced-form approach. The structural approach here models the economic mechanism, guided by economic theory. The reduced-form approach is convenient and can document stylized facts when suitable economic theory is not immediately available. There are constant debates about the pros and cons of the two approaches; see Journal of Economic Perspectives Vol. 24, No. 2 Spring 2010. In macroeconomics, the so-called Phillips curve, attributed to A.W. Phillips about the negative correlation between inflation and unemployment, is a stylized fact learned from the reduced-form approach. The Lucas critique (Lucas 1976) exposed its lack of microfoundation and advocated modeling deep parameters that are invariant to policy changes. The latter is a structural approach. Ironically, more than 40 years has passed since the Lucas critique, equations with little microfoundation still dominate the analytical apparatus of central bankers. 2.3 Summary In this lecture, we cover the conditional mean function and causality. When we are faced with a pair of random variable \\(\\left(y,x\\right)\\) drawn from some joint distribution, the CEF is the best predictor. When we go further into the structural causality about some treatment \\(d\\) to the dependent variable \\(y\\), under CIA we can find equivalence between ATE and the partial derivative of CEF. All analyses are conducted in population. We have not touched the sample yet. Historical notes: Regressions and conditional expectations are concepts from statistics and they are imported to econometrics in early time. Researchers at the Cowles Commission (now Cowles Foundation for Research in Economics) — Jacob Marschak (1898–1977), Tjalling Koopmans (1910–1985, Nobel Prize 1975), Trygve Haavelmo (1911–1999, Nobel Prize 1989) and their colleagues — were trailblazers of the econometric structural approach. The potential outcome framework is not peculiar to economics. It is widely used in other fields such as biostatistics and medical studies. It was initiated by Jerzy Neyman (1894–1981) and extended by Donald B. Rubin (1943– ), Professor of Statistics at Tsinghua University. Further reading: Lewbel (2019) offers a comprehensive summary of identification in econometrics. Accounting is an applied field with many claimed causal inference drawn from simple regressions; it is encouraging to hear Gow, Larcker, and Reiss (2016) to reflect causality in their practices. Zhentao Shi. Sep 17, 2020 References "],
["least-squares-linear-algebra.html", "3 Least Squares: Linear Algebra 3.1 Estimator 3.2 Subvector 3.3 Goodness of Fit 3.4 Summary", " 3 Least Squares: Linear Algebra 最小二乘法是计量经济学中最基本的估计方法，简单而透明。完全弄懂它，为未来研究更加复杂的线性估计量铺平道路。即使是非线性的估计量，它们的行为在一个点线性展开后，和线性估计量大同小异。在这一讲中，我们将会学习一系列最小二层估计的线性代数性质。 Ordinary least squares (OLS) is the most basic estimation technique in econometrics. It is simple and transparent. Understanding it thoroughly paves the way to study more sophisticated linear estimators. Moreover, many nonlinear estimators resemble the behavior of linear estimators in a neighborhood of the true value. In this lecture, we learn a series of facts from the linear algebra operation. 最小二乘估计是通用的统计方法，它不仅运用于计量经济学中也广泛的应用于其他统计分支。 但我们必须认清，最小二层古迹是一个纯粹的线性代数操作。它只能揭示相关性，并不能说明英国性。 只有经济理论，才能提供有关因果的假说。数据只能用来检验假说或者量化效果。 To manipulate Leopold Kronecker’s famous saying “God made the integers; all else is the works of man”, I would say “Gauss made OLS; all else is the works of applied researchers.” Popularity of OLS goes far beyond our dismal science. But be aware that OLS is a pure statistical or supervised machine learning method which reveals correlation instead of causality. Rather, economic theory hypothesizes causality while data are collected to test the theory or quantify the effect. 数学标记：\\(y_{i}\\)是标量。\\(x_{i}=\\left(x_{i1},\\ldots,x_{iK}\\right)&#39;\\)是一个\\(K\\times1\\) 向量。 \\(Y=\\left(y_{1},\\ldots,y_{n}\\right)&#39;\\)是一个\\(n\\times1\\)向量。 \\[ X=\\left[\\begin{array}{c} x_{1}&#39;\\\\ x_{2}&#39;\\\\ \\vdots\\\\ x_{n}&#39; \\end{array}\\right]=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1K}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2K}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{22} &amp; \\cdots &amp; x_{nK} \\end{array}\\right] \\] 是\\(n\\times K\\)矩阵。\\(I_{n}\\)是\\(n\\times n\\)单位矩阵. 3.1 Estimator 我们已经在前一讲中学习了线性映射模型。线性映射函数是？ As we have learned from the linear projection model, the projection coefficient \\(\\beta\\) in the regression \\[ \\begin{aligned}y &amp; =x&#39;\\beta+e\\end{aligned} \\] can be written as可以被写成 \\[ \\beta=\\left(E\\left[xx&#39;\\right]\\right)^{-1}E\\left[xy\\right].\\label{eq:pop_OLS} \\] 我们从\\(\\left(y,x\\right)\\)的联合分布中取出一对观测值，记作\\(\\left(y_{i},x_{i}\\right)\\) 重复n次之后，有n个观测值。，\\(i=1,\\ldots,n\\) repeated experiments. We possess a sample 我们就得到了一个样本\\(\\left(y_{i},x_{i}\\right)_{i=1}^{n}\\). 1.1*. Is \\(\\left(y_{i},x_{i}\\right)\\) 样本到底是谁寄的？还是固定的呢？ 我们在观测之前，随机变量的值是不确定的。 观测之后，他们的值就定下来了。 random or deterministic? Before we make the observation, they are treated as random variables whose realized values are uncertain. \\(\\left(y_{i},x_{i}\\right)\\) is treated as random when we talk about statistical properties — statistical properties of a fixed number is meaningless. After we make the observation, they become deterministic values which cannot vary anymore. 在实际操作中，我们手中只有一些给定的数字。（当然，现在的大数据也可以将文本，照片声音和图像处理成为数据，这些数据在计算机当中用零和一来表示。）我们把这些数据扔给计算机，让计算机给出一个结果。在统计上。我们认为这些数字是从一个概率分布上得出的思想实验。思想实验是一个学术用语，说白了，它就是一个故事。在公理体系的概率论当中，这个故事，在数学上是自洽的。但是数学本身是一个套套逻辑，而不是科学。概率模型的科学价值在于它在多大程度上能够毕竟事实的真相以及他是不是能够帮我们预测一些真相？在这门课的研究当中，我们假设数据来自于某种机制。我们把这种机制当成真相。在线性回归当中。Xy的联合分布就是真相。而我们想要研究线性映射系数beta。这个线性映射函数是真相的蕴含（implication）。 1.2. In reality, we have at hand fixed numbers (more recently, words, photos, audio clips, video clips, etc., which can all be represented in digital formats with 0 and 1) to feed into a computational operation, and the operation will return one or some numbers. All statistical interpretation about these numbers are drawn from the probabilistic thought experiments. A thought experiment* is an academic jargon for a story in plain language. Under the axiomatic approach of probability theory, such stories are mathematical consistent and coherent. But mathematics is a tautological system, not science. The scientific value of a probability model depends on how close it is to the truth or implications of the truth. In this course, we suppose that the data are generated from some mechanism, which is taken as the truth. In the linear regression model for example, the joint distribution of \\(\\left(y,x\\right)\\) is the truth, while we are interested in the linear projection coefficient \\(\\beta\\), which is an implication of the truth as in (\\[eq:pop_OLS\\]). The sample mean is a natural estimator of the population mean. Replace the population mean \\(E\\left[\\cdot\\right]\\) in (\\[eq:pop_OLS\\]) by the sample mean \\(\\frac{1}{n}\\sum_{i=1}^{n}\\cdot\\), and the resulting estimator is \\[\\begin{aligned} \\widehat{\\beta} &amp; =\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}\\\\ &amp; =\\left(\\frac{X&#39;X}{n}\\right)^{-1}\\frac{X&#39;y}{n}=\\left(X&#39;X\\right)^{-1}X&#39;y\\end{aligned}\\] if \\(X&#39;X\\) is invertible. This is one way to motivate the OLS estimator. Alternatively, we can derive the OLS estimator from minimizing the sum of squared residuals \\(\\sum_{i=1}^{n}\\left(y_{i}-x_{i}&#39;b\\right)^{2}\\), or equivalently \\[ Q\\left(b\\right)=\\frac{1}{2n}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}&#39;b\\right)^{2}=\\frac{1}{2n}\\left(Y-Xb\\right)&#39;\\left(Y-Xb\\right)=\\frac{1}{2n}\\left\\Vert Y-Xb\\right\\Vert ^{2}, \\] where the factor \\(\\frac{1}{2n}\\) is nonrandom and does not change the minimizer, and \\(\\left\\Vert \\cdot\\right\\Vert\\) is the Euclidean norm of a vector. Solve the first-order condition \\[ \\frac{\\partial}{\\partial b}Q\\left(b\\right)=\\left[\\begin{array}{c} \\partial Q\\left(b\\right)/\\partial b_{1}\\\\ \\partial Q\\left(b\\right)/\\partial b_{2}\\\\ \\vdots\\\\ \\partial Q\\left(b\\right)/\\partial b_{K} \\end{array}\\right]=-\\frac{1}{n}X&#39;\\left(Y-Xb\\right)=0. \\] This necessary condition for optimality gives exactly the same \\(\\widehat{\\beta}=\\left(X&#39;X\\right)^{-1}X&#39;y\\). Moreover, the second-order condition \\[ \\frac{\\partial^{2}}{\\partial b\\partial b&#39;}Q\\left(b\\right)=\\left[\\begin{array}{cccc} \\frac{\\partial^{2}}{\\partial b_{1}^{2}}Q\\left(b\\right) &amp; \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{2}}Q\\left(b\\right) &amp; \\cdots &amp; \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{1}}Q\\left(b\\right)\\\\ \\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{2}}Q\\left(b\\right) &amp; \\frac{\\partial^{2}}{\\partial b_{2}^{2}}Q\\left(b\\right) &amp; \\cdots &amp; \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{2}}Q\\left(b\\right)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{K}}Q\\left(b\\right) &amp; \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{K}}Q\\left(b\\right) &amp; \\cdots &amp; \\frac{\\partial^{2}}{\\partial b_{K}^{2}}Q\\left(b\\right) \\end{array}\\right]=\\frac{1}{n}X&#39;X \\] shows that \\(Q\\left(b\\right)\\) is convex in \\(b\\) due to the positive semi-definite matrix \\(X&#39;X/n\\). (The function \\(Q\\left(b\\right)\\) is strictly convex in \\(b\\) if \\(X&#39;X/n\\) is positive definite.) 1.3. In the derivation of OLS we presume that the \\(K\\) columns in \\(X\\) are linearly independent, which means there is no \\(K\\times1\\) vector \\(b\\) such that \\(b\\neq0_{K}\\) and \\(Xb=0_{n}\\). Linear independence of the columns implies \\(n\\geq K\\) and the invertibility of \\(X&#39;X/n\\). Linear independence is violated when some regressors are perfectly collinear, for example when we use dummy variables to indicate categorical variables and put all these categories into the regression. Modern econometrics software automatically detects and reports perfect collinearity. What is treacherous is nearly collinear*, meaning that the minimal eigenvalue of \\(X&#39;X/n\\) is close to 0, though not exactly equal to 0. We will talk about the consequence of near collinearity in the chapter of asymptotic theory. Here are some definitions and properties of the OLS estimator. Fitted value: \\(\\widehat{Y}=X\\widehat{\\beta}\\). Projection matrix: \\(P_{X}=X\\left(X&#39;X\\right)^{-1}X\\); Residual maker matrix: \\(M_{X}=I_{n}-P_{X}\\). \\(P_{X}X=X\\); \\(X&#39;P_{X}=X&#39;\\). \\(M_{X}X=0_{n\\times K}\\); \\(X&#39;M_{X}=0_{K\\times n}\\). \\(P_{X}M_{X}=M_{X}P_{X}=0_{n\\times n}\\). If \\(AA=A\\), we call it an idempotent matrix. Both \\(P_{X}\\) and \\(M_{X}\\) are idempotent. All eigenvalues of an idempotent matrix must be either 1 or 0. \\(\\mathrm{rank}\\left(P_{X}\\right)=K\\), and \\(\\mathrm{rank}\\left(M_{X}\\right)=n-K\\) (See the Appendix of this chapter). Residual: \\(\\widehat{e}=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=Y-X(X&#39;X)^{-1}X&#39;Y=(I_{n}-P_{X})Y=M_{X}Y=M_{X}\\left(X\\beta+e\\right)=M_{X}e\\). Notice \\(\\widehat{e}\\) and \\(e\\) are two different objects. \\(X&#39;\\widehat{e}=X&#39;M_{X}e=0_{K}\\). \\(\\sum_{i=1}^{n}\\widehat{e}_{i}=0\\) if \\(x_{i}\\) contains a constant. (Because \\(X&#39;\\widehat{e}=\\left[\\begin{array}{cccc} 1 &amp; 1 &amp; \\cdots &amp; 1\\\\ \\heartsuit &amp; \\heartsuit &amp; \\cdots &amp; \\heartsuit\\\\ \\cdots &amp; \\cdots &amp; \\ddots &amp; \\vdots\\\\ \\heartsuit &amp; \\heartsuit &amp; \\cdots &amp; \\heartsuit \\end{array}\\right]\\left[\\begin{array}{c} \\widehat{e}_{1}\\\\ \\widehat{e}_{2}\\\\ \\vdots\\\\ \\widehat{e}_{n} \\end{array}\\right]=\\left[\\begin{array}{c} 0\\\\ 0\\\\ \\vdots\\\\ 0 \\end{array}\\right]\\) , the the first row implies \\(\\sum_{i=1}^{n}\\widehat{e}_{i}=0\\). “\\(\\heartsuit\\)” indicates the entries irrelevant to our purpose.) The operation of OLS bears a natural geometric interpretation. Notice \\(\\mathcal{X}=\\left\\{ Xb:b\\in\\mathbb{R}^{K}\\right\\}\\) is the linear space spanned by the \\(K\\) columns of \\(X=\\left[X_{\\cdot1},\\ldots,X_{\\cdot K}\\right]\\), which is of \\(K\\)-dimension if the columns are linearly independent. The OLS estimator is the minimizer of \\(\\min_{b\\in\\mathbb{R}^{K}}\\left\\Vert Y-Xb\\right\\Vert\\) (Square the Euclidean norm or not does not change the minimizer because \\(a^{2}\\) is a monotonic transformation for \\(a\\geq0\\)). In other words, \\(X\\widehat{\\beta}\\) is the point in \\(\\mathcal{X}\\) such that it is the closest to the vector \\(Y\\) in terms of the Euclidean norm. The relationship \\(Y=X\\widehat{\\beta}+\\widehat{e}\\) decomposes \\(Y\\) into two orthogonal vectors \\(X\\widehat{\\beta}\\) and \\(\\widehat{e}\\) as \\(\\left\\langle X\\widehat{\\beta},\\widehat{e}\\right\\rangle =\\widehat{\\beta}&#39;X&#39;\\widehat{e}=0_{K}^{\\prime}\\), where \\(\\left\\langle \\cdot,\\cdot\\right\\rangle\\) is the inner product of two vectors. Therefore \\(X\\widehat{\\beta}\\) is the projection of \\(Y\\) onto \\(\\mathcal{X}\\), and \\(\\widehat{e}\\) is the corresponding projection residuals. The Pythagorean theorem implies \\[\\left\\Vert Y\\right\\Vert ^{2}=\\Vert X\\widehat{\\beta}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\\] ** 1.1**. Here is a simple simulated example to demonstrate the properties of OLS. Given \\(\\left(x_{1i},x_{2i},x_{3i},e_{i}\\right)^{\\prime}\\sim N\\left(0_{4},I_{4}\\right)\\), the dependent variable \\(y_{i}\\) is generated from \\[ y_{i}=0.5+2\\cdot x_{1i}-1\\cdot x_{2i}+e_{i} \\] The researcher does not know \\(x_{3i}\\) is redundant, and he regresses \\(y_{i}\\) on \\(\\left(1,x_{1i},x_{2i},x_{3i}\\right)\\). The estimated coefficient \\(\\widehat{\\beta}\\) is ( 0.315, 1.955, -0.852, 0.151). It is close to the true value, but not very accurate due to the small sample size. 3.2 Subvector The Frish-Waugh-Lovell (FWL) theorem is an algebraic fact about the formula of a subvector of the OLS estimator. To derive the FWL theorem we need to use the inverse of partitioned matrix. For a positive definite symmetric matrix \\(A=\\begin{pmatrix}A_{11} &amp; A_{12}\\\\ A_{12}&#39; &amp; A_{22} \\end{pmatrix}\\), the inverse can be written as \\[A^{-1}=\\begin{pmatrix}\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}&#39;\\right)^{-1} &amp; -\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}&#39;\\right)^{-1}A_{12}A_{22}^{-1}\\\\ -A_{22}^{-1}A_{12}&#39;\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}&#39;\\right)^{-1} &amp; \\left(A_{22}-A_{12}&#39;A_{11}^{-1}A_{12}\\right)^{-1} \\end{pmatrix}.\\] In our context of OLS estimator, let \\(X=\\left(\\begin{array}{cc} X_{1} &amp; X_{2}\\end{array}\\right)\\) \\[\\begin{aligned} \\begin{pmatrix}\\widehat{\\beta}_{1}\\\\ \\widehat{\\beta}_{2} \\end{pmatrix} &amp; =\\widehat{\\beta}=(X&#39;X)^{-1}X&#39;Y\\\\ &amp; =\\left(\\begin{pmatrix}X_{1}&#39;\\\\ X_{2}&#39; \\end{pmatrix}\\begin{pmatrix}X_{1} &amp; X_{2}\\end{pmatrix}\\right)^{-1}\\begin{pmatrix}X_{1}&#39;Y\\\\ X_{2}&#39;Y \\end{pmatrix}\\\\ &amp; =\\begin{pmatrix}X_{1}&#39;X_{1} &amp; X_{1}&#39;X_{2}\\\\ X_{2}&#39;X_{1} &amp; X_{2}&#39;X_{2} \\end{pmatrix}^{-1}\\begin{pmatrix}X_{1}&#39;Y\\\\ X_{2}&#39;Y \\end{pmatrix}\\\\ &amp; =\\begin{pmatrix}\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1} &amp; -\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1}X_{1}&#39;X_{2}\\left(X_{2}&#39;X_{2}\\right)^{-1}\\\\ \\heartsuit &amp; \\heartsuit \\end{pmatrix}\\begin{pmatrix}X_{1}&#39;Y\\\\ X_{2}&#39;Y \\end{pmatrix}.\\end{aligned}\\] The subvector \\[\\begin{aligned} \\widehat{\\beta}_{1} &amp; =\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1}X_{1}&#39;Y-\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1}X_{1}&#39;X_{2}\\left(X_{2}&#39;X_{2}\\right)^{-1}X_{2}&#39;Y\\\\ &amp; =\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1}X_{1}&#39;Y-\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1}X_{1}&#39;P_{X_{2}}Y\\\\ &amp; =\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1}\\left(X_{1}&#39;Y-X_{1}&#39;P_{X_{2}}Y\\right)\\\\ &amp; =\\left(X_{1}&#39;M_{X_{2}}&#39;X_{1}\\right)^{-1}X_{1}&#39;M_{X_{2}}Y.\\end{aligned}\\] Notice that \\(\\widehat{\\beta}_{1}\\) can be obtained by the following: Regress \\(Y\\) on \\(X_{2}\\), obtain the residual \\(\\tilde{Y}\\); Regress \\(X_{1}\\) on \\(X_{2}\\), obtain the residual \\(\\tilde{X}_{1}\\); Regress \\(\\tilde{Y}\\) on \\(\\tilde{X}_{1}\\), obtain OLS estimates \\(\\widehat{\\beta}_{1}\\). Similar derivation can also be carried out in the population linear projection. See Hansen (2020) \\[E\\] Chapter 2.22-23. 3.3 Goodness of Fit Consider the regression with the intercept \\(Y=X_{1}\\beta_{1}+\\beta_{2}+e.\\) The OLS estimator gives \\[Y=\\widehat{Y}+\\widehat{e}=\\left(X_{1}\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}\\right)+\\widehat{e}.\\label{eq:decomp_1}\\] Applying the FWL theorem with \\(X_{2}=\\iota\\), where \\(\\iota\\) (Greek letter, iota) is an \\(n\\times1\\) vector of 1’s. Then \\(M_{X_{2}}=M_{\\iota}=I_{n}-\\frac{1}{n}\\iota\\iota&#39;\\). Notice \\(M_{\\iota}\\) is the demeaner in that \\(M_{\\iota}z=z-\\bar{z}\\). It subtract the vector mean \\(\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\) from the original vector \\(z\\). The above three-step procedure becomes Regress \\(Y\\) on \\(\\iota\\), and the residual is \\(M_{\\iota}Y\\); Regress \\(X_{1}\\) on \\(\\iota\\), and the residual is \\(M_{\\iota}X_{1}\\); Regress \\(M_{\\iota}Y\\) on \\(M_{\\iota}X_{1}\\), and the OLS estimates is exactly the same as \\(\\widehat{\\beta}_{1}\\) in (\\[eq:decomp_1\\]). The last step gives the decomposition \\[M_{\\iota}Y=M_{\\iota}X_{1}\\widehat{\\beta}_{1}+\\tilde{e},\\label{eq:decomp_2}\\] and the Pythagorean theorem implies \\[\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}=\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\\] ** 1.1**. Show that \\(\\widehat{e}\\) in (\\[eq:decomp_1\\]) is exactly the same as \\(\\tilde{e}\\) in (\\[eq:decomp_2\\]). R-squared is a popular measure of goodness-of-fit in the linear regression. The (in-sample) R-squared \\[R^{2}=\\frac{\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}=1-\\frac{\\left\\Vert \\tilde{e}\\right\\Vert ^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}.\\] is well defined only when a constant is included in the regressors. ** 1.2**. Show \\[R^{2}=\\frac{\\widehat{Y}&#39;M_{\\iota}\\widehat{Y}}{Y&#39;M_{\\iota}Y}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{y_{i}}-\\overline{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}}\\] as in the decomposition (\\[eq:decomp_1\\]). In other words, it is the ratio between the sample variance of \\(\\widehat{Y}\\) and the sample variance of \\(Y\\). The magnitude of R-squared varies in different contexts. In macro models with the lagged dependent variables, it is not unusually to observe R-squared larger than 90%. In cross sectional regressions it is often below 20%. ** 1.3**. Consider a short regression “regress \\(y_{i}\\) on \\(x_{1i}\\)” and a long regression “regress \\(y_{i}\\) on \\(\\left(x_{1i},x_{2i}\\right)\\)”. Given the same dataset \\(\\left(Y,X_{1},X_{2}\\right)\\), show that the R-squared from the short regression is no larger than that from the long regression. In other words, we can always (weakly) increase \\(R^{2}\\) by adding more regressors. Conventionally we consider the regressions when the number of regressors \\(K\\) is much smaller the sample size \\(n\\). In the era of big data, it can happen that we have more potential regressors than the sample size. ** 1.4**. Show \\(R^{2}=1\\) when \\(K\\geq n\\). (When \\(K&gt;n\\), the matrix \\(X&#39;X\\) must be rank deficient. We can generalize the definition OLS fitting as any vector that minimizes \\(\\left\\Vert Y-Xb\\right\\Vert ^{2}\\) though the minimizer is not unique. ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## ALL 5 residuals are 0: no residual degrees of freedom! ## ## Coefficients: (2 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2229 NA NA NA ## X1 -0.6422 NA NA NA ## X2 0.1170 NA NA NA ## X3 1.1844 NA NA NA ## X4 0.5883 NA NA NA ## X5 NA NA NA NA ## X6 NA NA NA NA ## ## Residual standard error: NaN on 0 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: NaN ## F-statistic: NaN on 4 and 0 DF, p-value: NA With a new dataset \\(\\left(Y^{\\mathrm{new}},X^{\\mathrm{new}}\\right)\\), the out-of-sample (OOS) R-squared is \\[OOS\\ R^{2}=\\frac{\\widehat{\\beta}^{\\prime}X^{\\mathrm{new}\\prime}M_{\\iota}X^{\\mathrm{new}}\\widehat{\\beta}}{Y^{\\mathrm{new}\\prime}M_{\\iota}Y^{\\mathrm{new}}}.\\] OOS R-squred measures the goodness of fit in a new dataset given the coefficient estimated from the original data. In financial market shorter-term predictive models, a person may easily become rich if he can systematically achieve 2% OOS R-squared. 3.4 Summary The linear algebraic properties holds in finite sample no matter the data are taken as fixed numbers or random variables. The Gauss Markov theorem holds under two crucial assumptions: linear CEF and homoskedasticity. 高斯说，他在1795年就想出了最小二乘法的操作。他用三个点来预测了。矮行星位置。高斯没有在1809年之前把文章发表出来。而勒让德发表了同样的方法。今天，人们通常将最小二乘法归功于高斯。因为大家觉得像高斯这样的数学巨人没有必要。撒一个谎，来偷取勒让德的发现。 Historical notes: Carl Friedrich Gauss (1777–1855) claimed he had come up with the operation of OLS in 1795. With only three data points at hand, Gauss successfully applied his method to predict the location of the dwarf planet Ceres in 1801. While Gauss did not publish the work on OLS until 1809, Adrien-Marie Legendre (1752–1833) presented this method in 1805. Today people tend to attribute OLS to Gauss, assuming that a giant like Gauss had no need to tell a lie to steal Legendre’s discovery. "],
["least-squares-finite-sample-theory.html", "4 Least Squares: Finite Sample Theory 4.1 Maximum Likelihood 4.2 Likelihood Estimation for Regression 4.3 Finite Sample Distribution 4.4 Mean and Variance\\[mean-and-variance\\] 4.5 Gauss-Markov Theorem 4.6 Summary 4.7 Appendix", " 4 Least Squares: Finite Sample Theory We continue with properties of OLS. We will show that OLS coincides with the maximum likelihood estimator if the error term follows a normal distribution. We derive its finite-sample exact distribution which can be used for statistical inference. The Gauss-Markov theorem justifies the optimality of OLS under the classical assumptions. Suppose the data is generated from a parametric model. Statistical estimation looks for the unknown parameter from the observed data. A principle is an ideology about a proper way of estimation. Over the history of statistics, only a few principles are widely accepted. Among them Maximum Likelihood is the most important and fundamental. The maximum likelihood principle entails that the unknown parameter being found as the maximizer of the log-likelihood function. 4.1 Maximum Likelihood In this chapter, we first give an introduction of the maximum likelihood estimation. Consider a random sample of \\(Z=\\left(z_{1},z_{2},\\ldots,z_{n}\\right)\\) drawn from a parametric distribution with density \\(f_{z}\\left(z_{i};\\theta\\right)\\), where \\(z_{i}\\) is either a scalar random variable or a random vector. A parametric distribution is completely characterized by a finite-dimensional parameter \\(\\theta\\). We know that \\(\\theta\\) belongs to a parameter space \\(\\Theta\\). We use the data to estimate \\(\\theta\\). The log-likelihood of observing the entire sample \\(Z\\) is \\[L_{n}\\left(\\theta;Z\\right):=\\log\\left(\\prod_{i=1}^{n}f_{z}\\left(z_{i};\\theta\\right)\\right)=\\sum_{i=1}^{n}\\log f_{z}\\left(z_{i};\\theta\\right).\\label{eq:raw_likelihood}\\] In reality the sample \\(Z\\) is given and for each \\(\\theta\\in\\Theta\\) we can evaluate \\(L_{n}\\left(\\theta;Z\\right)\\). The maximum likelihood estimator is \\[\\widehat{\\theta}_{MLE}:=\\arg\\max_{\\theta\\in\\Theta}L_{n}\\left(\\theta;Z\\right).\\] Why maximizing the log-likelihood function is desirable? An intuitive explanation is that \\(\\widehat{\\theta}_{MLE}\\) makes observing \\(Z\\) the “most likely” in the entire parametric space. A more formal justification requires an explicitly defined distance. Suppose that the true parameter value that generates the data is \\(\\theta_{0}\\), so that the true distribution is \\(f_{z}\\left(z_{i};\\theta_{0}\\right)\\). Any generic point \\(\\theta\\in\\Theta\\) produces \\(f_{z}\\left(z_{i};\\theta\\right)\\). To measure their difference, we introduce the Kullback-Leibler divergence, or the Kullback-Leibler distance, defined as the logarithms of the expected log-likelihood ratio \\[\\begin{aligned} D_{f}\\left(\\theta_{0}\\Vert\\theta\\right) &amp; =D\\left(f_{z}\\left(z_{i};\\theta_{0}\\right)\\Vert f_{z}\\left(z_{i};\\theta\\right)\\right):=E_{\\theta_{0}}\\left[\\log\\frac{f_{z}\\left(z_{i};\\theta_{0}\\right)}{f_{z}\\left(z_{i};\\theta\\right)}\\right]\\\\ &amp; =E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{i};\\theta_{0}\\right)\\right]-E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{i};\\theta\\right)\\right].\\end{aligned}\\] We call it a “distance” because it is non-negative, although it is not symmetric in that \\(D_{f}\\left(\\theta_{1}\\Vert\\theta_{2}\\right)\\neq D_{f}\\left(\\theta_{2}\\Vert\\theta_{1}\\right)\\) and it does not satisfy the triangle inequality. To see \\(D_{f}\\left(\\theta_{0}\\Vert\\theta\\right)\\) is non-negative, notice that \\(-\\log\\left(\\cdot\\right)\\) is strictly convex and then by Jensen’s inequality \\[\\begin{aligned} E_{\\theta_{0}}\\left[\\log\\frac{f_{z}\\left(z_{i};\\theta_{0}\\right)}{f_{z}\\left(z_{i};\\theta\\right)}\\right] &amp; =E_{\\theta_{0}}\\left[-\\log\\frac{f_{z}\\left(z_{i};\\theta\\right)}{f_{z}\\left(z_{i};\\theta_{0}\\right)}\\right]\\geq-\\log\\left(E_{\\theta_{0}}\\left[\\frac{f_{z}\\left(z_{i};\\theta\\right)}{f_{z}\\left(z_{i};\\theta_{0}\\right)}\\right]\\right)\\\\ &amp; =-\\log\\left(\\int\\frac{f_{z}\\left(z_{i};\\theta\\right)}{f_{z}\\left(z_{i};\\theta_{0}\\right)}f_{z}\\left(z_{i};\\theta_{0}\\right)dz_{i}\\right)=-\\log\\left(\\int f_{z}\\left(z_{i};\\theta\\right)dz_{i}\\right)\\\\ &amp; =-\\log1=0,\\end{aligned}\\] where \\(\\int f_{z}\\left(z_{i};\\theta\\right)dz_{i}=1\\) for any pdf. The equality holds if and only if \\(f_{z}\\left(z_{i};\\theta\\right)=f_{z}\\left(z_{i};\\theta_{0}\\right)\\) almost everywhere. Furthermore, if there is a one-to-one mapping between \\(\\theta\\) and \\(f_{z}\\left(z_{i};\\theta\\right)\\) on \\(\\Theta\\) (identification), then \\(\\theta_{0}=\\arg\\min_{\\theta\\in\\Theta}D_{f}\\left(\\theta_{0}\\Vert\\theta\\right)\\) is the unique solution. In information theory, \\(-E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{i};\\theta_{0}\\right)\\right]\\) is the entropy of the continuous distribution of \\(f_{z}\\left(z_{i};\\theta_{0}\\right)\\). Entropy measures the uncertainty of a random variable; the larger is the value, the more chaotic is the random variable. The Kullback-Leibler distance is the relative entropy between the distribution \\(f_{z}\\left(z_{i};\\theta_{0}\\right)\\) and \\(f_{z}\\left(z_{i};\\theta\\right)\\). It measures the inefficiency of assuming that the distribution is \\(f_{z}\\left(z_{i};\\theta\\right)\\) when the true distribution is indeed \\(f_{z}\\left(z_{i};\\theta_{0}\\right)\\). (Cover and Thomas 2006, 19) Consider the Gaussian location model \\(z_{i}\\sim N\\left(\\mu,1\\right)\\), where \\(\\mu\\) is the unknown parameter to be estimated. The likelihood of observing \\(z_{i}\\) is \\(f_{z}\\left(z_{i};\\mu\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{i}-\\mu\\right)^{2}\\right)\\). The likelihood of observing the sample \\(Z\\) is \\[f_{Z}\\left(Z;\\mu\\right)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{i}-\\mu\\right)^{2}\\right)\\] and the log-likelihood is \\[L_{n}\\left(\\mu;Z\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}\\sum_{i=1}^{n}\\left(z_{i}-\\mu\\right)^{2}.\\] The (averaged) log-likelihood function for the \\(n\\) observations is \\[\\begin{aligned} \\ell_{n}\\left(\\mu\\right) &amp; =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2n}\\sum_{i=1}^{n}\\left(z_{i}-\\mu\\right)^{2}.\\end{aligned}\\] We work with the averaged log-likelihood \\(\\ell_{n}\\), instead of the (raw) log-likelihood \\(L_{n}\\), to make it directly comparable with the expected log density \\[\\begin{aligned} E_{\\mu_{0}}\\left[\\log f_{z}\\left(z;\\mu\\right)\\right] &amp; =E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\\\ &amp; =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{i}-\\mu\\right)^{2}\\right]\\\\ &amp; =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(\\left(z_{i}-\\mu_{0}\\right)+\\left(\\mu_{0}-\\mu\\right)\\right)^{2}\\right]\\\\ &amp; =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{i}-\\mu_{0}\\right)^{2}\\right]-E_{\\mu_{0}}\\left[z_{i}-\\mu_{0}\\right]\\left(\\mu_{0}-\\mu\\right)-\\frac{1}{2}\\left(\\mu_{0}-\\mu\\right)^{2}\\\\ &amp; =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}-\\frac{1}{2}\\left(\\mu-\\mu_{0}\\right)^{2}.\\end{aligned}\\] where the first equality holds because of random sampling. Obviously, \\(\\ell_{n}\\left(\\mu\\right)\\) is maximized at \\(\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\) while \\(E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\) is maximized at \\(\\mu=\\mu_{0}\\). The Kullback-Leibler divergence in this example is \\[D\\left(\\mu_{0}\\Vert\\mu\\right)=E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu_{0}\\right)\\right]-E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]=\\frac{1}{2}\\left(\\mu-\\mu_{0}\\right)^{2},\\] where \\(-E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu_{0}\\right)\\right]=\\frac{1}{2}\\left(\\log\\left(2\\pi\\right)+1\\right)\\) is the entropy of the normal distribution with unit variance. We use the following code to demonstrate the population log-likelihood \\(E\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\) when \\(\\mu_{0}=2\\) (solid line) and the 3 sample realizations when \\(n=4\\) (dashed lines). **there is a knitr** part 4.2 Likelihood Estimation for Regression Notation: \\(y_{i}\\) is a scalar, and \\(x_{i}=\\left(x_{i1},\\ldots,x_{iK}\\right)&#39;\\) is a \\(K\\times1\\) vector. \\(Y\\) is an \\(n\\times1\\) vector, and \\(X\\) is an \\(n\\times K\\) matrix. In this chapter we employ the classical statistical framework under restrictive distributional assumption \\[y_{i}|x_{i}\\sim N\\left(x_{i}&#39;\\beta,\\gamma\\right),\\label{eq:normal_yx}\\] where \\(\\gamma=\\sigma^{2}\\) to ease the differentiation. This assumption is equivalent to \\(e_{i}|x_{i}=\\left(y_{i}-x_{i}&#39;\\beta\\right)|x_{i}\\sim N\\left(0,\\gamma\\right)\\). Because the distribution of \\(e_{i}\\) is invariant to \\(x_{i}\\), the error term \\(e_{i}\\sim N\\left(0,\\gamma\\right)\\) and is statistically independent of \\(x_{i}\\). This is a very strong assumption. The likelihood of observing a pair \\(\\left(y_{i},x_{i}\\right)\\) is \\[\\begin{aligned} f_{yx}\\left(y_{i},x_{i}\\right) &amp; =f_{y|x}\\left(y_{i}|x_{i}\\right)f_{x}\\left(x\\right)\\\\ &amp; =\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}&#39;\\beta\\right)^{2}\\right)\\times f_{x}\\left(x\\right),\\end{aligned}\\] where \\(f_{yx}\\) is the joint pdf, \\(f_{y|x}\\) is the conditional pdf and \\(f_{x}\\) is the marginal pdf of \\(x\\), and the second equality holds under (\\[eq:normal\\_yx\\]). The likelihood of the random sample \\(\\left(y_{i},x_{i}\\right)_{i=1}^{n}\\) is \\[\\begin{aligned} \\prod_{i=1}^{n}f_{yx}\\left(y_{i},x_{i}\\right) &amp; =\\prod_{i=1}^{n}f_{y|x}\\left(y_{i}|x_{i}\\right)f_{x}\\left(x\\right)\\\\ &amp; =\\prod_{i=1}^{n}f_{y|x}\\left(y_{i}|x_{i}\\right)\\times\\prod_{i=1}^{n}f_{x}\\left(x\\right)\\\\ &amp; =\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}&#39;\\beta\\right)^{2}\\right)\\times\\prod_{i=1}^{n}f_{x}\\left(x\\right).\\end{aligned}\\] The parameters of interest \\(\\left(\\beta,\\gamma\\right)\\) are irrelevant to the second term \\(\\prod_{i=1}^{n}f_{x}\\left(x\\right)\\) for they appear only in the conditional likelihood \\[\\prod_{i=1}^{n}f_{y|x}\\left(y_{i}|x_{i}\\right)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}&#39;\\beta\\right)^{2}\\right).\\] We focus on the conditional likelihood. To facilitate derivation, we work with the (averaged) conditional log-likelihood function \\[\\ell_{n}\\left(\\beta,\\gamma\\right)=-\\frac{1}{2}\\log2\\pi-\\frac{1}{2}\\log\\gamma-\\frac{1}{2n\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}&#39;\\beta\\right)^{2},\\] for \\(\\log\\left(\\cdot\\right)\\) is a monotonic transformation that does not change the maximizer. The maximum likelihood estimator \\(\\widehat{\\beta}_{MLE}\\) can be found using the FOC: \\[\\begin{aligned} \\frac{\\partial}{\\partial\\beta}\\ell_{n}\\left(\\beta,\\gamma\\right) &amp; =\\frac{1}{n\\gamma}\\sum_{i=1}^{n}x_{i}\\left(y_{i}-x_{i}&#39;\\beta\\right)=0\\\\ \\frac{\\partial}{\\partial\\gamma}\\ell_{n}\\left(\\beta,\\gamma\\right) &amp; =-\\frac{1}{2\\gamma}+\\frac{1}{2n\\gamma^{2}}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}&#39;\\beta\\right)^{2}=0.\\end{aligned}\\] Rearranging the above equations in matrix form: \\[\\begin{aligned} X&#39;X\\beta &amp; =X&#39;Y\\\\ \\gamma &amp; =\\frac{1}{n}\\left(Y-X\\beta\\right)&#39;\\left(Y-X\\beta\\right).\\end{aligned}\\] We solve \\[\\begin{aligned} \\widehat{\\beta}_{MLE} &amp; =(X&#39;X)^{-1}X&#39;Y\\\\ \\widehat{\\gamma}_{\\mathrm{MLE}} &amp; =\\frac{1}{n}\\left(Y-X\\widehat{\\beta}_{MLE}\\right)&#39;\\left(Y-X\\widehat{\\beta}_{MLE}\\right)=\\widehat{e}&#39;\\widehat{e}/n\\end{aligned}\\] when \\(X&#39;X\\) is invertible. The MLE of the slope coefficient \\(\\widehat{\\beta}_{MLE}\\) coincides with the OLS estimator, and \\(\\widehat{e}\\) is exactly the OLS residual. 4.3 Finite Sample Distribution We can show the finite-sample exact distribution of \\(\\widehat{\\beta}\\) assuming the error term follows a Gaussian distribution. Finite sample distribution means that the distribution holds for any \\(n\\); it is in contrast to asymptotic distribution, which is a large sample approximation to the finite sample distribution. We first review some properties of a generic jointly normal random vector. \\[fact31\\] Let \\(z\\sim N\\left(\\mu,\\Omega\\right)\\) be an \\(l\\times1\\) random vector with a positive definite variance-covariance matrix \\(\\Omega\\). Let \\(A\\) be an \\(m\\times l\\) non-random matrix where \\(m\\leq l\\). Then \\(Az\\sim N\\left(A\\mu,A\\Omega A&#39;\\right)\\). \\[fact32\\]If \\(z\\sim N\\left(0,1\\right)\\), \\(w\\sim\\chi^{2}\\left(d\\right)\\) and \\(z\\) and \\(w\\) are independent. Then \\(\\frac{z}{\\sqrt{w/d}}\\sim t\\left(d\\right)\\). The OLS estimator \\[\\widehat{\\beta}=\\left(X&#39;X\\right)^{-1}X&#39;Y=\\left(X&#39;X\\right)^{-1}X&#39;\\left(X&#39;\\beta+e\\right)=\\beta+\\left(X&#39;X\\right)^{-1}X&#39;e,\\] and its conditional distribution can be written as \\[\\begin{aligned} \\widehat{\\beta}|X &amp; =\\beta+\\left(X&#39;X\\right)^{-1}X&#39;e|X\\\\ &amp; \\sim\\beta+\\left(X&#39;X\\right)^{-1}X&#39;\\cdot N\\left(0_{n},\\gamma I_{n}\\right)\\\\ &amp; \\sim N\\left(\\beta,\\gamma\\left(X&#39;X\\right)^{-1}X&#39;X\\left(X&#39;X\\right)^{-1}\\right)\\sim N\\left(\\beta,\\gamma\\left(X&#39;X\\right)^{-1}\\right)\\end{aligned}\\] by Fact \\[fact31\\]. The \\(k\\)-th element of the vector coefficient \\[\\widehat{\\beta}_{k}|X=\\eta_{k}&#39;\\widehat{\\beta}|X\\sim N\\left(\\beta_{k},\\gamma\\eta_{k}&#39;\\left(X&#39;X\\right)^{-1}\\eta_{k}\\right)\\sim N\\left(\\beta_{k},\\gamma\\left[\\left(X&#39;X\\right)^{-1}\\right]_{kk}\\right),\\] where \\(\\eta_{k}=\\left(1\\left\\{ l=k\\right\\} \\right)_{l=1,\\ldots,K}\\) is the selector of the \\(k\\)-th element. In reality, \\(\\sigma^{2}\\) is an unknown parameter, and \\[s^{2}=\\widehat{e}&#39;\\widehat{e}/\\left(n-K\\right)=e&#39;M_{X}e/\\left(n-K\\right)\\] is an unbiased estimator of \\(\\gamma\\). (Because \\[\\begin{aligned} E\\left[s^{2}|X\\right] &amp; =\\frac{1}{n-K}E\\left[e&#39;M_{X}e|X\\right]=\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[e&#39;M_{X}e|X\\right]\\right)\\\\ &amp; =\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[M_{X}ee&#39;|X\\right]\\right)=\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}E\\left[ee&#39;|X\\right]\\right)\\\\ &amp; =\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}\\gamma I_{n}\\right)=\\frac{\\gamma}{n-K}\\mathrm{trace}\\left(M_{X}\\right)=\\gamma\\end{aligned}\\] where we use the property of trace \\(\\mathrm{trace}\\left(AB\\right)=\\mathrm{trace}\\left(BA\\right)\\).) Under the null hypothesis \\(H_{0}:\\beta_{k}=\\beta_{k}^{*}\\), where \\(\\beta_{k}^{*}\\) is the hypothesized value we want to test. We can construct a \\(t\\)-statistic \\[T_{k}=\\frac{\\widehat{\\beta}_{k}-\\beta_{k}^{*}}{\\sqrt{s^{2}\\left[\\left(X&#39;X\\right)^{-1}\\right]_{kk}}},\\] which is infeasible is that sense that it can be directly computed from the data because there is no unknown object in this statistic. When the hypothesis is true, \\(\\beta_{k}=\\beta_{k}^{*}\\) and thus \\[\\begin{aligned} T_{k} &amp; =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{s^{2}\\left[\\left(X&#39;X\\right)^{-1}\\right]_{kk}}}\\nonumber \\\\ &amp; =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{\\sigma^{2}\\left[\\left(X&#39;X\\right)^{-1}\\right]_{kk}}}\\cdot\\frac{\\sqrt{\\sigma^{2}}}{\\sqrt{s^{2}}}\\nonumber \\\\ &amp; =\\frac{\\left(\\widehat{\\beta}_{k}-\\beta_{0,k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X&#39;X\\right)^{-1}\\right]_{kk}}}{\\sqrt{\\frac{e&#39;}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}},\\label{eq:t-stat}\\end{aligned}\\] where we introduce the population quantity \\(\\sigma^{2}\\) into the second equality to help derive the distribution of the numerator and the denominator of the last expression. The numerator \\[\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X&#39;X\\right)^{-1}\\right]_{kk}}\\sim N\\left(0,1\\right),\\] and the denominator \\(\\sqrt{\\frac{e&#39;}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}\\) follows \\(\\sqrt{\\frac{1}{n-K}\\chi^{2}\\left(n-K\\right)}\\). Moreover, because \\[\\begin{aligned} \\begin{bmatrix}\\widehat{\\beta}-\\beta\\\\ \\widehat{e} \\end{bmatrix} &amp; =\\begin{bmatrix}\\left(X&#39;X\\right)^{-1}X&#39;e\\\\ M_{X}e \\end{bmatrix}=\\begin{bmatrix}\\left(X&#39;X\\right)^{-1}X&#39;\\\\ M_{X} \\end{bmatrix}e\\\\ &amp; \\sim\\begin{bmatrix}\\left(X&#39;X\\right)^{-1}X&#39;\\\\ M_{X} \\end{bmatrix}\\cdot N\\left(0,\\gamma I_{n}\\right)\\sim N\\left(0,\\gamma\\begin{bmatrix}\\left(X&#39;X\\right)^{-1} &amp; 0\\\\ 0 &amp; M_{X} \\end{bmatrix}\\right)\\end{aligned}\\] are jointly normal with zero off-diagonal blocks, \\(\\left(\\widehat{\\beta}-\\beta\\right)\\) and \\(\\widehat{e}\\) are statistically independent. (This claim is true, although the covariance matrix of the \\(\\widehat{e}\\) is singular.) Given that \\(X\\) is viewed as if non-random, the numerator and the denominator of (\\[eq:t-stat\\]) are statistically independent as well is a function since the former is a function of \\(\\left(\\widehat{\\beta}-\\beta\\right)\\) and latter is a function of \\(\\widehat{e}\\). (Alternatively, the statistically independent can be verified by Basu’s theorem, See Appendix \\[subsec:Basu\\&#39;s-Theorem\\].) As a result, we conclude \\(T_{k}\\sim t\\left(n-K\\right)\\) by Fact \\[fact32\\]. This finite sample distribution allows us to conduct statistical inference. 4.4 Mean and Variance\\[mean-and-variance\\] Now we relax the normality assumption and statistical independence. Instead, we represent the regression model as \\(Y=X\\beta+e\\) and \\[\\begin{aligned} E[e|X] &amp; =0_{n}\\\\ \\mathrm{var}\\left[e|X\\right] &amp; =E\\left[ee&#39;|X\\right]=\\sigma^{2}I_{n}.\\end{aligned}\\] where the first condition is the mean independence assumption, and the second condition is the homoskedasticity assumption. These assumptions are about the first and second moments of \\(e_{i}\\) conditional on \\(x_{i}\\). Unlike the normality assumption, they do not restrict the distribution of \\(e_{i}\\). Unbiasedness: \\[\\begin{aligned} E\\left[\\widehat{\\beta}|X\\right] &amp; =E\\left[\\left(X&#39;X\\right)^{-1}XY|X\\right]=E\\left[\\left(X&#39;X\\right)^{-1}X\\left(X&#39;\\beta+e\\right)|X\\right]\\\\ &amp; =\\beta+\\left(X&#39;X\\right)^{-1}XE\\left[e|X\\right]=\\beta.\\end{aligned}\\] By the law of iterated expectations, the unconditional expectation \\(E\\left[\\widehat{\\beta}\\right]=E\\left[E\\left[\\widehat{\\beta}|X\\right]\\right]=\\beta.\\) Unbiasedness does not rely on homoskedasticity. Variance: \\[\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] &amp; =E\\left[\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)&#39;|X\\right]\\\\ &amp; =E\\left[\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\widehat{\\beta}-\\beta\\right)&#39;|X\\right]\\\\ &amp; =E\\left[\\left(X&#39;X\\right)^{-1}X&#39;ee&#39;X\\left(X&#39;X\\right)^{-1}|X\\right]\\\\ &amp; =\\left(X&#39;X\\right)^{-1}X&#39;E\\left[ee&#39;|X\\right]X\\left(X&#39;X\\right)^{-1} \\end{aligned}\\] where the second equality holds as Under the assumption of homoskedasticity, it can be simplified as \\[\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] &amp; =\\left(X&#39;X\\right)^{-1}X&#39;\\left(\\sigma^{2}I_{n}\\right)X\\left(X&#39;X\\right)^{-1}\\\\ &amp; =\\sigma^{2}\\left(X&#39;X\\right)^{-1}X&#39;I_{n}X\\left(X&#39;X\\right)^{-1}\\\\ &amp; =\\sigma^{2}\\left(X&#39;X\\right)^{-1}. \\end{aligned}\\] (Heteroskedasticity) If \\(e_{i}=x_{i}u_{i}\\), where \\(x_{i}\\) is a scalar random variable, \\(u_{i}\\) is statistically independent of \\(x_{i}\\), \\(E\\left[u_{i}\\right]=0\\) and \\(E\\left[u_{i}^{2}\\right]=\\sigma_{u}^{2}\\). Then \\(E\\left[e_{i}|x_{i}\\right]=E\\left[x_{i}u_{i}|x_{i}\\right]=x_{i}E\\left[u_{i}|x_{i}\\right]=0\\) but \\(E\\left[e_{i}^{2}|x_{i}\\right]=E\\left[x_{i}^{2}u_{i}^{2}|x_{i}\\right]=x_{i}^{2}E\\left[u_{i}^{2}|x_{i}\\right]=\\sigma_{u}^{2}x_{i}^{2}\\) is a function of \\(x_{i}\\). We say \\(e_{i}^{2}\\) is a heteroskedastic error. **knitr** It is important to notice that independently and identically distributed sample (iid) \\(\\left(y_{i},x_{i}\\right)\\) does not imply homoskedasticity. Homoskedasticity or heteroskedasticity is about the relationship between \\(\\left(x_{i},e_{i}=y_{i}-\\beta x\\right)\\) within an observation, whereas iid is about the relationship between \\(\\left(y_{i},x_{i}\\right)\\) and \\(\\left(y_{j},x_{j}\\right)\\) for \\(i\\neq j\\) across observations. 4.5 Gauss-Markov Theorem Gauss-Markov theorem is concerned about the optimality of OLS. It justifies OLS as the efficient estimator among all linear unbiased ones. Efficient here means that it enjoys the smallest variance in a family of estimators. We have shown that OLS is unbiased in that \\(E\\left[\\widehat{\\beta}\\right]=\\beta\\). There are numerous linearly unbiased estimators. For example, \\(\\left(Z&#39;X\\right)^{-1}Z&#39;y\\) for \\(z_{i}=x_{i}^{2}\\) is unbiased because \\(E\\left[\\left(Z&#39;X\\right)^{-1}Z&#39;y\\right]=E\\left[\\left(Z&#39;X\\right)^{-1}Z&#39;\\left(X\\beta+e\\right)\\right]=\\beta\\). We cannot say OLS is better than those other unbiased estimators because they are all unbiased — they are equally good at this aspect. We move to the second order property of variance: an estimator is better if its variance is smaller. For two generic random vectors \\(X\\) and \\(Y\\) of the same size, we say \\(X\\)’s variance is smaller or equal to \\(Y\\)’s variance if \\(\\left(\\Omega_{Y}-\\Omega_{X}\\right)\\) is a positive semi-definite matrix. The comparison is defined this way because for any non-zero constant vector \\(c\\), the variance of the linear combination of \\(X\\) \\[\\mathrm{var}\\left(c&#39;X\\right)=c&#39;\\Omega_{X}c\\leq c&#39;\\Omega_{Y}c=\\mathrm{var}\\left(c&#39;Y\\right)\\] is no bigger than the same linear combination of \\(Y\\). Let \\(\\tilde{\\beta}=A&#39;y\\) be a generic linear estimator, where \\(A\\) is any \\(n\\times K\\) functions of \\(X\\). As \\[E\\left[A&#39;y|X\\right]=E\\left[A&#39;\\left(X\\beta+e\\right)|X\\right]=A&#39;X\\beta.\\] So the linearity and unbiasedness of \\(\\tilde{\\beta}\\) implies \\(A&#39;X=I_{n}\\). Moreover, the variance \\[\\mbox{var}\\left(A&#39;y|X\\right)=E\\left[\\left(A&#39;y-\\beta\\right)\\left(A&#39;y-\\beta\\right)&#39;|X\\right]=E\\left[A&#39;ee&#39;A|X\\right]=\\sigma^{2}A&#39;A.\\] Let \\(C=A-X\\left(X&#39;X\\right)^{-1}.\\) \\[\\begin{aligned}A&#39;A-\\left(X&#39;X\\right)^{-1} &amp; =\\left(C+X\\left(X&#39;X\\right)^{-1}\\right)&#39;\\left(C+X\\left(X&#39;X\\right)^{-1}\\right)-\\left(X&#39;X\\right)^{-1}\\\\ &amp; =C&#39;C+\\left(X&#39;X\\right)^{-1}X&#39;C+C&#39;X\\left(X&#39;X\\right)^{-1}\\\\ &amp; =C&#39;C, \\end{aligned}\\] where the last equality follows as \\[\\left(X&#39;X\\right)^{-1}X&#39;C=\\left(X&#39;X\\right)^{-1}X&#39;\\left(A-X\\left(X&#39;X\\right)^{-1}\\right)=\\left(X&#39;X\\right)^{-1}-\\left(X&#39;X\\right)^{-1}=0.\\] Therefore \\(A&#39;A-\\left(X&#39;X\\right)^{-1}\\) is a positive semi-definite matrix. The variance of any \\(\\tilde{\\beta}\\) is no smaller than the OLS estimator \\(\\widehat{\\beta}\\). The above derivation shows OLS achieves the smallest variance among all linear unbiased estimators. Homoskedasticity is a restrictive assumption. Under homoskedasticity, \\(\\mathrm{var}\\left[\\widehat{\\beta}\\right]=\\sigma^{2}\\left(X&#39;X\\right)^{-1}\\). Popular estimator of \\(\\sigma^{2}\\) is the sample mean of the residuals \\(\\widehat{\\sigma}^{2}=\\frac{1}{n}\\widehat{e}&#39;\\widehat{e}\\) or the unbiased one \\(s^{2}=\\frac{1}{n-K}\\widehat{e}&#39;\\widehat{e}\\). Under heteroskedasticity, Gauss-Markov theorem does not apply. 4.6 Summary The exact distribution under the normality assumption of the error term is the classical statistical results. The Gauss Markov theorem holds under two crucial assumptions: linear CEF and homoskedasticity. Historical notes: MLE was promulgated and popularized by Ronald Fisher (1890–1962). He was a major contributor of the frequentist approach which dominates mathematical statistics today, and he sharply criticized the Bayesian approach. Fisher collected the iris flower dataset of 150 observations in his biological study in 1936, which can be displayed in R by typing iris. Fisher invented the many concepts in classical mathematical statistics, such as sufficient statistic, ancillary statistic, completeness, and exponential family, etc. Further reading: Phillips (1983) offered a comprehensive treatment of exact small sample theory in econometrics. After that, theoretical studies in econometrics swiftly shifted to large sample theory, which we will introduce in the next chapter. 4.7 Appendix 4.7.1 Joint Normal Distribution It is arguable that normal distribution is the most frequently encountered distribution in statistical inference, as it is the asymptotic distribution of many popular estimators. Moreover, it boasts some unique features that facilitates the calculation of objects of interest. This note summaries a few of them. An \\(n\\times1\\) random vector \\(Y\\) follows a joint normal distribution \\(N\\left(\\mu,\\Sigma\\right)\\), where \\(\\mu\\) is an \\(n\\times1\\) vector and \\(\\Sigma\\) is an \\(n\\times n\\) symmetric positive definite matrix. The probability density function is \\[f_{y}\\left(y\\right)=\\left(2\\pi\\right)^{-n/2}\\left(\\mathrm{det}\\left(\\Sigma\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2}\\left(y-\\mu\\right)&#39;\\Sigma^{-1}\\left(y-\\mu\\right)\\right)\\] where \\(\\mathrm{det}\\left(\\cdot\\right)\\) is the determinant of a matrix. The moment generating function is \\[M_{y}\\left(t\\right)=\\exp\\left(t&#39;\\mu+\\frac{1}{2}t&#39;\\Sigma t\\right).\\] We will discuss the relationship between two components of a random vector. To fix notation, \\[Y=\\left(\\begin{array}{c} Y_{1}\\\\ Y_{2} \\end{array}\\right)\\sim N\\left(\\left(\\begin{array}{c} \\mu_{1}\\\\ \\mu_{2} \\end{array}\\right),\\left(\\begin{array}{cc} \\Sigma_{11} &amp; \\Sigma_{12}\\\\ \\Sigma_{21} &amp; \\Sigma_{22} \\end{array}\\right)\\right)\\] where \\(Y_{1}\\) is an \\(m\\times1\\) vector, and \\(Y_{2}\\) is an \\(\\left(n-m\\right)\\times1\\) vector. \\(\\mu_{1}\\) and \\(\\mu_{2}\\) are the corresponding mean vectors, and \\(\\Sigma_{ij}\\), \\(j=1,2\\) are the corresponding variance and covariance matrices. From now on, we always maintain the assumption that \\(Y=\\left(Y_{1}&#39;,Y_{2}&#39;\\right)&#39;\\) is jointly normal. Fact \\[fact31\\] immediately implies a convenient feature of the normal distribution. Generally speaking, if we are given a joint pdf of two random variables and intend to find the marginal distribution of one random variables, we need to integrate out the other variable from the joint pdf. However, if the variables are jointly normal, the information of the other random variable is irrelevant to the marginal distribution of the random variable of interest. We only need to know the partial information of the part of interest, say the mean \\(\\mu_{1}\\) and the variance \\(\\Sigma_{11}\\) to decide the marginal distribution of \\(Y_{1}\\). \\[fact:marginal\\]The marginal distribution \\(Y_{1}\\sim N\\left(\\mu_{1},\\Sigma_{11}\\right)\\). This result is very convenient if we are interested in some component if an estimator, but not the entire vector of the estimator. For example, the OLS estimator of the linear regression model \\(y_{i}=x_{i}&#39;\\beta+e_{i}\\), under the classical assumption of (i) random sample; (ii) independence of \\(z_{i}\\) and \\(e_{i}\\); (iii) \\(e_{i}\\sim N\\left(0,\\gamma\\right)\\) is \\[\\widehat{\\beta}=\\left(X&#39;X\\right)^{-1}X&#39;y,\\] and the finite sample exact distribution of \\(\\widehat{\\beta}\\) is \\[\\left(\\widehat{\\beta}-\\beta\\right)|X\\sim N\\left(0,\\gamma\\left(X&#39;X\\right)^{-1}\\right)\\] If we are interested in the inference of only the \\(j\\)-th component of \\(\\beta_{0}^{\\left(j\\right)}\\), then from Fact \\[fact:marginal\\], \\[\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\left(X&#39;X\\right)_{kk}^{-1}\\sim N\\left(0,\\gamma\\right)\\] where \\(\\left[\\left(X&#39;X\\right)^{-1}\\right]_{kk}\\) is the \\(k\\)-th diagonal element of \\(\\left(X&#39;X\\right)^{-1}\\). The marginal distribution is independent of the other components. This saves us from integrating out the other components, which could be troublesome if the dimension of the vector is high. Generally, zero covariance of two random variables only indicates that they are uncorrelated, whereas full statistical independence is a much stronger requirement. However, if \\(Y_{1}\\) and \\(Y_{2}\\) are jointly normal, then zero covariance is equivalent to full independence. If \\(\\Sigma_{12}=0\\), then \\(Y_{1}\\) and \\(Y_{2}\\) are independent. If \\(\\Sigma\\) is invertible, then \\(Y&#39;\\Sigma^{-1}Y\\sim\\chi^{2}\\left(\\mathrm{rank}\\left(\\Sigma\\right)\\right)\\). The last result, which is useful in linear regression, is that if \\(Y_{1}\\) and \\(Y_{2}\\) are jointly normal, the conditional distribution of \\(Y_{1}\\) on \\(Y_{2}\\) is still jointly normal, with the mean and variance specified as in the following fact. \\(Y_{1}|Y_{2}\\sim N\\left(\\mu_{1}+\\Sigma_{12}\\Sigma_{22}^{-1}\\left(Y_{2}-\\mu_{2}\\right),\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\right)\\). 4.7.2 Basu’s Theorem* [\\[subsec:Basu\\&#39;s-Theorem\\]]{#subsec:Basu’s-Theorem label=“subsec:Basu’s-Theorem”} \\(Y=\\left(y_{1},\\ldots,y_{n}\\right)\\) consists of \\(n\\) iid observations. We say \\(T\\left(Y\\right)\\) is a sufficient statistic for a parameter \\(\\theta\\) if the conditional probability \\(f\\left(Y|T\\left(Y\\right)\\right)\\) does not depend on \\(\\theta\\). We say \\(S\\left(Y\\right)\\) is an ancillary statistic for \\(\\theta\\) if its distribution does not depend on \\(\\theta\\). Basu’s theorem says that a complete sufficient statistic is statistically independent from any ancillary statistic. Sufficient statistic is closely related to the exponential family in classical mathematical statistics. A parametric distribution indexed by \\(\\theta\\) is a member of the exponential family is its PDF can be written as \\[f\\left(Y|\\theta\\right)=h\\left(Y\\right)g\\left(\\theta\\right)\\exp\\left(\\eta\\left(\\theta\\right)&#39;T\\left(Y\\right)\\right),\\] where \\(g\\left(\\theta\\right)\\) and \\(\\eta\\left(\\theta\\right)\\) are functions depend, only on \\(\\theta\\) and \\(h\\left(Y\\right)\\) and \\(T\\left(Y\\right)\\) are functions depend only on \\(Y\\). (Univariate Gaussian location model.) For a normal distribution \\(y_{i}\\sim N\\left(\\mu,\\gamma\\right)\\) with known \\(\\gamma\\) and unknown \\(\\mu\\), the sample mean \\(\\bar{y}\\) is the sufficient statistic and the sample standard deviation \\(s^{2}\\) is an ancillary statistic. We first verify that the sample mean \\(\\bar{y}=n^{-1}\\sum_{i=1}^{n}y_{i}\\) is a sufficient statistic for \\(\\mu\\). Notice that the joint density of \\(Y\\) is \\[\\begin{aligned} f\\left(Y\\right) &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\\right)\\\\ &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(\\left(y_{i}-\\bar{y}\\right)+\\left(\\bar{y}-\\mu\\right)\\right)^{2}\\right)\\\\ &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(\\left(y_{i}-\\bar{y}\\right)^{2}+2\\left(y_{i}-\\bar{y}\\right)\\left(\\bar{y}-\\mu\\right)+\\left(\\bar{y}-\\mu\\right)^{2}\\right)\\right)\\\\ &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\\right)\\exp\\left(-\\frac{n}{2\\gamma}\\left(\\bar{y}-\\mu\\right)^{2}\\right).\\end{aligned}\\] Because \\(\\bar{y}\\sim N\\left(\\mu,\\gamma/n\\right),\\) the marginal density is \\[f\\left(\\bar{y}\\right)=\\left(2\\pi\\gamma/n\\right)^{-1/2}\\exp\\left(-\\frac{n}{2\\gamma}\\left(\\bar{y}-\\mu\\right)^{2}\\right).\\] For \\(\\bar{y}\\) is a statistic of \\(Y\\), we have \\(f\\left(Y,\\bar{y}\\right)=f\\left(Y\\right)\\). The conditional density is \\[f\\left(Y|\\bar{y}\\right)=\\frac{f\\left(Y,\\bar{y}\\right)}{f\\left(\\bar{y}\\right)}=\\frac{f\\left(Y\\right)}{f\\left(\\bar{y}\\right)}=\\sqrt{n}\\left(2\\pi\\gamma\\right)^{-\\frac{n-1}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\\right)\\] is independent of \\(\\mu\\), and thus \\(\\bar{y}\\) is a sufficient statistic for \\(\\mu\\). In the meantime, the sample standard deviation \\(s^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\) is an ancillary statistic for \\(\\mu\\) , because the distribution of \\(s^{2}\\) does not depend on \\(\\mu.\\) The normal distribution with known \\(\\sigma^{2}\\) and unknown \\(\\mu\\) belongs to the exponential family in view of the decomposition \\[\\begin{aligned} f(Y) &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\\right)\\\\ &amp; =\\underbrace{\\exp\\left(-\\sum_{i=1}^{n}\\frac{y_{i}^{2}}{2\\gamma}\\right)}_{h\\left(Y\\right)}\\cdot\\underbrace{\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{n}{2\\gamma}\\mu^{2}\\right)}_{g\\left(\\theta\\right)}\\cdot\\underbrace{\\exp\\left(\\frac{\\mu}{2\\gamma}n\\bar{y}\\right)}_{\\exp\\left(\\eta\\left(\\theta\\right)&#39;T\\left(Y\\right)\\right)}.\\end{aligned}\\] The exponential family is a class of distributions with the special functional form which is convenient for deriving sufficient statistics as well as other desirable properties in classical mathematical statistics. (Conditional Gaussian location model.) If \\(y_{i}\\sim N\\left(x_{i}\\beta,\\gamma\\right)\\) with known \\(\\gamma\\) and unknown \\(\\beta\\), We verify that the sample mean \\(\\widehat{\\beta}\\) is a sufficient statistic for \\(\\beta\\). Notice that the joint density of \\(Y\\) given \\(X\\) is \\[\\begin{aligned} f\\left(Y|X\\right) &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\\right)\\\\ &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(Y-X\\widehat{\\beta}\\right)&#39;\\left(Y-X\\widehat{\\beta}\\right)\\right)\\exp\\left(-\\frac{1}{2\\gamma}\\left(\\widehat{\\beta}-\\beta\\right)&#39;X&#39;X\\left(\\widehat{\\beta}-\\beta\\right)\\right).\\end{aligned}\\] Because \\(\\widehat{\\beta}\\sim N\\left(\\beta,\\gamma\\left(X&#39;X\\right)^{-1}\\right),\\) the marginal density is \\[f\\left(\\widehat{\\beta}|X\\right)=\\left(2\\pi\\gamma\\right)^{-\\frac{K}{2}}\\left(\\mathrm{det}\\left(\\left(X&#39;X\\right)^{-1}\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2\\gamma}\\left(\\widehat{\\beta}-\\beta\\right)&#39;X&#39;X\\left(\\widehat{\\beta}-\\beta\\right)\\right).\\] The conditional density is \\[\\begin{aligned} f\\left(Y|\\widehat{\\beta},X\\right) &amp; =\\frac{f\\left(Y|X\\right)}{f\\left(\\widehat{\\beta}|X\\right)}\\\\ &amp; =\\left(2\\pi\\gamma\\right)^{-\\frac{n-K}{2}}\\left(\\mathrm{det}\\left(\\left(X&#39;X\\right)^{-1}\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2\\gamma}\\left(Y-X\\widehat{\\beta}\\right)&#39;\\left(Y-X\\widehat{\\beta}\\right)\\right)\\end{aligned}\\] is independent of \\(\\beta\\), and thus \\(\\widehat{\\beta}\\) is a sufficient statistic for \\(\\beta\\). In the meantime, the sample standard deviation \\(s^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}\\widehat{\\beta}\\right)\\) is an ancillary statistic for \\(\\beta\\) , because the distribution of \\(s^{2}\\) does not depend on \\(\\beta.\\) Zhentao Shi. Oct 10. References "],
["basic-asymptotic-theory.html", "5 Basic Asymptotic Theory 5.1 Modes of Convergence 5.2 Law of Large Numbers 5.3 Central Limit Theorem 5.4 Tools for Transformations 5.5 Summary", " 5 Basic Asymptotic Theory Our universe, though enormous, consists of fewer than \\(10^{82}\\) atoms, which is a finite number. However, mathematical ideas are not bounded by secular realities. Asymptotic theory is about behaviors of statistics when the sample size is arbitrarily large up to infinity. It is a set of approximation techniques to simplify complicated finite-sample analysis. Asymptotic theory is the cornerstone of modern econometrics. It sheds lights on estimation and inference procedures under much more general conditions than what are covered by exact finite sample theory. Nevertheless, we always have at hand a finite sample, and mostly it is difficult to increase the sample size in reality. Asymptotic theory rarely answers “how large is large”, and we must be cautious about the treacherous landscape of asymptopia. In the era of big data, albeit the sheer size of data balloons dramatically, we build more sophisticated models to better capture heterogeneity in the data. Large sample is a relative notion to the complexity of the model and underlying (in)dependence structure of the data. Both the classical parametric approach, which is based on hard-to-verify parametric assumptions, and the asymptotic approach, which is predicated on imaginary infinite sequences, deviate from the reality. Which approach is more constructive can only be judged case by case. The prevalence of asymptotic theory is its mathematical amenability and generality. The law of evolution elevates asymptotic theory to the throne of mathematical statistics of our time. 5.1 Modes of Convergence We first review what is convergence for a non-random sequence, which you learned in high school. Let \\(z_{1},z_{2},\\ldots\\) be an infinite sequence of non-random variables. Convergence of this non-random sequence means that for any \\(\\varepsilon&gt;0\\), there exists an \\(N\\left(\\varepsilon\\right)\\) such that for all \\(n&gt;N\\left(\\varepsilon\\right)\\), we have \\(\\left|z_{n}-z\\right|&lt;\\varepsilon\\). We say \\(z\\) is the limit of \\(z_{n}\\), and write \\(z_{n}\\to z\\) or \\(\\lim_{n\\to\\infty}z_{n}=z\\). Instead of a deterministic sequence, we are interested in the convergence of a sequence of random variables. Since a random variable is “random” thanks to the induced probability measure by the measurable function, we must be clear what convergence means. Several modes of convergence are widely used. We say a sequence of random variables \\(\\left(z_{n}\\right)\\) converges in probability to \\(z\\), where \\(z\\) can be either a random variable or a non-random constant, if for any \\(\\varepsilon&gt;0\\), the probability \\(P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|&lt;\\varepsilon\\right\\} \\to1\\) (or equivalently \\(P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|\\geq\\varepsilon\\right\\} \\to0\\)) as \\(n\\to\\infty\\). We can write \\(z_{n}\\stackrel{p}{\\to}z\\) or \\(\\mathrm{plim}_{n\\to\\infty}z_{n}=z\\). A sequence of random variables \\(\\left(z_{n}\\right)\\) converges in squared-mean to \\(z\\), where \\(z\\) can be either a random variable or a non-random constant, if \\(E\\left[\\left(z_{n}-z\\right)^{2}\\right]\\to0.\\) It is denoted as \\(z_{n}\\stackrel{m.s.}{\\to}z\\). In these definitions either \\(P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|&gt;\\varepsilon\\right\\}\\) or \\(E\\left[\\left(z_{n}-z\\right)^{2}\\right]\\) is a non-random quantity, and it converges to 0 as a non-random sequence. Squared-mean convergence is stronger than convergence in probability. That is, \\(z_{n}\\stackrel{m.s.}{\\to}z\\) implies \\(z_{n}\\stackrel{p}{\\to}z\\) but the converse is untrue. Here is an example. \\[eg:in\\_p\\_in\\_ms\\]\\((z_{n})\\) is a sequence of binary random variables: \\(z_{n}=\\sqrt{n}\\) with probability \\(1/n\\), and \\(z_{n}=0\\) with probability \\(1-1/n\\). Then \\(z_{n}\\stackrel{p}{\\to}0\\) but \\(z_{n}\\stackrel{m.s.}{\\nrightarrow}0\\). To verify these claims, notice that for any \\(\\varepsilon&gt;0\\), we have \\(P\\left(\\omega:\\left|z_{n}\\left(\\omega\\right)-0\\right|&lt;\\varepsilon\\right)=P\\left(\\omega:z_{n}\\left(\\omega\\right)=0\\right)=1-1/n\\rightarrow1\\) and thereby \\(z_{n}\\stackrel{p}{\\to}0\\). On the other hand, \\(E\\left[\\left(z_{n}-0\\right)^{2}\\right]=n\\cdot1/n+0\\cdot(1-1/n)=1\\nrightarrow0,\\) so \\(z_{n}\\stackrel{m.s.}{\\nrightarrow}0\\). Example \\[eg:in\\_p\\_in\\_ms\\] highlights the difference between the two modes of convergence. Convergence in probability does not count what happens on a subset in the sample space of small probability. Squared-mean convergence deals with the average over the entire probability space. If a random variable can take a wild value, with small probability though, it may blow away the squared-mean convergence. On the contrary, such irregularity does not undermine convergence in probability. Both convergence in probability and squared-mean convergence are about convergence of random variables to a target random variable or constant. That is, the distribution of \\(z_{n}-z\\) is concentrated around 0 as \\(n\\to\\infty\\). Instead, convergence in distribution is about the convergence of CDF, but not the random variable. Let \\(F_{z_{n}}\\left(\\cdot\\right)\\) be the CDF of \\(z_{n}\\) and \\(F_{z}\\left(\\cdot\\right)\\) be the CDF of \\(z\\). We say a sequence of random variables \\(\\left(z_{n}\\right)\\) converges in distribution to a random variable \\(z\\) if \\(F_{z_{n}}\\left(a\\right)\\to F_{z}\\left(a\\right)\\) as \\(n\\to\\infty\\) at each point \\(a\\in\\mathbb{R}\\) such that where \\(F_{z}\\left(\\cdot\\right)\\) is continuous. We write \\(z_{n}\\stackrel{d}{\\to}z\\). Convergence in distribution is the weakest mode. If \\(z_{n}\\stackrel{p}{\\to}z\\), then \\(z_{n}\\stackrel{d}{\\to}z\\). The converse is not true in general, unless \\(z\\) is a non-random constant (A constant \\(z\\) can be viewed as a degenerate random variables, with a corresponding “CDF” \\(F_{z}\\left(\\cdot\\right)=1\\left\\{ \\cdot\\geq z\\right\\}\\). Let \\(x\\sim N\\left(0,1\\right)\\). If \\(z_{n}=x+1/n\\), then \\(z_{n}\\stackrel{p}{\\to}x\\) and of course \\(z_{n}\\stackrel{d}{\\to}x\\). However, if \\(z_{n}=-x+1/n\\), or \\(z_{n}=y+1/n\\) where \\(y\\sim N\\left(0,1\\right)\\) is independent of \\(x\\), then \\(z_{n}\\stackrel{d}{\\to}x\\) but \\(z_{n}\\stackrel{p}{\\nrightarrow}x\\). \\((z_{n})\\) is a sequence of binary random variables: \\(z_{n}=n\\) with probability \\(1/\\sqrt{n}\\), and \\(z_{n}=0\\) with probability \\(1-1/\\sqrt{n}\\). Then \\(z_{n}\\stackrel{d}{\\to}z=0.\\) Because \\[F_{z_{n}}\\left(a\\right)=\\begin{cases} 0 &amp; a&lt;0\\\\ 1-1/\\sqrt{n} &amp; 0\\leq a\\leq n\\\\ 1 &amp; a\\geq n \\end{cases}.\\] \\(F_{z}\\left(a\\right)=\\begin{cases} 0, &amp; a&lt;0\\\\ 1 &amp; a\\geq0 \\end{cases}\\). It is easy to verify that \\(F_{z_{n}}\\left(a\\right)\\) converges to \\(F_{z}\\left(a\\right)\\) pointwisely on each point in \\(\\left(-\\infty,0\\right)\\cup\\left(0,+\\infty\\right)\\), where \\(F_{z}\\left(a\\right)\\) is continuous. So far we have talked about convergence of scalar variables. These three modes of converges can be easily generalized to random vectors. In particular, the Cramer-Wold device collapses a random vector into a random vector via arbitrary linear combination. We say a sequence of \\(K\\)-dimensional random vectors \\(\\left(z_{n}\\right)\\) converge in distribution to \\(z\\) if \\(\\lambda&#39;z_{n}\\stackrel{d}{\\to}\\lambda&#39;z\\) for any \\(\\lambda\\in\\mathbb{R}^{K}\\) and \\(\\left\\Vert \\lambda\\right\\Vert _{2}=1.\\) 5.2 Law of Large Numbers (Weak) law of large numbers (LLN) is a collection of statements about convergence in probability of the sample average to its population counterpart. The basic form of LLN is: \\[\\frac{1}{n}\\sum_{i=1}^{n}(z_{i}-E[z_{i}])\\stackrel{p}{\\to}0\\] as \\(n\\to\\infty\\). Various versions of LLN work under different assumptions about some features and/or dependence of the underlying random variables. 5.2.1 Cherbyshev LLN We illustrate LLN by the simple example of Chebyshev LLN, which can be proved by elementary calculation. It utilizes the Chebyshev inequality. Chebyshev inequality: If a random variable \\(x\\) has a finite second moment \\(E\\left[x^{2}\\right]&lt;\\infty\\), then we have \\(P\\left\\{ \\left|x\\right|&gt;\\varepsilon\\right\\} \\leq E\\left[x^{2}\\right]/\\varepsilon^{2}\\) for any constant \\(\\varepsilon&gt;0\\). Show that if \\(r_{2}\\geq r_{1}\\geq1\\), then \\(E\\left[\\left|x\\right|^{r_{2}}\\right]&lt;\\infty\\) implies \\(E\\left[\\left|x\\right|^{r_{1}}\\right]&lt;\\infty.\\) (Hint: use Holder’s inequality.) The Chebyshev inequality is a special case of the Markov inequality. Markov inequality: If a random variable \\(x\\) has a finite \\(r\\)-th absolute moment \\(E\\left[\\left|x\\right|^{r}\\right]&lt;\\infty\\) for some \\(r\\ge1\\), then we have \\(P\\left\\{ \\left|x\\right|&gt;\\varepsilon\\right\\} \\leq E\\left[\\left|x\\right|^{r}\\right]/\\varepsilon^{r}\\) any constant \\(\\varepsilon&gt;0\\). It is easy to verify the Markov inequality. \\[\\begin{aligned}E\\left[\\left|x\\right|^{r}\\right] &amp; =\\int_{\\left|x\\right|&gt;\\varepsilon}\\left|x\\right|^{r}dF_{X}+\\int_{\\left|x\\right|\\leq\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\ &amp; \\geq\\int_{\\left|x\\right|&gt;\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\ &amp; \\geq\\varepsilon^{r}\\int_{\\left|x\\right|&gt;\\varepsilon}dF_{X}=\\varepsilon^{r}P\\left\\{ \\left|x\\right|&gt;\\varepsilon\\right\\} . \\end{aligned}\\] Rearrange the above inequality and we obtain the Markov inequality. Let the partial sum \\(S_{n}=\\sum_{i=1}^{n}x_{i}\\), where \\(\\mu_{i}=E\\left[x_{i}\\right]\\) and \\(\\sigma_{i}^{2}=\\mathrm{var}\\left[x_{i}\\right]\\). We apply the Chebyshev inequality to the sample mean \\(z_{n}=\\overline{x}-\\bar{\\mu}=n^{-1}\\left(S_{n}-E\\left[S_{n}\\right]\\right)\\). \\[\\begin{aligned} P\\left\\{ \\left|z_{n}\\right|\\geq\\varepsilon\\right\\} &amp; =P\\left\\{ n^{-1}\\left|S_{n}-E\\left[S_{n}\\right]\\right|\\geq\\varepsilon\\right\\} \\nonumber \\\\ &amp; \\leq E\\left[\\left(n^{-1}\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)\\right)^{2}\\right]/\\varepsilon^{2}\\nonumber \\\\ &amp; =\\left(n\\varepsilon\\right)^{-2}\\left\\{ E\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)^{2}\\right]+\\sum_{i=1}^{n}\\sum_{j\\neq i}E\\left[\\left(x_{i}-\\mu_{i}\\right)\\left(x_{j}-\\mu_{j}\\right)\\right]\\right\\} \\nonumber \\\\ &amp; =\\left(n\\varepsilon\\right)^{-2}\\left\\{ \\sum_{i=1}^{n}\\mathrm{var}\\left(x_{i}\\right)+\\sum_{i=1}^{n}\\sum_{j\\neq i}\\mathrm{cov}\\left(x_{i},x_{j}\\right)\\right\\} .\\label{eq:cheby_mean}\\end{aligned}\\] Convergence in probability holds if the right-hand side shrinks to 0 as \\(n\\to\\infty\\). For example, If \\(x_{1},\\ldots,x_{n}\\) are iid with \\(\\mathrm{var}\\left(x_{1}\\right)=\\sigma^{2}\\), then the RHS of (\\[eq:cheby\\_mean\\]) is \\(\\left(n\\varepsilon\\right)^{-2}\\left(n\\sigma^{2}\\right)=o\\left(n^{-1}\\right)\\to0\\). This result gives the Chebyshev LLN: Chebyshev LLN: If \\(\\left(z_{1},\\ldots,z_{n}\\right)\\) is a sample of iid observations, \\(E\\left[z_{1}\\right]=\\mu\\) , and \\(\\sigma^{2}=\\mathrm{var}\\left[z_{1}\\right]&lt;\\infty\\) exists, then \\(\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\stackrel{p}{\\to}\\mu.\\) The convergence in probability can be indeed maintained under much more general conditions than under iid case. The random variables in the sample do not have to be identically distributed, and they do not have to be independent either. Consider an inid (independent but non-identically distributed) sample \\(\\left(x_{1},\\ldots,x_{n}\\right)\\) with \\(E\\left[x_{i}\\right]=0\\) and \\(\\mathrm{var}\\left[x_{i}\\right]=\\sqrt{n}c\\) for some constant \\(c&gt;0\\). Use the Chebyshev inequality to show that \\(n^{-1}\\sum_{i=1}^{n}x_{i}\\stackrel{p}{\\to}0\\). Consider the time series moving average model \\(x_{i}=\\varepsilon_{i}+\\theta\\varepsilon_{i-1}\\) for \\(i=1,\\ldots,n\\), where \\(\\left|\\theta\\right|&lt;1\\), \\(E\\left[\\varepsilon_{i}\\right]=0\\), \\(\\mathrm{var}\\left[\\varepsilon_{i}\\right]=\\sigma^{2}\\), and \\(\\left(\\varepsilon_{i}\\right)_{i=0}^{n}\\) iid. Use the Chebyshev inequality to show that \\(n^{-1}\\sum_{i=1}^{n}x_{i}\\stackrel{p}{\\to}0\\). Another useful LLN is the Kolmogorov LLN. Since its derivation requires more advanced knowledge of probability theory, we state the result without proof. Kolmogorov LLN: If \\(\\left(z_{1},\\ldots,z_{n}\\right)\\) is a sample of iid observations and \\(E\\left[z_{1}\\right]=\\mu\\) exists, then \\(\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\stackrel{p}{\\to}\\mu\\). Compared with the Chebyshev LLN, the Kolmogorov LLN only requires the existence of the population mean, but not any higher moments. On the other hand, iid is essential for the Kolmogorov LLN. Consider three distributions: standard normal \\(N\\left(0,1\\right)\\), \\(t\\left(2\\right)\\) (zero mean, infinite variance), and the Cauchy distribution (no moments exist). We plot paths of the sample average with \\(n=2^{1},2^{2},\\ldots,2^{20}\\). We will see that the sample averages of \\(N\\left(0,1\\right)\\) and \\(t\\left(2\\right)\\) converge, but that of the Cauchy distribution does not. knitrout 5.3 Central Limit Theorem The central limit theorem (CLT) is a collection of probability results about the convergence in distribution to a stable distribution. The limiting distribution is usually the Gaussian distribution. The basic form of the CLT is: Under some conditions to be spelled out, the sample average of zero-mean random variables \\(\\left(z_{1},\\ldots,z_{n}\\right)\\) multiplied by \\(\\sqrt{n}\\) satisfies \\[\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}z_{i}\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\right)\\] as \\(n\\to\\infty\\). Various versions of CLT work under different assumptions about the random variables. Lindeberg-Levy CLT is the simplest CLT. If the sample \\(\\left(x_{1},\\ldots,x_{n}\\right)\\) is iid, \\(E\\left[x_{1}\\right]=0\\) and \\(\\mathrm{var}\\left[x_{1}\\right]=\\sigma^{2}&lt;\\infty\\), then \\(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\right)\\). Lindeberg-Levy CLT can be proved by the moment generating function. For any random variable \\(x\\), the function \\(M_{x}\\left(t\\right)=E\\left[\\exp\\left(xt\\right)\\right]\\) is called its the moment generating function (MGF) if it exists. MGF fully describes a distribution, just like PDF or CDF. For example, the MGF of \\(N\\left(\\mu,\\sigma^{2}\\right)\\) is \\(\\exp\\left(\\mu t+\\frac{1}{2}\\sigma^{2}t^{2}\\right)\\). If \\(E\\left[\\left|x\\right|^{k}\\right]&lt;\\infty\\) for a positive integer \\(k\\), then \\[M_{X}\\left(t\\right)=1+tE\\left[X\\right]+\\frac{t^{2}}{2}E\\left[X^{2}\\right]+\\ldots\\frac{t}{k!}E\\left[X^{k}\\right]+O\\left(t^{k+1}\\right).\\] Under the assumption of Lindeberg-Levy CLT, \\[M_{\\frac{X_{i}}{\\sqrt{n}}}\\left(t\\right)=1+\\frac{t^{2}}{2n}\\sigma^{2}+O\\left(\\frac{t^{3}}{n^{3/2}}\\right)\\] for all \\(i\\), and by independence we have \\[\\begin{aligned}M_{\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}}\\left(t\\right) &amp; =\\prod_{i=1}^{n}M_{\\frac{X_{i}}{\\sqrt{n}}}\\left(t\\right)=\\left(1+\\frac{t^{2}}{2n}\\sigma^{2}+O\\left(\\frac{t^{3}}{n^{3/2}}\\right)\\right)^{n}\\\\ &amp; \\to\\exp\\left(\\frac{\\sigma^{2}}{2}t^{2}\\right), \\end{aligned}\\] where the limit is exactly the characteristic function of \\(N\\left(0,\\sigma^{2}\\right)\\). This proof with MGF is simple and elementary. Its drawback is that not all distributions have a well-defined MGF. A more general proof can be carried out by replacing MGF with the characteristic function \\(\\varphi_{x}\\left(t\\right)=E\\left[\\exp\\left(\\mathrm{i}xt\\right)\\right]\\), where “\\(\\mathrm{i}\\)” is the imaginary number. The characteristic function is the Fourier transform of the probability measure and it always exists. Such a proof will require background knowledge of Fourier transform and inverse transform, which we do not pursuit here. Lindeberg-Feller CLT: \\(\\left(x_{i}\\right)_{i=1}^{n}\\) is inid. If the Lindeberg condition is satisfied (for any fixed \\(\\varepsilon&gt;0\\), \\(\\frac{1}{s_{n}^{2}}\\sum_{i=1}^{n}E\\left[x_{i}^{2}\\cdot\\boldsymbol{1}\\left\\{ \\left|x_{i}\\right|\\geq\\varepsilon s_{n}\\right\\} \\right]\\to0\\) where \\(s_{n}=\\sqrt{\\sum_{i=1}^{n}\\sigma_{i}^{2}}\\)), then we have \\[\\frac{\\sum_{i=1}^{n}x_{i}}{s_{n}}\\stackrel{d}{\\to}N\\left(0,1\\right).\\] Lyapunov CLT: \\(\\left(x_{i}\\right)_{i=1}^{n}\\) is inid. If \\(\\max_{i\\leq n}E\\left[\\left|x_{i}\\right|^{3}\\right]&lt;C&lt;\\infty,\\) then we have \\[\\frac{\\sum_{i=1}^{n}x_{i}}{s_{n}}\\stackrel{d}{\\to}N\\left(0,1\\right).\\] This is a simulated example. \\[knitrout\\] 5.4 Tools for Transformations In their original forms, LLN deals with the sample mean, and CLT handles the scaled (by \\(\\sqrt{n}\\)) and/or standardized (by standard deviation) sample mean. However, most of the econometric estimators of interest are functions of sample means. For example, in the OLS estimator \\[\\widehat{\\beta}=\\left(\\frac{1}{n}\\sum_{i}x_{i}x_{i}&#39;\\right)^{-1}\\frac{1}{n}\\sum_{i}x_{i}y_{i}\\] involves matrix inverse and the matrix-vector multiplication. We need tools to handle transformations. Continuous mapping theorem 1: If \\(x_{n}\\stackrel{p}{\\to}a\\) and \\(f\\left(\\cdot\\right)\\) is continuous at \\(a\\), then \\(f\\left(x_{n}\\right)\\stackrel{p}{\\to}f\\left(a\\right)\\). Continuous mapping theorem 2: If \\(x_{n}\\stackrel{d}{\\to}x\\) and \\(f\\left(\\cdot\\right)\\) is continuous almost surely on the support of \\(x\\), then \\(f\\left(x_{n}\\right)\\stackrel{d}{\\to}f\\left(x\\right)\\). Slutsky’s theorem: If \\(x_{n}\\stackrel{d}{\\to}x\\) and \\(y_{n}\\stackrel{p}{\\to}a\\), then \\(x_{n}+y_{n}\\stackrel{d}{\\to}x+a\\) \\(x_{n}y_{n}\\stackrel{d}{\\to}ax\\) \\(x_{n}/y_{n}\\stackrel{d}{\\to}x/a\\) if \\(a\\neq0\\). Slutsky’s theorem consists of special cases of the continuous mapping theorem 2. Only because the addition, multiplication and division are encountered so frequently in practice, we list it as a separate theorem. Delta method: if \\(\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)\\), and \\(f\\left(\\cdot\\right)\\) is continuously differentiable at \\(\\theta_{0}\\) (meaning \\(\\frac{\\partial}{\\partial\\theta}f\\left(\\cdot\\right)\\) is continuous at \\(\\theta_{0}\\)), then we have \\[\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\to}N\\left(0,\\frac{\\partial f}{\\partial\\theta&#39;}\\left(\\theta_{0}\\right)\\Omega\\left(\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)\\right)&#39;\\right).\\] Take a Taylor expansion of \\(f\\left(\\widehat{\\theta}\\right)\\) around \\(f\\left(\\theta_{0}\\right)\\): \\[f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)=\\frac{\\partial f\\left(\\dot{\\theta}\\right)}{\\partial\\theta&#39;}\\left(\\widehat{\\theta}-\\theta_{0}\\right),\\] where \\(\\dot{\\theta}\\) lies on the line segment between \\(\\widehat{\\theta}\\) and \\(\\theta_{0}\\). Multiply \\(\\sqrt{n}\\) on both sides, \\[\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)=\\frac{\\partial f\\left(\\dot{\\theta}\\right)}{\\partial\\theta&#39;}\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right).\\] Because \\(\\widehat{\\theta}\\stackrel{p}{\\to}\\theta_{0}\\) implies \\(\\dot{\\theta}\\stackrel{p}{\\to}\\theta_{0}\\) and \\(\\frac{\\partial}{\\partial\\theta&#39;}f\\left(\\cdot\\right)\\) is continuous at \\(\\theta_{0}\\), we have \\(\\frac{\\partial}{\\partial\\theta&#39;}f\\left(\\dot{\\theta}\\right)\\stackrel{p}{\\to}\\frac{\\partial f\\left(\\theta_{0}\\right)}{\\partial\\theta&#39;}\\) by the continuous mapping theorem 1. In view of \\(\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)\\), Slutsky’s Theorem implies \\[\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\to}\\frac{\\partial f\\left(\\theta_{0}\\right)}{\\partial\\theta&#39;}N\\left(0,\\Omega\\right)\\] and the conclusion follows. 5.5 Summary Asymptotic theory is a topic with vast breadth and depth. In this chapter we only scratch the very surface of it. We will discuss in the next chapter how to apply the asymptotic tools we learned here to the OLS estimator. Historical notes: Before 1980s, most econometricians did not have a good training in mathematical rigor to master asymptotic theory. A few prominent young (at that time) econometricians came to the field and changed the situation, among them were Halbert White (UCSD), Peter C.B. Phillips (Yale) and Peter Robinson (LSE), to name a few. Further reading: Halbert White (1950-2012) wrote an accessible book (White 2000 first edition 1984) to introduce asymptotics to econometricians. This book remains popular among researchers and graduate students in economics. Davidson (1994) is a longer and more self-contained monograph. References "],
["asymptotic-properties-of-least-squares.html", "6 Asymptotic Properties of Least Squares 6.1 Consistency 6.2 Asymptotic Distribution 6.3 Asymptotic Inference 6.4 Consistency of Feasible Variance Estimator 6.5 Summary 6.6 Appendix", " 6 Asymptotic Properties of Least Squares We have learned some basic asymptotic theory in the previous chapter. We apply these results to study asymptotic properties of the OLS estimator \\(\\widehat{\\beta}=\\left(X&#39;X\\right)^{-1}X&#39;Y\\), which is of key interest in our course. We will show (i) \\(\\widehat{\\beta}\\) is a consistent estimator of the linear projection coefficient \\(\\beta\\); (ii) \\(\\widehat{\\beta}\\) is asymptotically normal; (iii) the asymptotic normality allows asymptotic inference of \\(\\beta\\); (iv) under what condition the variance components in the test statistic can be consistently estimated so that the testing procedure is make feasible. 6.1 Consistency Consistency is the most basic requirement for estimators in large sample. Intuitively, it says that when the sample size is arbitrarily large, a desirable estimator should be arbitrarily close (in the sense of convergence in probability) to the population quantity of interest. Otherwise, if an estimator still deviates from the object of interest under infinite sample size, it is hard to persuade other researchers to use such an estimator unless compelling justification is provided. For a generic estimator \\(\\widehat{\\theta}\\), we say \\(\\widehat{\\theta}\\) is consistent for \\(\\theta\\) if \\(\\widehat{\\theta}\\stackrel{p}{\\to}\\theta\\), where \\(\\theta\\) is some non-random object. In OLS, we say \\(\\widehat{\\beta}\\) is consistent if \\(\\widehat{\\beta}\\stackrel{p}{\\to}\\beta\\) as \\(n\\to\\infty\\), where \\(\\beta\\) is the linear projection coefficient of the population model \\(y_{i}=x_{i}&#39;\\beta+e_{i}\\) with \\(E\\left[x_{i}e_{i}\\right]=0\\). To verify consistency, we write \\[\\widehat{\\beta}-\\beta=\\left(X&#39;X\\right)^{-1}X&#39;e=\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}e_{i}.\\label{eq:ols_d}\\] For simplicity, in this chapter we discuss the iid setting only. The first term, by LLN, \\[\\widehat{Q}:=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\stackrel{p}{\\to}Q:=E\\left[x_{i}x_{i}&#39;\\right].\\] Here \\(\\widehat{Q}\\) is the sample mean of \\(x_{i}x_{i}&#39;\\) and \\(Q\\) is the population mean of \\(x_{i}x_{i}&#39;\\). The second term, again by LLN, \\[\\frac{1}{n}\\sum_{i=1}^{n}x_{i}e_{i}\\stackrel{p}{\\to}0.\\] The continuous mapping theorem immediately implies \\[\\widehat{\\beta}-\\beta\\stackrel{p}{\\to}Q^{-1}\\times0=0.\\] The OLS estimator \\(\\widehat{\\beta}\\) is a consistent estimator of \\(\\beta\\). No matter whether \\(\\left(y_{i},x_{i}\\right)_{i=1}^{n}\\) is an iid, or inid, or dependent sample, consistency holds as long as the convergence in probability holds for the above two expressions and \\(Q\\) is an invertible matrix. 6.2 Asymptotic Distribution In finite sample, \\(\\widehat{\\beta}\\) is a random variable. We have shown the distribution of \\(\\widehat{\\beta}\\) under normality before. Without the restrictive normality assumption, how can we characterize the randomness of the OLS estimator? We know from the previous section that \\(\\hat{\\beta}-\\beta\\stackrel{p}{\\to}0\\) degenerates to a constant. To study its distribution, we must scale it up by a proper multiplier so that in the limit it neither degenerates nor explodes. The suitable scaling factor is \\(\\sqrt{n}\\), as in a CLT. \\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)=\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\right)^{-1}\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}e_{i}.\\] Since \\(E\\left[x_{i}e_{i}\\right]=0\\), we apply a CLT to obtain \\[\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}e_{i}\\stackrel{d}{\\to}N\\left(0,\\Sigma\\right)\\] where \\(\\Sigma=E\\left[x_{i}x_{i}&#39;e_{i}^{2}\\right]\\). By the continuous mapping theorem, \\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}Q^{-1}\\times N\\left(0,\\Sigma\\right)\\sim N\\left(0,\\Omega\\right)\\label{eq:asym_norm}\\] where \\(\\Omega=Q^{-1}\\Sigma Q^{-1}\\) is called the asymptotic variance. This result is the asymptotic normality of the OLS estimator. The asymptotic variance \\(\\Omega=Q^{-1}\\Sigma Q^{-1}\\) is called of the sandwich form. It can be simplified under conditional homoskedasticity \\(E\\left[e_{i}^{2}|x_{i}\\right]=\\sigma^{2}\\) for all \\(i\\), which gives \\[\\Sigma=E\\left[x_{i}x_{i}&#39;e_{i}^{2}\\right]=E\\left[x_{i}x_{i}&#39;E\\left[e_{i}^{2}|X\\right]\\right]=\\sigma^{2}E\\left[x_{i}x_{i}&#39;\\right]=\\sigma^{2}Q.\\] In this case, \\(\\Omega=Q^{-1}\\Sigma Q^{-1}=\\sigma^{2}Q^{-1}\\), and thus \\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}Q^{-1}\\right).\\label{eq:asym_norm_homo}\\] If we are interested in the \\(k\\)-th parameter \\(\\beta_{k}\\), then the joint distribution in (\\[eq:asym\\_norm\\]) implies \\[\\begin{aligned} \\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right) &amp; =\\sqrt{n}\\eta_{k}&#39;\\left(\\widehat{\\beta}-\\beta\\right)\\nonumber \\\\ &amp; \\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\eta_{k}&#39;Q^{-1}\\eta_{k}\\right)\\sim N\\left(0,\\sigma^{2}[Q^{-1}]_{kk}\\right),\\label{eq:asym_norm_homok}\\end{aligned}\\] where \\(\\eta_{k}=\\left(0,\\ldots,0,1,0\\ldots,0\\right)&#39;\\) is the selector of the \\(k\\)-th element. If \\(\\Omega^{-1/2}\\) is multiplied on both sides of \\[eq:asym\\_norm\\], we have \\[\\Omega^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,I_{K}\\right).\\label{eq:asym_norm-pivot}\\] We say the asymptotic distribution in \\[eq:asym\\_norm-pivot\\], \\(N\\left(0,I_{K}\\right)\\), is pivotal because it does not involve any unknown parameter. In contrast, the asymptotic distribution in \\[eq:asym\\_norm\\] is not pivotal because \\(\\Omega\\) is unknown in \\(N\\left(0,\\Omega\\right)\\). If we are interested in the \\(k\\)-th parameter \\(\\beta_{k}\\), we can write \\[eq:asym\\_norm-pivot\\] into the pivotal form as \\[\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)}{\\sqrt{\\sigma^{2}[Q^{-1}]_{kk}}}\\stackrel{d}{\\to}N\\left(0,1\\right).\\label{eq:asym_norm_homok_pivot}\\] 6.3 Asymptotic Inference Up to now we have derived the asymptotic distribution of \\(\\widehat{\\beta}\\). However, \\[eq:asym\\_norm\\] or \\[eq:asym\\_norm-pivot\\] will be useful for statistical inference only if \\(\\Omega\\) is known. In reality \\(\\Omega\\) is mostly unknown, and therefore we will need to estimate it to make statistical inference feasible. Suppose \\(\\tilde{\\Omega}\\) is any consistent estimator for \\(\\Omega\\) in that \\(\\tilde{\\Omega}\\stackrel{p}{\\to}\\Omega\\). When we replace \\(\\Omega\\) in \\[eq:asym\\_norm-pivot\\] with \\(\\tilde{\\Omega}\\), we have \\[\\begin{aligned} \\tilde{\\Omega}^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right) &amp; =\\tilde{\\Omega}^{-1/2}\\Omega^{1/2}\\times\\Omega^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right).\\end{aligned}\\] Because \\(\\Omega\\) is positive definite, we have the first factor \\(\\tilde{\\Omega}^{-1/2}\\Omega^{1/2}\\stackrel{p}{\\to}I_{K}\\) by the continuous mapping theorem. The second factor is asymptotic normal by \\[eq:asym\\_norm-pivot\\]. Thus Slutsky’s theorem implies \\[\\tilde{\\Omega}^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,I_{K}\\right)\\label{eq:asym_norm_feasible}\\] and \\[eq:asym\\_norm\\_feasible\\] is a feasible statistic for asymptotic inference. The next question is how to consistently estimate \\(\\Omega=Q^{-1}\\Sigma Q^{-1}\\), or equivalent how to come up with an \\(\\tilde{\\Omega}\\). We have had \\(\\widehat{Q}\\stackrel{p}{\\to}Q\\). If we have a consistent estimator \\(\\tilde{\\Sigma}\\) for \\(\\Sigma\\), then we can plug in these consistent estimators to form \\(\\tilde{\\Omega}=\\widehat{Q}^{-1}\\tilde{\\Sigma}\\widehat{Q}^{-1}\\). The tricky question is how to consistently estimate \\(\\Sigma=E\\left[x_{i}x_{i}&#39;e_{i}^{2}\\right]\\). We cannot use the sample mean of \\(x_{i}x_{i}&#39;e_{i}^{2}\\) to estimate \\(\\Sigma\\) because \\(e_{i}\\) is unobservable. Under homoskedasticity \\(\\Omega=Q^{-1}\\Sigma Q^{-1}=\\sigma^{2}Q^{-1}\\), and similarly we cannot use the sample mean of \\(e_{i}^{2}\\) to estimate \\(\\sigma^{2}\\). Heteroskedasticity is ubiquitous in econometrics. A regression example that naturally generates conditional heteroskedasticity is the linear probability model \\(y_{i}=x_{i}&#39;\\beta+e_{i}\\), where \\(y_{i}\\in\\left\\{ 0,1\\right\\}\\) is a binary dependent variable. Assume CEF as \\(E\\left[y_{i}|x_{i}\\right]=x_{i}&#39;\\beta\\), so we can use OLS to consistently estimate \\(\\beta\\). The conditional variance \\[\\mathrm{var}\\left[e_{i}|x_{i}\\right]=\\mathrm{var}\\left[y_{i}|x_{i}\\right]=E\\left[y_{i}|x_{i}\\right]\\left(1-E\\left[y_{i}|x_{i}\\right]\\right)=x_{i}&#39;\\beta\\left(1-x_{i}&#39;\\beta\\right)\\] explicitly depends on \\(x_{i}\\). In other words, the conditional variance varies with \\(x_{i}\\). Naturally, one may attempt to use the OLS residual \\(\\widehat{e}_{i}=\\widehat{y}_{i}-x_{i}&#39;\\widehat{\\beta}\\) to replace the regression error \\(e_{i}\\), so that we would have the plug-in estimators \\(\\widehat{\\Omega}=\\widehat{\\sigma}^{2}\\widehat{Q}^{-1}\\) for homoskedasticity, where \\(\\widehat{\\sigma}^{2}=\\widehat{e}&#39;\\widehat{e}/\\left(n-K\\right)\\) or \\(\\widehat{\\sigma}^{2}=\\widehat{e}&#39;\\widehat{e}/n\\), and \\(\\widehat{\\Omega}=\\widehat{Q}^{-1}\\widehat{\\Sigma}\\widehat{Q}^{-1}\\) for heteroskedasticity, where \\(\\widehat{\\Sigma}=n^{-1}\\sum_{i}x_{i}x_{i}&#39;\\widehat{e}_{i}^{2}\\). If we choose \\(\\widehat{\\sigma}^{2}=\\widehat{e}&#39;\\widehat{e}/\\left(n-K\\right)\\) and replace \\(\\sigma^{2}\\) in \\[eq:asym\\_norm\\_homok\\_pivot\\], then the resulting statistic \\(T_{k}=\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)}{\\sqrt{\\widehat{\\sigma}^{2}[\\widehat{Q}^{-1}]_{kk}}}\\) is exactly the \\(t\\)-statistic in the finite sample analysis. Recall that under the classical normal-error assumption, the \\(t\\)-statistics follows exact finite sample \\(t\\)-distribution with degrees of freedom \\(n-K\\). In asymptotic analysis, we allow \\(e_{i}\\) to be any distribution if \\(E\\left[e_{i}^{2}|x_{i}\\right]&lt;\\infty\\) (We impose this assumption for simplicity. It can be further relaxed in inid cases.) The asymptotic normality allows us to conduct asymptotic statistical inference. For the same \\(t\\)-statistic, we must draw the critical values from the normal distribution, because \\[T_{k}=\\frac{\\sqrt{\\sigma^{2}[Q^{-1}]_{kk}}}{\\sqrt{\\widehat{\\sigma}^{2}[\\widehat{Q}^{-1}]_{kk}}}\\cdot\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)}{\\sqrt{\\sigma^{2}[Q^{-1}]_{kk}}}\\stackrel{d}{\\to}1\\times N\\left(0,1\\right)\\sim N\\left(0,1\\right)\\] by Slutsky’s theorem if \\(\\widehat{\\sigma}^{2}\\stackrel{p}{\\to}\\sigma^{2}\\). The next section will give sufficient conditions for \\(\\widehat{\\sigma}^{2}\\stackrel{p}{\\to}\\sigma^{2}\\) and \\(\\widehat{\\Sigma}\\stackrel{p}{\\to}\\Sigma\\). 6.4 Consistency of Feasible Variance Estimator We first show under what conditions all elements of \\(\\Sigma=E\\left[x_{i}x_{i}&#39;e_{i}^{2}\\right]\\) are finite. That is, \\(\\left\\Vert \\Sigma\\right\\Vert _{\\infty}&lt;\\infty\\), where \\(\\left\\Vert \\cdot\\right\\Vert _{\\infty}\\) is the value of the largest element in absolute value of a matrix or vector. Let \\(z_{i}=x_{i}e_{i}\\), so \\(\\Sigma=E\\left[z_{i}z_{i}&#39;\\right]\\). For a generic random variable \\(u_{i}\\) with finite variance, define its \\(L_{2}\\)-norm as \\(\\sqrt{E\\left[u_{i}^{2}\\right]}\\). Given another generic random variable \\(v_{i}\\) with finite variance, define the inner product of \\(u_{i}\\) and \\(v_{i}\\) as \\(E\\left[u_{i}v_{i}\\right]\\). \\(\\left|E\\left[u_{i}v_{i}\\right]\\right|\\leq\\sqrt{E\\left[u_{i}^{2}\\right]E\\left[v_{i}^{2}\\right]}\\). Because of the Cauchy-Schwarz inequality (cross moments are no larger than variance) \\[\\left\\Vert \\Sigma\\right\\Vert _{\\infty}=\\max_{k\\in[K]}E\\left[z_{ik}^{2}\\right],\\] where \\(\\left[K\\right]:=\\left\\{ 1,2,\\ldots,K\\right\\}\\). For each \\(k\\), \\[E\\left[z_{ik}^{2}\\right]=E\\left[x_{ik}^{2}e_{i}^{2}\\right]\\leq\\left(E\\left[x_{ik}^{4}\\right]E\\left[e_{i}^{4}\\right]\\right)^{1/2}\\] where the last inequality again follows by the Cauchy-Schwarz inequality. It implies that the sufficient conditions for finite variance are \\[\\max_{k}E\\left[x_{ik}^{4}\\right]&lt;\\infty\\ \\ \\mbox{and }\\ \\ E\\left[e_{i}^{4}\\right]&lt;\\infty.\\label{eq:4th_moment}\\] We will maintain these conditions in the following derivation. 6.4.1 Homoskedasticity For the estimation of variance, if the error is homoskedastic, \\[\\begin{aligned}\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2} &amp; =\\frac{1}{n}\\sum_{i=1}^{n}\\left(e_{i}+x_{i}&#39;\\left(\\widehat{\\beta}-\\beta\\right)\\right)^{2}\\\\ &amp; =\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}+\\left(\\frac{2}{n}\\sum_{i=1}^{n}e_{i}x_{i}\\right)&#39;\\left(\\widehat{\\beta}-\\beta\\right)+\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\widehat{\\beta}-\\beta\\right)&#39;x_{i}x_{i}&#39;\\left(\\widehat{\\beta}-\\beta\\right). \\end{aligned} \\label{eq:v-homo1}\\] For a generic \\(m\\)-vector \\(u\\), define its \\(L_{2}\\)-norm as \\(\\left\\Vert u\\right\\Vert _{2}=\\sqrt{u&#39;u}\\). Given another generic \\(m\\)-vector \\(v\\), define the inner product of \\(u\\) and \\(v\\) as \\(\\left\\langle u,v\\right\\rangle =u&#39;v\\). \\(\\left|\\left\\langle u,v\\right\\rangle \\right|\\leq\\left\\Vert u\\right\\Vert _{2}\\left\\Vert v\\right\\Vert _{2}\\), or equivalently \\(\\left|u&#39;v\\right|\\leq\\sqrt{\\left(u&#39;u\\right)\\left(v&#39;v\\right)}\\). Notice \\(\\frac{1}{n}\\sum_{i=1}^{n}e_{i}x_{i}\\stackrel{p}{\\to}E\\left[e_{i}x_{i}\\right]=0\\), the second term of \\[eq:v-homo1\\] is \\[\\begin{aligned} \\left|\\left(\\frac{2}{n}\\sum_{i=1}^{n}e_{i}x_{i}\\right)&#39;\\left(\\widehat{\\beta}-\\beta\\right)\\right| &amp; \\leq2\\left\\Vert \\frac{1}{n}\\sum_{i=1}^{n}x_{i}e_{i}\\right\\Vert _{2}\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\nonumber \\\\ &amp; =o_{p}\\left(1\\right)o_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\label{eq:homo1}\\end{aligned}\\] by Cauchy-Schwarz inequality. For a generic \\(m\\times m\\) symmetric positive semi-definite matrix \\(A\\) and a generic \\(m\\) vector \\(u\\), we have the \\[\\left\\Vert u\\right\\Vert _{2}^{2}\\lambda_{\\min}\\left(A\\right)\\leq u&#39;Au\\leq\\left\\Vert u\\right\\Vert _{2}^{2}\\lambda_{\\max}\\left(A\\right).\\] The third term of \\[eq:v-homo1\\] is bounded by \\[\\begin{aligned} \\left(\\widehat{\\beta}-\\beta\\right)\\left(\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}x_{i}x&#39;_{i}\\right)\\left(\\widehat{\\beta}-\\beta\\right) &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\lambda_{\\max}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x&#39;_{i}\\right)\\nonumber \\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\mathrm{trace}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x&#39;_{i}\\right)\\nonumber \\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}K\\max_{k}\\left\\{ \\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{2}\\right\\} \\nonumber \\\\ &amp; =o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right),\\label{eq:homo2}\\end{aligned}\\] where the stochastic order follows by \\[\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{2}\\stackrel{p}{\\to}E\\left[x_{ik}^{2}\\right]&lt;\\infty\\] in view of the condition \\[eq:4th\\_moment\\]. (\\[eq:homo1\\]) and (\\[eq:homo2\\]) implies that \\[\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}+o_{p}\\left(1\\right)+o_{p}\\left(1\\right)=\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}+o_{p}\\left(1\\right)\\stackrel{p}{\\to}\\sigma_{e}^{2}.\\] (See Appendix for the operations of small op and big Op.) 6.4.2 Heteroskedasticity The basic strategy of proof is similar for the general case of heteroskedasticity, though each step is more complicated. \\[\\begin{aligned}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\widehat{e}_{i}^{2} &amp; =\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\left(e_{i}+x_{i}&#39;\\left(\\widehat{\\beta}-\\beta\\right)\\right)^{2}\\\\ &amp; =\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;e_{i}^{2}+\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\cdot e_{i}x_{i}&#39;\\left(\\widehat{\\beta}-\\beta\\right)+\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}&#39;\\left(\\left(\\widehat{\\beta}-\\beta\\right)&#39;x_{i}\\right)^{2}. \\end{aligned} \\label{eq:v-hetero}\\] For a generic \\(m\\)-vector \\(u\\), its \\(L_{p}\\)-norm (for \\(p\\geq1\\)) is defined as \\(\\left\\Vert u\\right\\Vert _{p}=\\left(\\left|u_{1}\\right|^{p}+\\cdots+\\left|u_{m}\\right|^{p}\\right)^{1/p}\\). For two generic \\(m\\)-vectors \\(u\\) and \\(v\\), \\[\\left|u&#39;v\\right|\\leq\\left\\Vert u\\right\\Vert _{p}\\left\\Vert q\\right\\Vert _{q}\\] for any \\(p,q\\in[1,\\infty)\\) and \\(1/p+1/q=1\\). Cauchy-Schwarz inequality is a special case of Holder’s inequality when \\(p=q=2\\). The second term of \\[eq:v-hetero\\] is bounded by \\[\\begin{aligned} &amp; \\max_{k,k&#39;}\\left|\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}x_{ik&#39;}\\cdot e_{i}x_{i}&#39;\\left(\\widehat{\\beta}-\\beta\\right)\\right|\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\max_{k,k&#39;}\\left\\Vert \\frac{1}{n}\\sum_{i=1}^{n}x_{i}e_{i}x_{ik}x_{ik&#39;}\\right\\Vert _{2}\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\sqrt{K}\\max_{k,k&#39;,k&#39;&#39;}\\left|\\frac{1}{n}\\sum_{i=1}^{n}e_{i}x_{ik}x_{ik&#39;}x_{ik&#39;&#39;}\\right|\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\sqrt{K}\\left(\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{4}\\right)^{1/4}\\max_{k,k&#39;,k&#39;&#39;}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{ik}x_{ik&#39;}x_{ik&#39;&#39;}\\right)^{4/3}\\right)^{3/4}\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\sqrt{K}\\left(\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{4}\\right)^{1/4}\\max_{k}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{4}\\right)^{3/4}\\\\ &amp; =o_{p}\\left(1\\right)O_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right) \\end{aligned}\\] where the third inequality hold by the Holder’s inequality with \\(p=4\\) and \\(q=4/3\\), and the stochastic order is guaranteed if under suitable conditions \\[\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{4}\\stackrel{p}{\\to}E\\left[e_{i}^{4}\\right]&lt;\\infty\\ \\ \\text{and }\\ \\ \\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{4}\\stackrel{p}{\\to}E\\left[x_{ik}^{4}\\right]&lt;\\infty.\\] The third term of \\[eq:v-hetero\\] is bounded by \\[\\begin{aligned} &amp; \\max_{k_{1},k_{2}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}x_{ik_{1}}x_{ik_{2}}\\left(\\left(\\widehat{\\beta}-\\beta\\right)&#39;x_{i}\\right)^{2}\\right|\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k_{1},k_{2}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}x_{ik_{1}}x_{ik_{2}}\\left(x_{i}x_{i}&#39;\\right)\\right|\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k_{1},k_{2},k_{3},k_{4}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}x_{ik_{1}}x_{ik_{2}}x_{ik_{3}}x_{ik_{4}}\\right|\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k_{1},k_{2}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}x_{ik_{1}}^{2}x_{ik_{2}}^{2}\\right|\\\\ &amp; \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k}\\left|\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{4}\\right|\\\\ &amp; =o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right). \\end{aligned}\\] where the third and the fourth inequalities follow by applying Cauchy Schwarz inequality. 6.5 Summary One of the most important techniques in asymptotic theory is manipulating inequalities. These derivations of the variances look complicated at first glance, but is often encountered in proofs of theoretical results. After many years of torment, you will be accustomed to these routine calculations. Historical notes: White (1980) drew attention of economic contexts that violate the classical statistical assumptions in linear regressions. It seeded econometricians’ care, or obsession, in variance estimation for statistical inference. The following decades has witnessed a plethora of proposals of variance estimation that deal with various deviation from the classical assumptions. Further reading: In this chapter all vectors are of finite dimensional. Some results can be extended to allow infinite \\(K\\) when \\(K\\to\\infty\\) at a much slower speed than \\(n\\). Such asymptotic development will require multiple indices, and it goes beyond the simplest case of \\(n\\to\\infty\\) that we learned here. Big data is accompanied by big model, in which the model itself is indexed by the sample size and can grow more sophisticated as \\(n\\) get bigger. In the proofs of my latest paper Shi, Su, and Xie (2020), You will find loads of inequality operations of similar flavor to this chapter. 6.6 Appendix We introduce the “big Op and small op” notation. They are the stochastic counterparts of the “big O and small o” notation in deterministic cases. Small op: \\(x_{n}=o_{p}\\left(r_{n}\\right)\\) if \\(x_{n}/r_{n}\\stackrel{p}{\\to}0\\). Big Op: \\(x_{n}=O_{p}\\left(r_{n}\\right)\\) if for any \\(\\varepsilon&gt;0\\), there exists a \\(c&gt;0\\) such that \\(P\\left(\\left|x_{n}\\right|/r_{n}&gt;c\\right)&lt;\\varepsilon\\). Some operations: \\(o_{p}\\left(1\\right)+o_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\); \\(o_{p}\\left(1\\right)+O_{p}\\left(1\\right)=O_{p}\\left(1\\right)\\); \\(o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\). The big Op and small op notation allows us to keep using equalities in calculation while expressing the stochastic order of random objects. Zhentao Shi. Oct 21, 2020. References "],
["asymptotic-properties-of-mle.html", "7 Asymptotic Properties of MLE 7.1 Examples of MLE 7.2 Consistency 7.3 Asymptotic Normality 7.4 Information Matrix Equality 7.5 Cramer-Rao Lower Bound 7.6 Summary", " 7 Asymptotic Properties of MLE 7.1 Examples of MLE Normal, Logistic, Probit, Poisson 7.2 Consistency We specify a parametric distribution (pdf) \\(f\\left(x;\\theta\\right)\\) and a parameter space \\(\\Theta\\). Define \\(Q\\left(\\theta\\right)=E\\left[\\log f\\left(x;\\theta\\right)\\right]\\), and \\(\\theta_{0}=\\arg\\max_{\\theta\\in\\Theta}Q\\left(\\theta\\right)\\) maximizes the expected log-likelihood. Given a sample of \\(n\\) observations, we compute the average sample log-likelihood \\(\\ell_{n}\\left(\\theta\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\log f\\left(x;\\theta\\right)\\). The MLE estimator is \\(\\widehat{\\theta}=\\arg\\max_{\\theta\\in\\Theta}\\ell_{n}\\left(\\theta\\right)\\). We say that correctly specified if the data \\(\\left(x_{1},\\ldots,x_{n}\\right)\\) is generated from the pdf \\(f\\left(x;\\theta\\right)\\) for some \\(\\theta\\in\\Theta\\). Otherwise if the data is not generated from any member in the class of distributions \\(\\mathcal{M}^{*}:=\\left\\{ \\theta\\in\\Theta:f\\left(x;\\theta\\right)\\right\\}\\), we say it is misspecified. When the model is misspecified, strictly speaking the log-likelihood function \\(\\ell_{n}\\left(\\theta\\right)\\) should be called quasi log-likelihood and the MLE estimator \\(\\widehat{\\theta}\\) should be called the quasi MLE. We will discuss under what condition \\(\\widehat{\\theta}\\stackrel{p}{\\to}\\theta_{0}\\), that is, the maximizer of the sample log-likelihood converges in probability to the maximizer of the expected log-likelihood in population. Notice that unlike OLS, most MLE estimators do not admit a closed-form. They are defined as a maximizer and solved by numerical optimization. The first requirement for the consistency of MLE is that \\(\\theta_{0}\\) uniquely defined. Suppose \\(\\theta_{0}\\in\\mathrm{int}\\left(\\Theta\\right)\\) lies in the interior of \\(\\Theta\\). Let \\(N\\left(\\theta_{0},\\varepsilon\\right)=\\left\\{ \\theta\\in\\Theta:\\left|\\theta-\\theta_{0}\\right|&lt;\\varepsilon\\right\\}\\) is a neighborhood around \\(\\theta_{0}\\) with radius \\(\\varepsilon\\) for some \\(\\varepsilon&gt;0\\). The value \\(\\theta_{0}\\) is identified if for any \\(\\varepsilon&gt;0\\), there exists a \\(\\delta=\\delta\\left(\\varepsilon\\right)&gt;0\\) such that \\(Q\\left(\\theta_{0}\\right)&gt;\\sup_{\\theta\\in\\Theta\\backslash N\\left(\\theta_{0},\\varepsilon\\right)}Q\\left(\\theta\\right)+\\delta\\). We know under suitable condition, LLN implies \\(\\ell_{n}\\left(\\theta\\right)\\stackrel{p}{\\to}Q\\left(\\theta\\right)\\) for each \\(\\theta\\in\\Theta\\). This is a pointwise result, meaning \\(\\theta\\) is taken as fixed as \\(n\\to\\infty\\). However, \\(\\widehat{\\theta}\\) is random in finite-sample, which makes \\(\\ell_{n}(\\widehat{\\theta})\\) a complicated function of the data in particular when \\(\\widehat{\\theta}\\) has no closed-form solution. We therefore need to strengthen the pointwise LLN. We say a uniform law of large numbers (ULLN) for \\(Q\\left(\\theta\\right)\\) holds on \\(\\Theta\\) if \\[P\\left\\{ \\sup_{\\theta\\in\\Theta}\\left|\\ell_{n}\\left(\\theta\\right)-Q\\left(\\theta\\right)\\right|\\geq\\varepsilon\\right\\} \\to0\\label{eq:ULLN}\\] for all \\(\\varepsilon&gt;0\\) as \\(n\\to\\infty\\). ULLN can be established under pointwise LLN plus some regularity conditions, for example when \\(\\Theta\\) is a compact set, and \\(\\log f\\left(x;\\cdot\\right)\\) is continuous in \\(\\theta\\) almost everywhere on the support of \\(x\\). If \\(\\theta_{0}\\) is identified and ULLN \\[eq:ULLN\\] hold, then \\(\\widehat{\\theta}\\stackrel{p}{\\to}\\theta_{0}\\). According to the definition of consistency, we can check \\[\\begin{aligned} &amp; P\\left\\{ \\left|\\widehat{\\theta}-\\theta_{0}\\right|&gt;\\varepsilon\\right\\} \\leq P\\left\\{ Q\\left(\\theta_{0}\\right)-Q(\\widehat{\\theta})&gt;\\delta\\right\\} \\\\ &amp; =P\\left\\{ Q\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)+\\ell_{n}\\left(\\theta_{0}\\right)-\\ell_{n}(\\widehat{\\theta})+\\ell_{n}\\left(\\widehat{\\theta}\\right)-Q(\\widehat{\\theta})&gt;\\delta\\right\\} \\\\ &amp; \\leq P\\left\\{ \\left|Q\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)\\right|+\\ell_{n}\\left(\\theta_{0}\\right)-\\ell_{n}(\\widehat{\\theta})+\\left|\\ell_{n}\\left(\\widehat{\\theta}\\right)-Q(\\widehat{\\theta})\\right|&gt;\\delta\\right\\} \\\\ &amp; \\leq P\\left\\{ \\left|Q\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)\\right|+\\left|\\ell_{n}(\\widehat{\\theta})-Q(\\widehat{\\theta})\\right|\\geq\\delta\\right\\} \\\\ &amp; \\leq P\\left\\{ 2\\sup_{\\theta\\in\\Theta}\\left|\\ell_{n}\\left(\\theta\\right)-Q\\left(\\theta\\right)\\right|\\geq\\delta\\right\\} =P\\left\\{ \\sup_{\\theta\\in\\Theta}\\left|\\ell_{n}\\left(\\theta\\right)-Q\\left(\\theta\\right)\\right|\\geq\\frac{\\delta}{2}\\right\\} \\to0.\\end{aligned}\\] The first line holds because of identification, the third line by the triangle inequality, the fourth line by the definition of MLE that \\(\\ell_{n}(\\widehat{\\theta})\\geq\\ell_{n}\\left(\\theta_{0}\\right)\\), and the last line by ULLN. Identification is a necessary condition for consistent estimation. Although \\(\\widehat{\\theta}\\) has no closed-form solution in general, we establish consistency via ULLN over all point \\(\\theta\\in\\Theta\\) under consideration. 7.3 Asymptotic Normality The next step is to derive the asymptotic distribution of the MLE estimator. Let \\(s\\left(x;\\theta\\right)=\\partial\\log f\\left(x;\\theta\\right)/\\partial\\theta\\) and \\(h\\left(x;\\theta\\right)=\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta&#39;}\\log f\\left(x;\\theta\\right)\\) \\[thm:mis-MLE\\] Under suitable regularity conditions, the MLE estimator \\[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\left(E\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1}\\mathrm{var}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\left(E\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1}\\right).\\] The “suitable regularity conditions” will be spelled out later. Indeed, those conditions can be observed in the proof. That \\(\\widehat{\\theta}\\) is a maximizer entails \\(\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\widehat{\\theta}\\right)=0\\). Take a Taylor expansion of \\(\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\widehat{\\theta}\\right)\\) around \\(\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)\\): \\[0-\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)=\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\widehat{\\theta}\\right)-\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)=\\frac{\\partial}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\dot{\\theta}\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\] where \\(\\dot{\\theta}\\) is some point on the line segment connecting \\(\\widehat{\\theta}\\) and \\(\\theta_{0}.\\) Rearrange the above equation and multiply both side by \\(\\sqrt{n}:\\) \\[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=-\\left(\\frac{\\partial}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)^{-1}\\sqrt{n}\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right).\\label{eq:taylor1}\\] When \\(Q\\left(\\theta\\right)\\) is differentiable at \\(\\theta_{0}\\), we have \\(\\frac{\\partial}{\\partial\\theta}Q\\left(\\theta_{0}\\right)=0\\) by the first condition of optimality of \\(\\theta_{0}\\) for \\(Q\\left(\\theta\\right)\\). Notice that \\(E\\left[s\\left(x;\\theta_{0}\\right)\\right]=\\frac{\\partial}{\\partial\\theta}Q\\left(\\theta_{0}\\right)=0\\) if differentiation and integration are interchangeable. By CLT, the second factor in \\[eq:taylor1\\] follows \\[\\sqrt{n}\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\mathrm{var}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\right).\\] Suppose the second factor in \\[eq:taylor1\\] follows \\(\\frac{\\partial}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\dot{\\theta}\\right)\\stackrel{p}{\\to}E\\left[h\\left(x;\\theta_{0}\\right)\\right]\\) (sufficient if we assume \\(E\\left[\\frac{\\partial^{3}}{\\partial\\theta_{i}\\partial\\theta_{j}\\partial\\theta_{l}}\\log f\\left(x;\\theta_{0}\\right)\\right]\\) is continuous in \\(\\theta\\) for all \\(i,j,l\\leq K\\). Thus we have the conclusion by Slutsky’s theorem. When the model is misspecified, the asymptotic variance takes a complicated sandwich form. When the parametric model is correctly specified, then the asymptotic variance can be further simplified, thanks to the following important result of information matrix equality. 7.4 Information Matrix Equality When the model is correctly specified, \\(\\theta_{0}\\) is the true parameter value. The variance \\(\\mathcal{I}\\left(\\theta_{0}\\right):=\\mathrm{var}_{f\\left(x;\\theta_{0}\\right)}\\left[\\frac{\\partial}{\\partial\\theta}\\log f\\left(x;\\theta_{0}\\right)\\right]\\) is called the (Fisher) information matrix, and \\(\\mathcal{H}\\left(\\theta_{0}\\right):=E_{f\\left(x;\\theta_{0}\\right)}\\left[h\\left(x;\\theta_{0}\\right)\\right]\\) is called the expected Hessian matrix. Here we emphasize the true underlying distribution \\(f\\left(x;\\theta_{0}\\right)\\) by writing it as the subscript of the mathematical expectations. \\[fact:Info\\]Under suitable regularity conditions, we have \\(\\mathcal{I}\\left(\\theta_{0}\\right)=-\\mathcal{H}\\left(\\theta_{0}\\right)\\) Because \\(f\\left(x;\\theta_{0}\\right)\\) a pdf, \\(\\int f\\left(x;\\theta_{0}\\right)dx=1\\). Take partial derivative with respect to \\(\\theta\\), \\[\\begin{aligned} 0 &amp; =\\int\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)dx=\\int\\frac{\\partial f\\left(x;\\theta_{0}\\right)/\\partial\\theta}{f\\left(x;\\theta_{0}\\right)}f\\left(x;\\theta_{0}\\right)dx\\nonumber \\\\ &amp; =\\int\\left[s\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx=E_{f\\left(x;\\theta_{0}\\right)}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\label{eq:info_eqn_1}\\end{aligned}\\] where the third equality holds as by the chain rule \\[s\\left(x;\\theta_{0}\\right)=\\frac{\\partial f\\left(x;\\theta_{0}\\right)/\\partial\\theta}{f\\left(x;\\theta_{0}\\right)}.\\label{eq:ell_d}\\] Take a second partial derivative of (\\[eq:info\\_eqn\\_1\\]) with respective to \\(\\theta\\), according to the chain rule: \\[\\begin{aligned} 0 &amp; =\\int\\left[h\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx+\\int\\left[s\\left(x;\\theta_{0}\\right)\\right]\\frac{\\partial}{\\partial\\theta&#39;}f\\left(x;\\theta_{0}\\right)dx\\\\ &amp; =\\int\\left[h\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx+\\int s\\left(x;\\theta_{0}\\right)\\frac{\\partial f\\left(x;\\theta_{0}\\right)/\\partial\\theta}{f\\left(x;\\theta\\right)}f\\left(x;\\theta_{0}\\right)dx\\\\ &amp; =\\int\\left[h\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx+\\int\\left[s\\left(x;\\theta_{0}\\right)s\\left(x;\\theta_{0}\\right)&#39;\\right]f\\left(x;\\theta_{0}\\right)dx\\\\ &amp; =E_{f\\left(x;\\theta_{0}\\right)}\\left[h\\left(x;\\theta_{0}\\right)\\right]+E_{f\\left(x;\\theta_{0}\\right)}\\left[s\\left(x;\\theta_{0}\\right)s\\left(x;\\theta_{0}\\right)&#39;\\right]\\\\ &amp; =\\mathcal{H}\\left(\\theta_{0}\\right)+\\mathcal{I}\\left(\\theta_{0}\\right).\\end{aligned}\\] The second equality follows by (\\[eq:ell\\_d\\]). The last equality by \\[eq:info\\_eqn\\_1\\] as the zero mean ensures the variance of \\(\\frac{\\partial}{\\partial\\theta}\\log f\\left(x;\\theta_{0}\\right)\\) is equal to the expectation of its out-product. Notice that a correct specification is essential for the information matrix equality. If the true data generating distribution is \\(g\\notin\\mathcal{M}^{*}\\), then \\[eq:info\\_eqn\\_1\\] breaks down because \\[0=\\int\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)=\\int\\left[g^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)\\right]g=E_{g}\\left[g^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)\\right]\\] but \\(g^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)\\neq\\left(f\\left(x;\\theta_{0}\\right)\\right)^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)=\\frac{\\partial}{\\partial\\theta}\\log f\\left(\\theta_{0}\\right)\\). The asymptotic variance in Theorem \\[thm:mis-MLE\\], \\[\\left(E_{g}\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1}\\mathrm{var}_{g}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\left(E_{g}\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1},\\] written explicitly in \\(E_{g}\\left[\\cdot\\right]\\), is still valid. When the parametric model \\(\\mathcal{M}^{*}\\) is correctly specified, then we can replace \\(E_{g}\\left[\\frac{\\partial^{2}\\ell_{n}}{\\partial\\theta\\partial\\theta&#39;}\\left(\\theta_{0}\\right)\\right]\\) by \\(\\mathcal{H}\\left(\\theta_{0}\\right)\\) and replace \\(\\mathrm{var}_{g}\\left[\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\theta_{0}\\right)\\right]\\) by \\(\\mathcal{I}\\left(\\theta_{0}\\right)\\), we simplify the asymptotic variance as \\[\\left(\\mathcal{H}\\left(\\theta_{0}\\right)\\right)^{-1}\\mathcal{I}\\left(\\theta_{0}\\right)\\left(\\mathcal{H}\\left(\\theta_{0}\\right)\\right)^{-1}=\\left(-\\mathcal{I}\\left(\\theta_{0}\\right)\\right)^{-1}\\mathcal{I}\\left(\\theta_{0}\\right)\\left(-\\mathcal{I}\\left(\\theta_{0}\\right)\\right)^{-1}=\\left(\\mathcal{I}\\left(\\theta_{0}\\right)\\right)^{-1}\\] by the information matrix equality Fact \\[fact:Info\\]. If the model is correctly specified, under the conditions for Theorem \\[eq:info\\_eqn\\_1\\] and Fact \\[fact:Info\\] the MLE estimator \\[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\left[\\mathcal{I}\\left(\\theta_{0}\\right)\\right]^{-1}\\right).\\] This is the classical asymptotic normality result of MLE. 7.5 Cramer-Rao Lower Bound 7.6 Summary Further reading: White (1996), Newey and McFadden (1994). Zhentao Shi. Oct 29, 2020. References "],
["hypothesis-testing.html", "8 Hypothesis Testing 8.1 Testing 8.2 Confidence Interval\\[confidence-interval\\] 8.3 Bayesian Credible Set 8.4 Applications in OLS 8.5 Summary 8.6 Appendix", " 8 Hypothesis Testing Notation: \\(\\mathbf{X}\\) denotes a random variable or random vector. \\(\\mathbf{x}\\) is its realization. A hypothesis is a statement about the parameter space \\(\\Theta\\). Hypothesis testing checks whether the data support a null hypothesis \\(\\Theta_{0}\\), which is a subset of \\(\\Theta\\) of interest. Ideally the null hypothesis should be suggested by scientific theory. The alternative hypothesis \\(\\Theta_{1}=\\Theta\\backslash\\Theta_{0}\\) is the complement of \\(\\Theta_{0}\\). Based on the observed evidence, hypothesis testing decides to accept or reject the null hypothesis. If the null hypothesis is rejected by the data, it implies that from the statistical perspective the data is incompatible with the proposed scientific theory. In this chapter, we will first introduce the idea and practice of hypothesis testing and the related confidence interval. While we mainly focus on the frequentist interpretation of hypothesis, we briefly discuss the Bayesian approach to statistical decision. As an application of the testing procedures to the linear regression model, we elaborate how to test a linear or nonlinear hypothesis of the slope coefficients based on the unrestricted or restricted OLS estimators. 8.1 Testing 8.1.1 Decision Rule and Errors If \\(\\Theta_{0}\\) is a singleton, we call it a simple hypothesis; otherwise we call it a composite hypothesis. For example, if the parameter space \\(\\Theta=\\mathbb{R}\\), then \\(\\Theta_{0}=\\left\\{ 0\\right\\}\\) (or equivalently \\(\\theta_{0}=0\\)) is a simple hypothesis, whereas \\(\\Theta_{0}=(-\\infty,0]\\) (or equivalently \\(\\theta_{0}\\leq0\\)) is a composite hypothesis. A test function is a mapping \\[\\phi:\\mathcal{X}^{n}\\mapsto\\left\\{ 0,1\\right\\} ,\\] where \\(\\mathcal{X}\\) is the sample space. The null hypothesis is accepted if \\(\\phi\\left(\\mathbf{x}\\right)=0\\), or rejected if \\(\\phi\\left(\\mathbf{x}\\right)=1\\). We call the set \\(A_{\\phi}=\\left\\{ \\mathbf{x}\\in\\mathcal{X}^{n}:\\phi_{\\theta}\\left(\\mathbf{x}\\right)=0\\right\\}\\) the acceptance region, and its complement \\(R_{\\phi}=\\left\\{ \\mathbf{x}\\in\\mathcal{X}^{n}:\\phi\\left(\\mathbf{x}\\right)=1\\right\\}\\) the rejection region. The power function of a test \\(\\phi\\) is \\[\\beta\\left(\\theta\\right)=P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=1\\right\\} =E_{\\theta}\\left[\\phi\\left(\\mathbf{X}\\right)\\right].\\] The power function measures the probability that the test function rejects the null when the data is generated under the true parameter \\(\\theta\\), reflected in \\(P_{\\theta}\\) and \\(E_{\\theta}\\). The power of a test for some \\(\\theta\\in\\Theta_{1}\\) is the value of \\(\\beta\\left(\\theta\\right)\\). The size of the test is \\(\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right).\\) Notice that the definition of power depends on a \\(\\theta\\) in the alternative hypothesis \\(\\Theta_{1}\\), whereas that of size is independent of \\(\\theta\\) due to the supremum over the set of null \\(\\Theta_{0}\\). The level of a test is any value \\(\\alpha\\in\\left(0,1\\right)\\) such that \\(\\alpha\\geq\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right)\\), which is often used when it is difficult to attain the exact supremum. A test of size \\(\\alpha\\) is also of level \\(\\alpha\\) or bigger; while a test of level \\(\\alpha\\) must have size smaller or equal to \\(\\alpha\\). The concept of level is useful if we do not have sufficient information to derive the exact size of a test. If \\(\\left(X_{1i},X_{2i}\\right)_{i=1}^{n}\\) are randomly drawn from some unknown joint distribution, but we know the marginal distribution is \\(X_{ji}\\sim N\\left(\\theta_{j},1\\right)\\), for \\(j=1,2\\). In order to test the joint hypothesis \\(\\theta_{1}=\\theta_{2}=0\\), we can construct a test function \\[\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)=1\\left\\{ \\left\\{ \\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\cup\\left\\{ \\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\right\\} ,\\] where \\(z_{1-\\alpha/4}\\) is the \\(\\left(1-\\alpha/4\\right)\\)-th quantile of the standard normal distribution. The level of this test is \\[\\begin{aligned}P\\left(\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)\\right) &amp; \\leq P\\left(\\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right)+P\\left(\\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right)\\\\ &amp; =\\alpha/2+\\alpha/2=\\alpha. \\end{aligned}\\] where the inequality follows by the Bonferroni inequality \\[P\\left(A\\cup B\\right)\\leq P\\left(A\\right)+P\\left(B\\right).\\] (The seemingly trivial Bonferroni inequality is useful in many proofs of probability results.) Therefore, the level of \\(\\phi\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)\\) is \\(\\alpha\\), but the exact size is unknown without the knowledge of the joint distribution. (Even if we know the correlation of \\(X_{1i}\\) and \\(X_{2i}\\), putting two marginally normal distributions together does not make a jointly normal vector in general.) accept $H_{0}$ reject $H_{0}$ $H_{0}$ true correct decision Type I error $H_{0}$ false Type II error correct decision : \\[tab:Decisions-and-States\\] Actions, States and Consequences The probability of committing Type I error is \\(\\beta\\left(\\theta\\right)\\) for some \\(\\theta\\in\\Theta_{0}\\). The probability of committing Type II error is \\(1-\\beta\\left(\\theta\\right)\\) for some \\(\\theta\\in\\Theta_{1}\\). The philosophy on hypothesis testing has been debated for centuries. At present the prevailing framework in statistics textbooks is the frequentist perspective. A frequentist views the parameter as a fixed constant. They keep a conservative attitude about the Type I error: Only if overwhelming evidence is demonstrated shall a researcher reject the null. Under the principle of protecting the null hypothesis, a desirable test should have a small level. Conventionally we take \\(\\alpha=0.01,\\) 0.05 or 0.1. We say a test is unbiased if \\(\\beta\\left(\\theta\\right)&gt;\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right)\\) for all \\(\\theta\\in\\Theta_{1}\\). There can be many tests of correct size. A trivial test function \\(\\phi(\\mathbf{x})=1\\left\\{ 0\\leq U\\leq\\alpha\\right\\}\\) for all \\(\\theta\\in\\Theta\\), where \\(U\\) is a random variable from a uniform distribution on \\(\\left[0,1\\right]\\), has correct size \\(\\alpha\\) but no non-trivial power at the alternative. On the other extreme, the trivial test function \\(\\phi\\left(\\mathbf{x}\\right)=1\\) for all \\(\\mathbf{x}\\) enjoys the biggest power but suffers incorrect size. Usually, we design a test by proposing a test statistic \\(T_{n}:\\mathcal{X}^{n}\\mapsto\\mathbb{R}^{+}\\) and a critical value \\(c_{1-\\alpha}\\). Given \\(T_{n}\\) and \\(c_{1-\\alpha}\\), we write the test function as \\[\\phi\\left(\\mathbf{X}\\right)=1\\left\\{ T_{n}\\left(\\mathbf{X}\\right)&gt;c_{1-\\alpha}\\right\\} .\\] To ensure such a \\(\\phi\\left(\\mathbf{x}\\right)\\) has correct size, we need to figure out the distribution of \\(T_{n}\\) under the null hypothesis (called the null distribution), and choose a critical value \\(c_{1-\\alpha}\\) according to the null distribution and the desirable size or level \\(\\alpha\\). Another commonly used indicator in hypothesis testing is \\(p\\)-value: \\[\\sup_{\\theta\\in\\Theta_{0}}P_{\\theta}\\left\\{ T_{n}\\left(\\mathbf{x}\\right)\\leq T_{n}\\left(\\mathbf{X}\\right)\\right\\} .\\] In the above expression, \\(T_{n}\\left(\\mathbf{x}\\right)\\) is the realized value of the test statistic \\(T_{n}\\), while \\(T_{n}\\left(\\mathbf{X}\\right)\\) is the random variable generated by \\(\\mathbf{X}\\) under the null \\(\\theta\\in\\Theta_{0}\\). The interpretation of the \\(p\\)-value is tricky. \\(p\\)-value is the probability that we observe \\(T_{n}(\\mathbf{X})\\) being greater than the realized \\(T_{n}(\\mathbf{x})\\) if the null hypothesis is true. \\(p\\)-value is not the probability that the null hypothesis is true. Under the frequentist perspective, the null hypothesis is either true or false, with certainty. The randomness of a test comes only from sampling, not from the hypothesis. \\(p\\)-value measures whether the dataset is compatible with the null hypothesis. \\(p\\)-value is closely related to the corresponding test. When \\(p\\)-value is smaller than the specified test size \\(\\alpha\\), the test rejects the null. So far we have been talking about hypothesis testing in finite sample. The discussion and terminologies can be carried over to the asymptotic world when \\(n\\to\\infty\\). If we denote the power function as \\(\\beta_{n}\\left(\\theta\\right)\\), in which we make its dependence on the sample size \\(n\\) explicit, the test is of asymptotic size \\(\\alpha\\) if \\(\\limsup_{n\\to\\infty}\\beta_{n}\\left(\\theta\\right)\\leq\\alpha\\) for all \\(\\theta\\in\\Theta_{0}\\). A test is consistent if \\(\\beta_{n}\\left(\\theta\\right)\\to1\\) for every \\(\\theta\\in\\Theta_{1}\\). 8.1.2 Optimality Just as there may be multiple valid estimators for a task of estimation, there may be multiple tests for a task of hypothesis testing. For a class of tests of the same level \\(\\alpha\\) under the null \\(\\Psi_{\\alpha}=\\left\\{ \\phi:\\sup_{\\theta\\in\\Theta_{0}}\\beta_{\\phi}\\left(\\theta\\right)\\leq\\alpha\\right\\}\\) where we put a subscript \\(\\phi\\) in \\(\\beta_{\\phi}\\left(\\theta\\right)\\) to distinguish the power for different tests, it is natural to prefer a test \\(\\phi^{*}\\) that exhibits higher power than all other tests under consideration at each point of the alternative hypothesis in that \\[\\beta_{\\phi^{*}}\\left(\\theta\\right)\\geq\\beta_{\\phi}\\left(\\theta\\right)\\] for every \\(\\theta\\in\\Theta_{1}\\) and every \\(\\phi\\in\\Psi_{\\alpha}\\). If such a test \\(\\phi^{*}\\in\\Psi_{\\alpha}\\) exists, we call it the uniformly most powerful test. Suppose a random sample of size 6 is generated from \\[\\left(X_{1},\\ldots,X_{6}\\right)\\sim\\text{iid.}N\\left(\\theta,1\\right),\\] where \\(\\theta\\) is unknown. We want to infer the population mean of the normal distribution. The null hypothesis is \\(H_{0}\\): \\(\\theta\\leq0\\) and the alternative is \\(H_{1}\\): \\(\\theta&gt;0\\). All tests in \\[\\Psi=\\left\\{ 1\\left\\{ \\bar{X}\\geq c/\\sqrt{6}\\right\\} :c\\geq1.64\\right\\}\\] has the correct level. Since \\(\\bar{X}=N\\left(\\theta,1/6\\right)\\), the power function for those in \\(\\Psi\\) is \\[\\begin{aligned} \\beta_{\\phi}\\left(\\theta\\right) &amp; =P\\left(\\bar{X}\\geq\\frac{c}{\\sqrt{6}}\\right)=P\\left(\\frac{\\bar{X}-\\theta}{1/\\sqrt{6}}\\geq\\frac{\\frac{c}{\\sqrt{6}}-\\theta}{1/\\sqrt{6}}\\right)\\\\ &amp; =P\\left(N\\geq c-\\sqrt{6}\\theta\\right)=1-\\Phi\\left(c-\\sqrt{6}\\theta\\right)\\end{aligned}\\] where \\(N=\\frac{\\bar{X}-\\theta}{1/\\sqrt{6}}\\) follows the standard normal, and \\(\\Phi\\) is the cdf of the standard normal. It is clear that \\(\\beta_{\\phi}\\left(\\theta\\right)\\) is monotonically decreasing in \\(c\\). Thus the test function \\[\\phi_{\\theta=0}\\left(\\mathbf{X}\\right)=1\\left\\{ \\bar{X}\\geq1.64/\\sqrt{6}\\right\\}\\] is the most powerful test in \\(\\Psi\\), as \\(c=1.64\\) is the lower bound that \\(\\Psi_{\\alpha}\\) allows in order to keep the level \\(\\alpha\\). 8.1.3 Likelihood-Ratio Test and Wilks’ theorem When estimators are not available in closed-forms, the likelihood-ratio test (LRT) serves as a very general testing statistic under the likelihood principle. Let \\(\\ell_{n}\\left(\\theta\\right)=n^{-1}\\sum_{i}\\log f\\left(x_{i};\\theta\\right)\\) be the average sample log-likelihood, and \\(\\widehat{\\theta}=\\arg\\max_{\\theta\\in\\Theta}\\ell_{n}\\left(\\theta\\right)\\) is the maximum likelihood estimator (MLE). Take a Taylor expansion of \\(\\ell_{n}\\left(\\theta_{0}\\right)\\) around \\(\\ell_{n}\\left(\\widehat{\\theta}\\right)\\): \\[\\begin{aligned} \\ell_{n}\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\widehat{\\theta}\\right) &amp; =\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\widehat{\\theta}\\right)&#39;\\left(\\theta_{0}-\\widehat{\\theta}\\right)+\\frac{1}{2}\\left(\\theta_{0}-\\widehat{\\theta}\\right)&#39;\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\theta_{0}-\\widehat{\\theta}\\right)+O\\left(\\left\\Vert \\widehat{\\theta}-\\theta_{0}\\right\\Vert _{2}^{3}\\right)\\\\ &amp; =\\frac{1}{2}\\left(\\widehat{\\theta}-\\theta_{0}\\right)&#39;\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)+O\\left(\\left\\Vert \\widehat{\\theta}-\\theta_{0}\\right\\Vert _{2}^{3}\\right)\\\\ &amp; =\\frac{1}{2}\\left(\\widehat{\\theta}-\\theta_{0}\\right)&#39;\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)+o_{p}\\left(1\\right)\\end{aligned}\\] by that \\(\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\widehat{\\theta}\\right)=0\\) due to the first order condition of optimality. Define \\(L_{n}\\left(\\theta\\right):=\\sum_{i}\\log f\\left(x_{i};\\theta\\right)\\), and the likelihood-ratio statistic as \\[\\mathcal{LR}:=2\\left(L_{n}\\left(\\widehat{\\theta}\\right)-L_{n}\\left(\\theta_{0}\\right)\\right)=2n\\left(\\ell_{n}\\left(\\widehat{\\theta}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)\\right).\\] Obviously \\(\\mathcal{LR}\\geq0\\) because \\(\\widehat{\\theta}\\) maximizes \\(\\ell_{n}\\left(\\theta\\right)\\). Multiply \\(-2n\\) to the two sides of the above Taylor expansion: \\[\\mathcal{LR}=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)&#39;\\left(-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)+o_{p}\\left(1\\right)\\] Notice that when the model is correctly specified we have \\[\\begin{aligned} -\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\theta_{0}\\right) &amp; \\stackrel{p}{\\to}-\\mathcal{H}\\left(\\theta_{0}\\right)=\\mathcal{I}\\left(\\theta_{0}\\right)\\\\ \\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) &amp; \\stackrel{d}{\\to}N\\left(0,\\mathcal{I}^{-1}\\left(\\theta_{0}\\right)\\right)\\end{aligned}\\] By Slutsky’s theorem: \\[\\left(-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta&#39;}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)^{1/2}\\left[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\right]\\stackrel{d}{\\to}\\mathcal{I}^{1/2}\\left(\\theta_{0}\\right)\\times N\\left(0,\\mathcal{I}^{-1}\\left(\\theta_{0}\\right)\\right)\\sim N\\left(0,I_{k}\\right).\\] and then \\(\\mathcal{LR}\\stackrel{d}{\\to}\\chi_{K}^{2}\\) by the continuous mapping theorem. Wilks’ theorem, or Wilks’ phenomenon is referred to the fact that \\(\\mathcal{LR}\\stackrel{d}{\\to}\\chi^{2}\\left(K\\right)\\) when the parametric model is correctly specified. 8.1.4 Score Test 8.2 Confidence Interval\\[confidence-interval\\] An interval estimate is a function \\(C:\\mathcal{X}^{n}\\mapsto\\left\\{ \\Theta_{1}:\\Theta_{1}\\subseteq\\Theta\\right\\}\\) that maps a point in the sample space to a subset of the parameter space. The coverage probability of an interval estimator \\(C\\left(\\mathbf{X}\\right)\\) is defined as \\(P_{\\theta}\\left(\\theta\\in C\\left(\\mathbf{X}\\right)\\right)\\). When \\(\\theta\\) is of one dimension, we usually call the interval estimator confidence interval. When \\(\\theta\\) is of multiple dimensions, we call the it confidence region and it of course includes the one-dimensional \\(\\theta\\) as a special case. The coverage probability is the frequency that the interval estimator captures the true parameter that generates the sample. From the frequentist perspective, the parameter is fixed while the confidence region is random. It is not the probability that \\(\\theta\\) is inside the given confidence interval. Suppose a random sample of size 6 is generated from \\(\\left(X_{1},\\ldots,X_{6}\\right)\\sim\\text{iid }N\\left(\\theta,1\\right).\\) Find the coverage probability of the random interval is \\(\\left[\\bar{X}-1.96/\\sqrt{6},\\ \\bar{X}+1.96/\\sqrt{6}\\right].\\) Hypothesis testing and confidence region are closely related. Sometimes it is difficult to directly construct the confidence region, but easy to test a hypothesis. One way to construct confidence region is by inverting a test. Suppose \\(\\phi_{\\theta}\\) is a test of size \\(\\alpha\\). If \\(C\\left(\\mathbf{X}\\right)\\) is constructed as \\[C\\left(\\mathbf{X}\\right)=\\left\\{ \\theta\\in\\Theta:\\phi\\left(\\mathbf{X}\\right)=0\\right\\} .\\] The coverage probability of the true data generating parameter \\(\\theta\\) is \\[P_{\\theta}\\left\\{ \\theta\\in C\\left(\\mathbf{X}\\right)\\right\\} =P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=0\\right\\} =1-P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=1\\right\\} =1-\\beta\\left(\\theta\\right)\\geq1-\\alpha\\] where the last inequality follows as \\(\\beta\\left(\\theta\\right)\\leq\\alpha\\) for \\(\\theta\\in\\Theta_{0}\\). If \\(\\Theta_{0}\\) is a singleton, the equality holds. knitr 8.3 Bayesian Credible Set The Bayesian framework offers a coherent and natural language for statistical decision. However, the major criticism against Bayesian statistics is the arbitrariness of the choice of the prior. The Bayesian approach views both the data \\(\\mathbf{X}_{n}\\) and the parameter \\(\\theta\\) as random variables. Before she observes the data, she holds a prior distribution \\(\\pi\\) about \\(\\theta\\). After observing the data, she updates the prior distribution to a posterior distribution \\(p(\\theta|\\mathbf{X}_{n})\\). The Bayes Theorem connects the prior and the posterior as \\[p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)\\] where \\(f(\\mathbf{X}_{n}|\\theta)\\) is the likelihood function. Here is a classical example to illustrate the Bayesian approach to statistical inference. Suppose \\(\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})\\) is an iid sample drawn from a normal distribution with unknown \\(\\theta\\) and known \\(\\sigma\\). If a researcher’s prior distribution \\(\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})\\), her posterior distribution is, by some routine calculation, also a normal distribution \\[p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),\\] where \\(\\tilde{\\theta}=\\frac{\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\theta_{0}+\\frac{n\\sigma_{0}^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\bar{x}\\) and \\(\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\). Thus the Bayesian credible set is \\[\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).\\] This posterior distribution depends on \\(\\theta_{0}\\) and \\(\\sigma_{0}^{2}\\) from the prior. When the sample size is sufficiently large the posterior can be approximated by \\(N(\\bar{x},\\sigma^{2}/n)\\), where the prior information is overwhelmed by the information accumulated from the data. In contrast, a frequentist will estimate \\(\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)\\). Her confidence interval is \\[\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).\\] The Bayesian credible set and the frequentist confidence interval are different for finite \\(n\\), but they coincide when \\(n\\to\\infty\\). 8.4 Applications in OLS We will introduce three tests for a hypothesis of the linear regression coefficients, namely the Wald test, the Lagrangian multiplier (LM) test, and the likelihood ratio test. The Wald test is based on the unrestricted OLS estimator \\(\\widehat{\\beta}\\). The LM test is based on the restricted estimator \\(\\tilde{\\beta}\\). The LRT, as we have discussed, is based on the difference of the log-likelihood function evaluated at the unrestricted OLS estimator and that on the restricted estimator. Let \\(R\\) be a \\(q\\times K\\) constant matrix with \\(q\\leq K\\) and \\(\\mbox{rank}\\left(R\\right)=q\\). All linear restrictions about \\(\\beta\\) can be written in the form of \\(R\\beta=r\\), where \\(r\\) is a \\(q\\times1\\) constant vector. We want to simultaneously test \\(\\beta_{1}=1\\) and \\(\\beta_{3}+\\beta_{4}=2\\) in the above example. The null hypothesis can be expressed in the general form \\(R\\beta=r\\), where the restriction matrix \\(R\\) is \\[R=\\begin{pmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\end{pmatrix}\\] and \\(r=\\left(1,2\\right)&#39;\\). 8.4.1 Wald Test Suppose the OLS estimator \\(\\widehat{\\beta}\\) is asymptotic normal, i.e. \\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)\\] where \\(\\Omega\\) is a \\(K\\times K\\) positive definite covariance matrix and. Since \\(R\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,R\\Omega R&#39;\\right)\\), the quadratic form \\[n\\left(\\widehat{\\beta}-\\beta\\right)&#39;R&#39;\\left(R\\Omega R&#39;\\right)^{-1}R\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}.\\] Now we intend to test the linear null hypothesis \\(R\\beta=r\\). Under the null, the Wald statistic \\[\\mathcal{W}=n\\left(R\\widehat{\\beta}-r\\right)&#39;\\left(R\\widehat{\\Omega}R&#39;\\right)^{-1}\\left(R\\widehat{\\beta}-r\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}\\] where \\(\\widehat{\\Omega}\\) is a consistent estimator of \\(\\Omega\\). (Single test) In a linear regression \\[\\begin{aligned}y &amp; =x_{i}&#39;\\beta+e_{i}=\\sum_{k=1}^{5}\\beta_{k}x_{ik}+e_{i}.\\nonumber\\\\ E\\left[e_{i}x_{i}\\right] &amp; =\\mathbf{0}_{5},\\label{eq:example} \\end{aligned}\\] where \\(y\\) is wage and \\[x=\\left(\\mbox{edu},\\mbox{age},\\mbox{experience},\\mbox{experience}^{2},1\\right)&#39;.\\] To test whether education affects wage, we specify the null hypothesis \\(\\beta_{1}=0\\). Let \\(R=\\left(1,0,0,0,0\\right)\\) and \\(r=0\\). \\[\\sqrt{n}\\widehat{\\beta}_{1}=\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)=\\sqrt{n}R\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,R\\Omega R&#39;\\right)\\sim N\\left(0,\\Omega_{11}\\right),\\label{eq:R11}\\] where \\(\\Omega{}_{11}\\) is the \\(\\left(1,1\\right)\\) (scalar) element of \\(\\Omega\\). Under \\[H_{0}:R\\beta=\\left(1,0,0,0,0\\right)\\left(\\beta_{1},\\ldots,\\beta_{5}\\right)&#39;=\\beta_{1}=0,\\] we have \\(\\sqrt{n}R\\left(\\widehat{\\beta}-\\beta\\right)=\\sqrt{n}\\widehat{\\beta}_{1}\\stackrel{d}{\\to}N\\left(0,\\Omega_{11}\\right).\\) Therefore, \\[\\sqrt{n}\\frac{\\widehat{\\beta}_{1}}{\\widehat{\\Omega}_{11}^{1/2}}=\\sqrt{\\frac{\\Omega_{11}}{\\widehat{\\Omega}_{11}}}\\sqrt{n}\\frac{\\widehat{\\beta}_{1}}{\\sqrt{\\Omega_{11}}}\\] If \\(\\widehat{\\Omega}\\stackrel{p}{\\to}\\Omega\\), then \\(\\left(\\Omega_{11}/\\widehat{\\Omega}_{11}\\right)^{1/2}\\stackrel{p}{\\to}1\\) by the continuous mapping theorem. As \\(\\sqrt{n}\\widehat{\\beta}_{1}/\\Omega_{11}^{1/2}\\stackrel{d}{\\to}N\\left(0,1\\right)\\), we conclude \\(\\sqrt{n}\\widehat{\\beta}_{1}/\\widehat{\\Omega}_{11}^{1/2}\\stackrel{d}{\\to}N\\left(0,1\\right).\\) The above example is a test about a single coefficient, and the test statistic is essentially the square of the t-statistic, and the null distribution is the square of a standard normal. In order to test a nonlinear regression, we use the delta method. (This is not a good example because it can be rewritten into a linear hypothesis.) In the example of linear regression, the optimal experience level can be found by setting to zero the first order condition with respective to experience, \\(\\beta_{3}+2\\beta_{4}\\mbox{experience}^{*}=0\\). We test the hypothesis that the optimal experience level is 20 years; in other words, \\[\\mbox{experience}^{*}=-\\frac{\\beta_{3}}{2\\beta_{4}}=20.\\] This is a nonlinear hypothesis. If \\(q\\leq K\\) where \\(q\\) is the number of restrictions, we have \\[n\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)&#39;\\left(\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)\\Omega\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)&#39;\\right)^{-1}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\to}\\chi_{q}^{2},\\] where in this example, \\(\\theta=\\beta\\), \\(f\\left(\\beta\\right)=-\\beta_{3}/\\left(2\\beta_{4}\\right)\\). The gradient \\[\\frac{\\partial f}{\\partial\\beta&#39;}\\left(\\beta\\right)=\\left(0,0,-\\frac{1}{2\\beta_{4}},\\frac{\\beta_{3}}{2\\beta_{4}^{2}},0\\right)\\] Since \\(\\widehat{\\beta}\\stackrel{p}{\\to}\\beta_{0}\\), by the continuous mapping theorem, if \\(\\beta_{0,4}\\neq0\\), we have \\(\\frac{\\partial}{\\partial\\beta}f\\left(\\widehat{\\beta}\\right)\\stackrel{p}{\\to}\\frac{\\partial}{\\partial\\beta}f\\left(\\beta_{0}\\right)\\). Therefore, the (nonlinear) Wald test is \\[\\mathcal{W}=n\\left(f\\left(\\widehat{\\beta}\\right)-20\\right)&#39;\\left(\\frac{\\partial f}{\\partial\\beta&#39;}\\left(\\widehat{\\beta}\\right)\\widehat{\\Omega}\\frac{\\partial f}{\\partial\\beta&#39;}\\left(\\widehat{\\beta}\\right)\\right)^{-1}\\left(f\\left(\\widehat{\\beta}\\right)-20\\right)\\stackrel{d}{\\to}\\chi_{1}^{2}.\\] This is a valid test with correct asymptotic size. However, we can equivalently state the null hypothesis as \\(\\beta_{3}+40\\beta_{4}=0\\) and we can construct a Wald statistic accordingly. Asymptotically equivalent though, in general a linear hypothesis is preferred to a nonlinear one, due to the approximation error in the delta method under the null and more importantly the invalidity of the Taylor expansion under the alternative. It also highlights the problem of Wald test being variant to re-parametrization. 8.4.2 Lagrangian Multiplier Test The key difference between the Wald test and LM test is that the former is based on the unrestricted OLS estimator while the latter is based on the restricted OLS estimator. Estimate the constrained OLS estimator \\[\\min_{\\beta}\\left(y-X\\beta\\right)&#39;\\left(y-X\\beta\\right)\\mbox{ s.t. }R\\beta=r.\\] We know that the restricted minimization problem can be converted into an unrestricted problem \\[L\\left(\\beta,\\lambda\\right)=\\frac{1}{2n}\\left(y-X\\beta\\right)&#39;\\left(y-X\\beta\\right)+\\lambda&#39;\\left(R\\beta-r\\right),\\label{eq:Lagran}\\] where \\(L\\left(\\beta,\\lambda\\right)\\) is called the Lagrangian, and \\(\\lambda\\) is the Lagrangian multiplier. The LM test is also called the score test, because the derivation is based on the score function of the restricted OLS estimator. Set the first-order condition of \\[eq:Lagran\\] as zero: \\[\\begin{aligned} \\frac{\\partial}{\\partial\\beta}L &amp; =-\\frac{1}{n}X&#39;\\left(y-X\\tilde{\\beta}\\right)+\\tilde{\\lambda}R=-\\frac{1}{n}X&#39;e+\\frac{1}{n}X&#39;X\\left(\\tilde{\\beta}-\\beta_{0}\\right)+R&#39;\\tilde{\\lambda}=0.\\\\ \\frac{\\partial}{\\partial\\lambda}L &amp; =R\\tilde{\\beta}-r=R\\left(\\tilde{\\beta}-\\beta_{0}\\right)=0\\end{aligned}\\] where \\(\\tilde{\\beta}\\) and \\(\\tilde{\\lambda}\\) denote the roots of these equation, and \\(\\beta_{0}\\) is the hypothesized true value. The two equations can be written as a linear system \\[\\begin{pmatrix}\\widehat{Q} &amp; R&#39;\\\\ R &amp; 0 \\end{pmatrix}\\begin{pmatrix}\\tilde{\\beta}-\\beta_{0}\\\\ \\tilde{\\lambda} \\end{pmatrix}=\\begin{pmatrix}\\frac{1}{n}X&#39;e\\\\ 0 \\end{pmatrix},\\] where \\(\\hat{Q}=X&#39;X/n\\). \\[\\begin{pmatrix}\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1} &amp; \\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}\\\\ \\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1} &amp; -(R&#39;Q^{-1}R)^{-1} \\end{pmatrix}\\begin{pmatrix}\\widehat{Q} &amp; R&#39;\\\\ R &amp; 0 \\end{pmatrix}=I_{K+q}.\\] Given the above fact, we can explicitly express \\[\\begin{aligned} \\begin{pmatrix}\\tilde{\\beta}-\\beta_{0}\\\\ \\tilde{\\lambda} \\end{pmatrix}\\begin{aligned}=\\end{aligned} &amp; \\begin{pmatrix}\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1} &amp; \\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}\\\\ \\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1} &amp; -(R&#39;Q^{-1}R)^{-1} \\end{pmatrix}\\begin{pmatrix}\\frac{1}{n}X&#39;e\\\\ 0 \\end{pmatrix}\\\\ = &amp; \\begin{pmatrix}\\widehat{Q}^{-1}\\frac{1}{n}X&#39;e-\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{n}X&#39;e\\\\ \\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{n}X&#39;e \\end{pmatrix}\\end{aligned}\\] The \\(\\tilde{\\lambda}\\) component is \\[\\begin{aligned} \\sqrt{n}\\tilde{\\lambda} &amp; =\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;e\\\\ &amp; \\stackrel{d}{\\to}N\\left(0,\\left(RQ^{-1}R&#39;\\right)^{-1}RQ^{-1}\\Omega Q^{-1}R&#39;\\left(RQ^{-1}R&#39;\\right)^{-1}\\right)\\end{aligned}\\] as \\(\\widehat{Q}\\stackrel{p}{\\to}Q\\). Denote \\(\\Sigma=\\left(RQ^{-1}R&#39;\\right)^{-1}RQ^{-1}\\Omega Q^{-1}R&#39;\\left(RQ^{-1}R&#39;\\right)^{-1}\\), we have \\[n\\tilde{\\lambda}&#39;\\Sigma^{-1}\\tilde{\\lambda}\\stackrel{d}{\\to}\\chi_{q}^{2}.\\] Let \\[\\widehat{\\Sigma}=\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\widehat{\\Omega}\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}.\\] If \\(\\widehat{\\Omega}\\stackrel{p}{\\to}\\Omega\\), we have \\[\\begin{aligned} \\mathcal{LM} &amp; =n\\tilde{\\lambda}&#39;\\widehat{\\Sigma}^{-1}\\tilde{\\lambda}=n\\tilde{\\lambda}&#39;\\Sigma^{-1}\\tilde{\\lambda}+n\\tilde{\\lambda}&#39;\\left(\\widehat{\\Sigma}^{-1}-\\Sigma^{-1}\\right)\\tilde{\\lambda}\\\\ &amp; =n\\tilde{\\lambda}&#39;\\Sigma^{-1}\\tilde{\\lambda}+o_{p}\\left(1\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}.\\end{aligned}\\] This is the general expression of the LM test. In the special case of homoskedasticity, \\(\\Sigma=\\sigma^{2}\\left(RQ^{-1}R&#39;\\right)^{-1}RQ^{-1}QQ^{-1}R&#39;\\left(RQ^{-1}R&#39;\\right)^{-1}=\\sigma^{2}\\left(RQ^{-1}R&#39;\\right)^{-1}.\\) Replace \\(\\Sigma\\) with the estimated \\(\\hat{\\Sigma}\\), we have \\[\\begin{aligned}\\frac{n\\tilde{\\lambda}&#39;R\\hat{Q}^{-1}R&#39;\\tilde{\\lambda}}{\\hat{\\sigma}^{2}} &amp; =\\frac{1}{n\\hat{\\sigma}^{2}}\\left(y-X\\tilde{\\beta}\\right)&#39;X\\hat{Q}^{-1}R&#39;(R\\hat{Q}^{-1}R&#39;)^{-1}R\\hat{Q}^{-1}X&#39;\\left(y-X\\tilde{\\beta}\\right)\\stackrel{d}{\\to}\\chi_{q}^{2}.\\end{aligned}\\] If we test the hypothesis that the optimal experience level is 20 years; \\(\\mbox{experience}^{*}=-\\frac{\\beta_{3}}{2\\beta_{4}}=20.\\) We can replace \\(\\beta_{3}\\) by \\(-40\\beta_{4}\\) so we only need to estimate 3 slope coefficients in the OLS to construct the LM test. Moreover, the LM test is invariant to re-parametrization. 8.4.3 Likelihood-Ratio Test for Regression In the previous section we have discussed the LRT. Here we put it into the context regression with Gaussian error. Let \\(\\gamma=\\sigma_{e}^{2}\\). Under the classical assumptions of normal regression model, \\[L_{n}\\left(\\beta,\\gamma\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\gamma-\\frac{1}{2\\gamma}\\left(Y-X\\beta\\right)&#39;\\left(Y-X\\beta\\right).\\] For the unrestricted estimator, we know \\[\\widehat{\\gamma}=\\gamma\\left(\\widehat{\\beta}\\right)=n^{-1}\\left(Y-X\\widehat{\\beta}\\right)&#39;\\left(Y-X\\widehat{\\beta}\\right)\\] and the sample log-likelihood function evaluated at the MLE is \\[\\widehat{L}_{n}=L_{n}\\left(\\widehat{\\beta},\\widehat{\\gamma}\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\widehat{\\gamma}-\\frac{n}{2}\\] and the restricted estimator \\(\\tilde{L}_{n}=L_{n}\\left(\\tilde{\\beta},\\tilde{\\gamma}\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\tilde{\\gamma}-\\frac{n}{2}\\). The likelihood ratio is \\[\\begin{aligned} \\mathcal{LR} &amp; =2\\left(\\widehat{L}_{n}-\\tilde{L}_{n}\\right)=n\\log\\left(\\tilde{\\gamma}/\\widehat{\\gamma}\\right).\\end{aligned}\\] If the normal regression is correctly specified, we can immediately conclude \\(\\mathcal{LR}\\stackrel{d}{\\to}\\chi_{q}^{2}.\\) Now we drop the Gaussian error assumption while keep the conditional homoskedasticity. In this case, the classical results is not applicable because \\(L_{n}\\left(\\beta,\\gamma\\right)\\) is not a (genuine) log-likelihood function; instead it is the quasi log-likelihood function. Notice \\[\\begin{aligned} \\mathcal{LR} &amp; =n\\log\\left(1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}\\right)=n\\left(\\log1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}+O\\left(\\frac{\\left|\\tilde{\\gamma}-\\widehat{\\gamma}\\right|^{2}}{\\widehat{\\gamma}^{2}}\\right)\\right)\\nonumber \\\\ &amp; =n\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}+o_{p}\\left(1\\right)\\label{eq:LRT1}\\end{aligned}\\] by a Taylor expansion of \\(\\log\\left(1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}\\right)\\) around \\(\\log1=0\\). We focus on \\[\\begin{aligned} n\\left(\\tilde{\\gamma}-\\widehat{\\gamma}\\right) &amp; =n\\left(\\gamma\\left(\\tilde{\\beta}\\right)-\\gamma\\left(\\widehat{\\beta}\\right)\\right)\\nonumber \\\\ &amp; =n\\left(\\frac{\\partial\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+\\frac{1}{2}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)&#39;\\frac{\\partial^{2}\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta\\partial\\beta&#39;}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+O\\left(\\left\\Vert \\tilde{\\beta}-\\widehat{\\beta}\\right\\Vert _{2}^{3}\\right)\\right)\\nonumber \\\\ &amp; =\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)&#39;\\widehat{Q}\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+o_{p}\\left(1\\right)\\label{eq:LRT2}\\end{aligned}\\] where the last line follows by \\(\\frac{\\partial\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta}=-\\frac{2}{n}X&#39;\\left(Y-X\\widehat{\\beta}\\right)=-\\frac{2}{n}X&#39;\\widehat{e}=0\\) and \\(\\frac{1}{2}\\cdot\\frac{\\partial^{2}\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta\\partial\\beta&#39;}=\\frac{1}{2}\\cdot\\frac{2}{n}X&#39;X=\\widehat{Q}\\). From the derivation of LM test, we have \\[\\begin{aligned}\\sqrt{n}\\left(\\tilde{\\beta}-\\beta_{0}\\right) &amp; =\\left(\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\right)\\frac{1}{\\sqrt{n}}X&#39;e\\\\ &amp; =\\frac{1}{\\sqrt{n}}\\left(X&#39;X\\right)^{-1}X&#39;e-\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;e\\\\ &amp; =\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{0}\\right)-\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;e. \\end{aligned}\\] Rearrange the above equation to obtain \\[\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)=-\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;e\\] and thus the quadratic form \\[\\begin{aligned} &amp; &amp; \\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)&#39;\\widehat{Q}\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)\\nonumber \\\\ &amp; = &amp; \\frac{1}{\\sqrt{n}}e&#39;X\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\widehat{Q}\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;e\\nonumber \\\\ &amp; = &amp; \\frac{1}{\\sqrt{n}}e&#39;X\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;e\\nonumber \\\\ &amp; = &amp; \\frac{1}{\\sqrt{n}}e&#39;X\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;e.\\label{eq:LRT3}\\end{aligned}\\] Collecting (\\[eq:LRT1\\]), (\\[eq:LRT2\\]) and (\\[eq:LRT3\\]), we have \\[\\begin{aligned} \\mathcal{LR} &amp; =n\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\cdot\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\sigma_{e}^{2}}+o_{p}\\left(1\\right)\\\\ &amp; =\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\frac{1}{\\sqrt{n}}\\frac{e}{\\sigma_{e}}&#39;X\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;\\frac{e}{\\sigma_{e}}+o_{p}\\left(1\\right)\\end{aligned}\\] Notice that under homoskedasticity, CLT gives \\[\\begin{aligned} R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;\\frac{e}{\\sigma_{e}} &amp; =R\\widehat{Q}^{-1/2}\\widehat{Q}^{-1/2}\\frac{1}{\\sqrt{n}}X&#39;\\frac{e}{\\sigma_{e}}\\\\ &amp; \\stackrel{d}{\\to}RQ^{-1/2}\\times N\\left(0,I_{K}\\right)\\sim N\\left(0,RQ^{-1}R&#39;\\right),\\end{aligned}\\] and thus \\[\\frac{1}{\\sqrt{n}}\\frac{e}{\\sigma_{e}}&#39;X\\widehat{Q}^{-1}R&#39;\\left(R\\widehat{Q}^{-1}R&#39;\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X&#39;\\frac{e}{\\sigma_{e}}\\stackrel{d}{\\to}\\chi_{q}^{2}.\\] Moreover, \\(\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\stackrel{p}{\\to}1\\). By Slutsky’s theorem, we conclude \\[\\mathcal{LR}\\stackrel{d}{\\to}\\chi_{q}^{2}.\\] under homoskedasticity. 8.5 Summary Applied econometrics is a field obsessed of hypothesis testing, in the hope to establish at least statistical association and ideally causality. Hypothesis testing is a fundamentally important topic in statistics. The states and the decisions in Table \\[tab:Decisions-and-States\\] remind us the intrinsic connections with game theory in economics. I, a game player, plays a sequential game against the “nature”. Step0: The parameter space \\(\\Theta\\) is partitioned into the null hypothesis \\(\\Theta_{0}\\) and the alternative hypothesis \\(\\Theta_{1}\\) according to a scientific theory. Step1: Before I observe the data, I design a test function \\(\\phi\\) according to \\(\\Theta_{0}\\) and \\(\\Theta_{1}\\). In game theory terminology, the contingency plan \\(\\phi\\) is my strategy. Step2: Once I observe the fixed data \\(\\mathbf{x}\\), I act according to the instruction of \\(\\phi\\left(\\mathbf{x}\\right)\\) — either accept \\(\\Theta_{0}\\) or reject \\(\\Theta_{0}\\). Step3: Nature reveals the true parameter \\(\\theta^{*}\\) behind \\(\\mathbf{x}\\). Then I can evaluate the gain/loss of my decision \\(\\phi\\left(\\mathbf{x}\\right)\\). When the loss function (negative payoff) is specified as \\[\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)=\\phi\\left(\\mathbf{x}\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\phi\\left(\\mathbf{x}\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} ,\\] the randomness of the data will incur the risk (expected loss) \\[\\mathscr{R}\\left(\\theta,\\phi\\right)=E\\left[\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)\\right]=\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} .\\] I am a rational person. I understand the structure of the game and I want to do a good job in Step 1 in designing my strategy. I want to minimize my risk. If I am a frequentist, one and only one of \\(1\\left\\{ \\theta\\in\\Theta_{0}\\right\\}\\) and \\(1\\left\\{ \\theta\\in\\Theta_{1}\\right\\}\\) can happen. An unbiased test makes sure \\(\\sup_{\\theta\\in\\Theta_{0}}\\beta_{\\phi}\\left(\\theta\\right)\\leq\\alpha\\). When many tests are unbiased, ideally I would like to pick the best one. If it exists, in a class \\(\\Psi_{\\alpha}\\) of unbiased tests of size \\(\\alpha\\) the uniformly most power test \\(\\phi^{*}\\) satisfies \\(\\mathscr{R}\\left(\\theta,\\phi^{*}\\right)\\geq\\sup_{\\phi\\in\\Psi_{\\alpha}}\\mathscr{R}\\left(\\theta,\\phi\\right)\\) for every \\(\\theta\\in\\Theta_{1}\\). For simple versus simple tests, LRT is the uniformly most powerful test according to Neyman-Pearson Lemma. If I am a Bayesian, I do not mind imposing probability (weight) on the parameter space, which is my prior belief \\(\\pi\\left(\\theta\\right)\\). My Bayesian risk becomes \\[\\begin{aligned} \\mathscr{BR}\\left(\\pi,\\phi\\right) &amp; =E_{\\pi\\left(\\theta\\right)}\\left[\\mathscr{R}\\left(\\theta,\\phi\\right)\\right]=\\int\\left[\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} \\right]\\pi\\left(\\theta\\right)d\\theta\\\\ &amp; =\\int_{\\left\\{ \\theta\\in\\Theta_{0}\\right\\} }\\beta_{\\phi}\\left(\\theta\\right)\\pi\\left(\\theta\\right)d\\theta+\\int_{\\left\\{ \\theta\\in\\Theta_{1}\\right\\} }(1-\\beta_{\\phi}\\left(\\theta\\right))\\pi\\left(\\theta\\right)d\\theta.\\end{aligned}\\] This is the average (with respect to \\(\\pi\\left(\\theta\\right)\\)) risk over the null and the alternative. Historical notes: Hypothesis testing started to take the modern shape at the beginning of the 20th century. Karl Pearson (1957–1936) laid the foundation of hypothesis testing and introduced the \\(\\chi^{2}\\) test, the \\(p\\)-value, among many other concepts that we keep using today. Neyman-Pearson Lemma was named after Jerzy Neyman (1894–1981) and Egon Pearson (1895–1980), Karl’s son. Further reading: Young and Smith (2005) is a concise but in-depth reference for statistical inference. 8.6 Appendix 8.6.1 Neyman-Pearson Lemma We have discussed an example of the uniformly most power test in the Gaussian location model. Under the likelihood principle, if the test is a simple versus simple (the null hypothesis is a singleton \\(\\theta_{0}\\) and the alternative hypothesis is another single point \\(\\theta_{1}\\)), then LRT \\[\\begin{aligned} \\phi\\left(\\mathbf{X}\\right) &amp; :=1\\left\\{ \\mathcal{LR}\\geq c_{LR}\\right\\} ,\\end{aligned}\\] where \\(c_{LR}\\) is the critical value depending on the size of the the test, is a uniformly most powerful test. This result is the well-known Neyman-Pearson Lemma. Notice \\(\\exp\\left(L_{n}\\left(\\theta\\right)\\right)=\\Pi_{i}f\\left(x_{i};\\theta\\right)=f\\left(\\mathbf{x};\\theta\\right)\\) where \\(f\\left(\\mathbf{x};\\theta_{0}\\right)\\) is the joint density of \\(\\left(x_{1},\\ldots,x_{n}\\right)\\), the LRT can be equivalently written in likelihood ratio form (without log) \\[\\phi\\left(\\mathbf{X}\\right)=1\\left\\{ f\\left(\\mathbf{X};\\widehat{\\theta}\\right)/f\\left(\\mathbf{X};\\theta_{0}\\right)\\geq c\\right\\}\\] where \\(c:=\\exp\\left(c_{LR}/2\\right)\\). To see its is the most power test in the simple to simple context, consider another test \\(\\psi\\) of the same size at the single null hypothesis \\(\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\theta_{0}\\right)=\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)=\\alpha\\), where \\(f\\left(\\mathbf{x};\\theta_{0}\\right)=\\) is the joint density of the sample \\(\\mathbf{X}\\). For any constant \\(c&gt;0\\), the power of \\(\\phi\\) at the alternative \\(\\theta_{1}\\) is \\[\\begin{aligned} E_{\\theta_{1}}\\left[\\phi\\left(\\mathbf{X}\\right)\\right] &amp; =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)\\nonumber \\\\ &amp; =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)-c\\left[\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)-\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\right]\\nonumber \\\\ &amp; =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)-c\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\nonumber \\\\ &amp; =\\int\\phi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right).\\label{eq:NP1}\\end{aligned}\\] Define \\(\\xi_{c}:=f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\). The fact that \\(\\phi\\left(\\mathbf{x}\\right)=1\\) if \\(\\xi_{c}\\geq0\\) and \\(\\phi\\left(\\mathbf{x}\\right)=0\\) if \\(\\xi_{c}&lt;0\\) implies \\[\\begin{aligned} &amp; &amp; \\int\\phi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)=\\int\\phi\\left(\\mathbf{x}\\right)\\xi_{c}\\\\ &amp; = &amp; \\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\phi\\left(\\mathbf{x}\\right)\\xi_{c}+\\int_{\\left\\{ \\xi_{c}&lt;0\\right\\} }\\phi\\left(\\mathbf{x}\\right)\\xi_{c}=\\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\xi_{c}=\\int\\xi_{c}\\cdot1\\left\\{ \\xi_{c}\\geq0\\right\\} \\\\ &amp; \\geq &amp; \\int\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\cdot1\\left\\{ \\xi_{c}\\geq0\\right\\} =\\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\\\ &amp; \\geq &amp; \\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}+\\int_{\\left\\{ \\xi_{c}&lt;0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}=\\int\\psi\\left(x\\right)\\xi_{c}\\\\ &amp; = &amp; \\int\\psi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)\\end{aligned}\\] where the first inequality follows because the test function \\(0\\leq\\psi\\left(\\mathbf{x}\\right)\\leq1\\) for any realization of \\(\\mathbf{x}\\), and where the second inequality holds because \\(\\int_{\\left\\{ \\xi_{c}&lt;0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\leq0\\). We continue \\[eq:NP1\\]: \\[\\begin{aligned} E_{\\theta_{1}}\\left[\\phi\\left(\\mathbf{X}\\right)\\right] &amp; \\geq &amp; \\int\\psi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\\\ &amp; = &amp; \\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)=E_{\\theta_{1}}\\left[\\psi\\left(\\mathbf{X}\\right)\\right].\\end{aligned}\\] In other words, \\(\\phi\\left(\\mathbf{X}\\right)\\) is more powerful at \\(\\theta_{1}\\) than any other test \\(\\psi\\) of the same size at the null. Neyman-Pearson lemma establishes the optimality of LRT in single versus simple hypothesis testing. It can be generalized to show the existence of the uniformly most power test in one sided composite null hypothesis \\(H_{0}:\\theta\\leq\\theta_{0}\\) or \\(H_{0}:\\theta\\geq\\theta_{0}\\) in the parametric class of distributions exhibiting monotone likelihood ratio. Zhentao Shi. Nov 4, 2020. References "],
["panel-data.html", "9 Panel Data 9.1 Fixed Effect 9.2 Random Effect 9.3 Summary", " 9 Panel Data Economists mostly work with observational data. The data generation process is out of the control of researchers. If we only have a cross sectional dataset at hand, it is difficult to deal with heterogeneity among the individuals. On the other hand, panel data offers a chance to handle heterogeneity of some particular forms. A panel dataset tracks the same individuals across time \\(t=1,\\ldots,T\\). We assume the observations are independent across \\(i=1,\\ldots,n\\), while we allow some form of dependence within a group across \\(t=1,\\ldots,T\\) for the same \\(i\\). We maintain the linear equation \\[y_{it}=\\beta_{1}+x_{it}&#39;\\beta_{2}+u_{it},\\ i=1,\\ldots,n;t=1,\\ldots,T\\label{eq:basic_eq}\\] where \\(u_{it}=\\alpha_{i}+\\epsilon_{it}\\) is called the composite error. Note that \\(\\alpha_{i}\\) is the time-invariant unobserved heterogeneity, while \\(\\epsilon_{it}\\) varies across individuals and time periods. The most important techniques of panel data estimation are the fixed effect regression and the random effect regression. The asymptotic distributions of both estimators can be derived from knowledge about the OLS regression. In this sense, panel data estimation becomes applied examples of the theory that we have covered in this course. It highlights the fundamental role of linear regression theory in econometrics. 9.1 Fixed Effect The unobservable individual-specific heterogeneity \\(\\alpha_{i}\\) is absorbed into the composite error \\(u_{it}=\\alpha_{i}+\\epsilon_{it}\\). If \\(\\mathrm{cov}\\left(\\alpha_{i},x_{it}\\right)=0\\), the OLS is consistent; otherwise the consistency breaks down. The fixed effect model allows \\(\\alpha_{i}\\) and \\(x_{it}\\) to be arbitrarily correlated. Let us rewrite (\\[eq:basic\\_eq\\]) as \\[y_{it}=w_{it}&#39;\\boldsymbol{\\beta}+u_{it},\\label{eq:basic_eq2}\\] where \\(\\boldsymbol{\\beta}=\\left(\\beta_{1},\\beta_{2}&#39;\\right)&#39;\\) and \\(w_{it}=\\left(1,x_{it}&#39;\\right)&#39;\\) are \\(K+1\\) vectors, i.e., \\(\\boldsymbol{\\beta}\\) is the parameter including the intercept, and \\(w_{it}\\) is the explanatory variables including the constant. Show that \\(\\mathrm{cov}\\left(\\alpha_{i},x_{it}\\right)\\neq0\\), running OLS in (\\[eq:basic\\_eq2\\]) will deliver an inconsistent estimator. While naively run OLS in the fixed effect model is inconsistent, the trick to regain consistency is to eliminate \\(\\alpha_{i}\\). This section develops the consistency and asymptotic distribution of the within estimator, the default fixed-effect (FE) estimator. The within estimator transforms the data by subtracting all the observable variables by the corresponding group means. Averaging the \\(T\\) equations of the original regression for the same \\(i\\), we have \\[\\overline{y}_{i}=\\beta_{1}+\\overline{x}_{i}&#39;\\beta_{2}+\\bar{u}_{it}=\\beta_{1}+\\overline{x}_{i}&#39;\\beta_{2}+\\alpha_{i}+\\bar{\\epsilon}_{it}.\\label{eq:group_mean}\\] where \\(\\overline{y}_{i}=\\frac{1}{T}\\sum_{t=1}^{T}y_{it}\\). Subtracting the averaged equation from the original equation gives \\[\\tilde{y}_{it}=\\tilde{x}_{it}&#39;\\beta_{2}+\\tilde{\\epsilon}_{it}\\label{eq:FE_demean}\\] where \\(\\tilde{y}_{it}=y_{it}-\\overline{y}_{i}\\). We then run OLS with the demeaned data, and obtain the within estimator \\[\\widehat{\\beta}_{2}^{FE}=\\left(\\tilde{X}&#39;\\tilde{X}\\right)^{-1}\\tilde{X}&#39;\\tilde{y},\\] where \\(\\tilde{y}=\\left(y_{it}\\right)_{i,t}\\) stacks all the \\(nT\\) observations into a vector, and similarly defined is \\(\\tilde{X}\\) as an \\(nT\\times K\\) matrix, where \\(K\\) is the dimension of \\(\\beta_{2}\\). We know that OLS would be consistent if \\(E\\left[\\tilde{\\epsilon}_{it}|\\tilde{x}_{it}\\right]=0\\). Below we provide a sufficient condition, which is often called strict exogeneity. Assumption FE.1 \\(E\\left[\\epsilon_{it}|\\alpha_{i},\\mathbf{x}_{i}\\right]=0\\) where \\(\\mathbf{x}_{i}=\\left(x_{i1},\\ldots,x_{iT}\\right)\\). Its strictness is relative to the contemporary exogeneity \\(E\\left[\\epsilon_{it}|\\alpha_{i},x_{it}\\right]=0\\). FE.1 is more restrictive as it assumes that the error \\(\\epsilon_{it}\\) is mean independent of the past, present and future explanatory variables. When we talk about the consistency in panel data, typically we are considering \\(n\\to\\infty\\) while \\(T\\) stays fixed. This asymptotic framework is appropriate for panel datasets with many individuals but only a few time periods. Proposition If FE.1 is satisfied, then \\(\\widehat{\\beta}_{2}^{FE}\\) is consistent. The variance estimation for the FE estimator is a little bit tricky. We assume a homoskedasitcity condition to simplify the calculation. Violation of this assumption changes the form of the asymptotic variance, but does not jeopardize the asymptotic normality. Assumption FE.2 \\(\\mathrm{var}\\left(\\epsilon_{i}|\\alpha_{i},\\mathbf{x}_{i}\\right)=\\sigma_{\\epsilon}^{2}I_{T}\\) for all \\(i\\). Under FE.1 and FE.2, \\(\\widehat{\\sigma}_{\\epsilon}^{2}=\\frac{1}{n\\left(T-1\\right)}\\sum_{i=1}^{n}\\sum_{t=1}^{T}\\widehat{\\tilde{\\epsilon}}_{it}^{2}\\) is a consistent estimator of \\(\\sigma_{\\epsilon}^{2}\\), where \\(\\widehat{\\tilde{\\epsilon}}=\\tilde{y}_{it}-\\tilde{x}_{it}&#39;\\widehat{\\beta}_{2}^{FE}\\). Note that the denominator is \\(n\\left(T-1\\right)\\), not \\(nT\\). The necessity of adjusting the degree of freedom can be easily seen from the FWL theorem: the FE estimator for the slope coefficient is numerical the same as its counterpart in the full regression with a dummy variable for each cross sectional unit. If FE.1 and FE.2 are satisfied, then \\[\\left(\\widehat{\\sigma}_{\\epsilon}^{2}\\left(\\tilde{X}&#39;\\tilde{X}\\right)^{-1}\\right)^{-1/2}\\left(\\widehat{\\beta}_{2}^{FE}-\\beta_{2}^{0}\\right)\\stackrel{d}{\\to}N\\left(0,I_{K}\\right).\\] Let \\(M_{\\iota}=I_{T}-\\frac{1}{T}\\iota_{T}\\iota_{T}&#39;\\) be the within-group demeaner, and \\(M=I_{n}\\otimes M_{\\iota}\\) (“\\(\\otimes\\)” denotes the Kronecker product). The FE estimator can be explicitly written as \\[\\widehat{\\beta}_{2}^{FE}=\\left(\\tilde{X}&#39;\\tilde{X}\\right)^{-1}\\tilde{X}&#39;\\tilde{Y}=\\left(X&#39;MX\\right)^{-1}X&#39;MY.\\] So \\[\\sqrt{nT}\\left(\\widehat{\\beta}_{2}^{FE}-\\beta_{2}^{0}\\right)=\\left(\\frac{X&#39;MX}{nT}\\right)^{-1}\\frac{X&#39;M\\epsilon}{\\sqrt{nT}}=\\left(\\frac{\\tilde{X}&#39;\\tilde{X}}{nT}\\right)^{-1}\\frac{\\tilde{X}&#39;\\epsilon}{\\sqrt{nT}}\\] Since \\[\\begin{aligned} \\mathrm{var}\\left(\\frac{\\tilde{X}&#39;\\epsilon}{\\sqrt{nT}}|X\\right) &amp; =\\frac{1}{nT}E\\left(X&#39;M\\epsilon\\epsilon&#39;MX|X\\right)=\\frac{1}{nT}X&#39;ME\\left(\\epsilon\\epsilon&#39;|X\\right)MX=\\left(\\frac{\\tilde{X}&#39;\\tilde{X}}{nT}\\right)\\sigma^{2},\\end{aligned}\\] We apply a law of large numbers and conclude \\[\\left(\\tilde{X}&#39;\\tilde{X}\\right)^{1/2}\\left(\\widehat{\\beta}_{2}^{FE}-\\beta_{2}^{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\sigma_{\\epsilon}^{2}I_{K}\\right).\\] For simplicity, suppose we can direct observe \\(\\tilde{\\epsilon}_{it}\\). Then \\[\\begin{aligned} \\frac{1}{n\\left(T-1\\right)}E\\left[\\sum_{i=1}^{n}\\sum_{t=1}^{T}\\tilde{\\epsilon}_{it}^{2}\\right] &amp; =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{T-1}E\\left[\\epsilon_{i}&#39;M_{\\iota}\\epsilon_{i}\\right]\\\\ &amp; =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{T-1}\\mathrm{tr}\\left(E\\left[M_{\\iota}E\\left[\\epsilon_{i}\\epsilon_{i}&#39;|\\mathbf{x}_{i}\\right]\\right]\\right)\\\\ &amp; =\\frac{\\sigma_{\\epsilon}^{2}}{n}\\sum_{i=1}^{n}\\frac{1}{T-1}\\mathrm{tr}\\left(M_{\\iota}\\right)=\\sigma_{\\epsilon}^{2}.\\end{aligned}\\] Although in reality we only observe \\(\\widehat{\\tilde{\\epsilon}}_{it}\\), we can show that the estimation error between \\(\\widehat{\\tilde{\\epsilon}}_{it}\\) and \\(\\tilde{\\epsilon}_{it}\\) is negligible. Thus by the law of large numbers \\[\\widehat{\\sigma}_{\\epsilon}^{2}=\\frac{1}{n\\left(T-1\\right)}\\sum_{i=1}^{n}\\sum_{t=1}^{T}\\widehat{\\tilde{\\epsilon}}_{it}^{2}\\stackrel{d}{\\to}\\frac{1}{n\\left(T-1\\right)}E\\left[\\sum_{i=1}^{n}\\sum_{t=1}^{T}\\tilde{\\epsilon}_{it}^{2}\\right]=\\sigma_{\\epsilon}^{2}\\] is a consistent estimator of the variance. The stated conclusion follows. We implicitly assume regularity conditions that allow us to invoke a law of large numbers and a central limit theorem. We ignore those technical details here. It is important to notice that the within-group demean in FE eliminates all time-invariant explanatory variables, including the intercept. Therefore from FE we cannot obtain the coefficient estimates of these time-invariant variables. 9.2 Random Effect The random effect estimator pursues efficiency at a knife-edge special case \\(\\mathrm{cov}\\left(\\alpha_{i},x_{it}\\right)=0\\). As mentioned above, FE is consistent when \\(\\alpha_{i}\\) and \\(x_{it}\\) are uncorrelated. However, an inspection of the covariance matrix reveals that OLS is inefficient. The starting point is again the original model, while we assume Assumption RE.1 \\(E\\left[\\epsilon_{it}|\\alpha_{i},\\mathbf{x}_{i}\\right]=0\\) and \\(E\\left[\\alpha_{i}|\\mathbf{x}_{i}\\right]=0\\). RE.1 obviously implies \\(\\mathrm{cov}\\left(\\alpha_{i},x_{it}\\right)=0\\), so \\[S=\\mathrm{var}\\left(u_{i}|\\mathbf{x}_{i}\\right)=\\sigma_{\\alpha}^{2}\\mathbf{1}_{T}\\mathbf{1}_{T}&#39;+\\sigma_{\\epsilon}^{2}I_{T},\\ \\mbox{for all }i=1,\\ldots,n.\\] Because the covariance matrix is not a scalar multiplication of the identity matrix, OLS is inefficient. As mentioned before, FE estimation kills all time-invariant regressors. In contrast, RE allows time-invariant explanatory variables. The infeasible GLS estimator is \\[\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{infeasible}}^{RE}=\\left(\\sum_{i=1}^{n}\\mathbf{w}_{i}&#39;S^{-1}\\mathbf{w}_{i}\\right)^{-1}\\sum_{i=1}^{n}\\mathbf{w}_{i}&#39;S^{-1}\\mathbf{y}_{i}=\\left(W&#39;\\mathbf{S}^{-1}W\\right)^{-1}W&#39;\\mathbf{S}^{-1}y\\] where \\(\\mathbf{S}=I_{T}\\otimes S\\). In practice, \\(\\sigma_{\\alpha}^{2}\\) and \\(\\sigma_{\\epsilon}^{2}\\) in \\(S\\) are unknown, so we seek consistent estimators. Again, we impose a simplifying assumption parallel to FE.2. Assumption RE.2 \\(\\mathrm{var}\\left(\\epsilon_{i}|\\mathbf{x}_{i},\\alpha_{i}\\right)=\\sigma_{\\epsilon}^{2}I_{T}\\) and \\(\\mathrm{var}\\left(\\alpha_{i}|\\mathbf{x}_{i}\\right)=\\sigma_{\\alpha}^{2}.\\) Under this assumption, we can consistently estimate the variances from the residuals \\(\\widehat{u}_{it}=y_{it}-x_{it}&#39;\\widehat{\\boldsymbol{\\beta}}^{RE}\\). That is \\[\\begin{aligned}\\widehat{\\sigma}_{u}^{2} &amp; =\\frac{1}{nT}\\sum_{i=1}^{n}\\sum_{t=1}^{T}\\widehat{u}_{it}^{2}\\\\ \\widehat{\\sigma}_{\\alpha}^{2} &amp; =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{T\\left(T-1\\right)}\\sum_{t=1}^{T}\\sum_{r=1}^{T}\\sum_{r\\neq t}\\widehat{u}_{it}\\widehat{u}_{ir}. \\end{aligned}\\] Given the estimated variance and covariance, we can construct \\(\\widehat{\\mathbf{S}}=\\left(\\widehat{\\sigma}_{u}^{2}-\\widehat{\\sigma}_{\\epsilon}^{2}\\right)\\cdot I_{T}+\\widehat{\\sigma}_{\\epsilon}^{2}\\cdot\\boldsymbol{1}_{T}\\boldsymbol{1}_{T}&#39;\\) and then follows the feasible GLS (FGLS) \\[\\widehat{\\boldsymbol{\\beta}}^{RE}=\\left(W&#39;\\mathbf{\\widehat{S}}^{-1}W\\right)^{-1}W&#39;\\widehat{\\mathbf{S}}^{-1}y\\] Show that if RE.1 and RE.2 are satisfied, then \\[\\left(\\widehat{\\sigma}_{u}^{2}\\left(W&#39;\\widehat{\\mathbf{S}}^{-1}W\\right)^{-1}\\right)^{-1/2}\\left(\\widehat{\\boldsymbol{\\beta}}^{RE}-\\boldsymbol{\\beta}_{0}\\right)\\stackrel{d}{\\to}N\\left(0,I_{K+1}\\right).\\] In econometrics practice, the FE estimator is more popular than the RE estimator as the former is consistent in more general conditions. 9.3 Summary The formula of the FE estimator or the RE estimators is not important because the estimation and inference are automatically handled by econometric packages. What is important is the conceptual difference of FE and RE on their treatment of the unobservable individual heterogeneity. Panel data is the first generation of economic “big data”, as the number of observations of a cross section is multiplied by the number of time periods. It reflected econometrician’s pursuit of controlling heterogeneity, so that the OLS estimate is more credible for causal interpretation. Further reading: Hsiao (2014) is a comprehensive monograph on the topic of panel data. Su, Shi, and Phillips (2016) extends fixed effect models to incorporate group heterogeneity. Zhentao Shi. Nov 8, 2020. References "],
["endogeneity.html", "10 Endogeneity 10.1 Identification 10.2 Instruments 10.3 Sources of Endogeneity 10.4 Summary", " 10 Endogeneity In microeconomic analysis, exogenous variables are the factors determined outside of the economic system under consideration, and endogenous variables are those decided within the economic system. A microeconomic exercise that we encountered so many times goes as follows. If a person has a utility function \\(u\\left(q_{1},q_{2}\\right)\\) where \\(q_{1}\\) and \\(q_{2}\\) are the quantities of two goods. He faces a budget \\(p_{1}q_{1}+p_{2}q_{2}\\leq C\\), where \\(p_{1}\\) and \\(p_{2}\\) are the prices of the two goods, respectively. What is the optimal quantities \\(q_{1}^{*}\\) and \\(q_{2}^{*}\\) he will purchase? In this question the utility function \\(u\\left(\\cdot,\\cdot\\right)\\), the prices \\(p_{1}\\) and \\(p_{2}\\), and the budget \\(C\\) are exogenous. The optimal purchase \\(q_{1}^{*}\\) and \\(q_{2}^{*}\\) are endogenous. The terms “endogenous” and “exogenous” in microeconomics will be carried over into multiple-equation econometric models. While in a single-equation regression model \\[y_{i}=x_{i}&#39;\\beta+e_{i}\\label{eq:generative}\\] is only part of the equation system. To make it simple, in the single-equation model we say an \\(x_{ik}\\) is endogenous, or is an endogenous variable, if \\(\\mathrm{cov}\\left(x_{ik},e_{i}\\right)\\neq0\\); otherwise \\(x_{ik}\\) is an exogenous variable. Empirical works using linear regressions are routinely challenged by questions about endogeneity. Such questions plague economic seminars and referee reports. To defend empirical strategies in quantitative economic studies, it is important to understand the sources of potential endogeneity and thoroughly discuss attempts for resolving endogeneity. 10.1 Identification Endogeneity usually implies difficulty in identifying the parameter of interest with only \\(\\left(y_{i},x_{i}\\right)\\). Identification is critical for the interpretation of empirical economic research. We say a parameter is identified if the mapping between the parameter in the model and the distribution of the observed variable is one-to-one; otherwise we say the parameter is under-identified. This is an abstract definition, and let us discuss it in the family linear regression context. The linear projection model implies the moment equation \\[\\mathbb{E}\\left[x_{i}x_{i}&#39;\\right]\\beta=\\mathbb{E}\\left[x_{i}y_{i}\\right]. (citation)\\] If \\(E\\left[x_{i}x_{i}&#39;\\right]\\) is of full rank, then \\(\\beta=\\left(\\mathbb{E}\\left[x_{i}x_{i}&#39;\\right]\\right)^{-1}\\mathbb{E}\\left[x_{i}y_{i}\\right]\\) is a function of the quantities of the population moment and it is identified. On the contrary, if some \\(x_{k}\\)’s are perfect collinear so that \\(\\mathbb{E}\\left[x_{i}x_{i}&#39;\\right]\\) is rank deficient, there are multiple \\(\\beta\\) that satisfies the \\(k\\)-equation system (\\[eq:k-equation-FOC\\]). Identification fails. Suppose \\(x_{i}\\) is a scalar random variable, \\[\\begin{pmatrix}x_{i}\\\\ e_{i} \\end{pmatrix}\\sim N\\left(\\begin{pmatrix}0\\\\ 0 \\end{pmatrix},\\begin{pmatrix}1 &amp; \\sigma_{xe}\\\\ \\sigma_{xe} &amp; 1 \\end{pmatrix}\\right)\\] follows a joint normal distribution, and the dependent variable \\(y_{i}\\) is generated from (\\[eq:generative\\]). The joint normal assumption implies that the conditional mean \\[\\mathbb{E}\\left[y_{i}|x_{i}\\right]=\\beta x_{i}+\\mathbb{E}\\left[e_{i}|x_{i}\\right]=\\left(\\beta+\\sigma_{xe}\\right)x_{i}\\] coincides with the linear projection model, and \\(\\beta+\\sigma_{xe}\\) is the linear projection coefficient. From the observable random variable \\(\\left(y_{i},x_{i}\\right)\\), we can only learn \\(\\beta+\\sigma_{xe}\\). As we cannot learn \\(\\sigma_{xe}\\) from the data due to the unobservable \\(e_{i}\\), there is no way to recover \\(\\beta\\). This is exactly the omitted variable bias that we have discussed earlier in this course. The gap lies between the available data \\(\\left(y_{i},x_{i}\\right)\\) and the identification of the model. In the special case that we assume \\(\\sigma_{xe}=0\\), the endogeneity vanishes and \\(\\beta\\) is identified. The linear projection model is so far the most general model in this course that justifies OLS. OLS is consistent for the linear projection coefficient. By the definition of the linear projection model, \\(\\mathbb{E}\\left[x_{i}e_{i}\\right]=0\\) so there is no room for endogeneity in the linear projection model. In other words, if we talk about endogeneity, we must not be working with the linear projection model, and the coefficients we pursue the structural parameter, rather than the linear projection coefficients. In econometrics we are often interested in a model with economic interpretation. The common practice in empirical research assumes that the observed data are generated from a parsimonious model, and the next step is to estimate the unknown parameters in the model. Since it is often possible to name some factors not included in the regressors but they are correlated with the included regressors and in the mean time also affects \\(y_{i}\\), endogeneity becomes a fundamental problem. To resolve endogeneity, we seek extra variables or data structure that may guarantee the identification of the model. The most often used methods are (i) fixed effect model (ii) instrumental variables: The fixed effect model requires that multiple observations, often across time, are collected for each individual \\(i\\). Moreover, the source of endogeneity is time invariant and enters the model additively in the form \\[y_{it}=x_{it}&#39;\\beta+u_{it},\\] where \\(u_{it}=\\alpha_{i}+\\epsilon_{it}\\) is the composite error. The panel data approach extends \\(\\left(y_{i},x_{i}\\right)\\) to \\(\\left(y_{it},x_{it}\\right)_{i=1}^{T}\\) if data are available along the time dimension. The instrumental variable approach extends \\(\\left(y_{i},x_{i}\\right)\\) to \\(\\left(y_{i},x_{i},z_{i}\\right)\\), where the extra random variable \\(z_{i}\\) is called the instrument variable. It is assumed that \\(z_{i}\\) is orthogonal to the error \\(e_{i}\\) . Therefore, along with the model it adds an extra variable \\(z_{i}\\). Either the panel data approach or the instrumental variable approach entails extra information beyond \\(\\left(y_{i},x_{i}\\right)\\). Without such extra data, there is no way to resolve the identification failure. Just as the linear project model is available for any joint distribution of \\(\\left(y_{i},x_{i}\\right)\\) with existence of suitable moments, from a pure statistical point of view a linear IV model is an artifact depends only on the choice of \\(\\left(y_{i},x_{i},z_{i}\\right)\\) without referencing to any economics. In essence, the linear IV model seeks a linear combination \\(y_{i}-\\beta x_{i}\\) that is orthogonal to the linear space spanned by \\(z_{i}\\). 10.2 Instruments There are two requirements for valid IVs: orthogonality and relevance. Orthogonality entails that the model is correctly specified. If relevance is violated, meaning that the IVs are not correlated with the endogenous variable, then multiple parameters can generate the observable data. Identification, as in the standard definition in econometrics, breaks down. A structural equation is a model of economic interest. Consider the following linear structural model \\[y_{i}=x_{1i}&#39;\\beta_{1}+z_{1i}&#39;\\beta_{2}+\\epsilon_{i},\\label{eq:basic_1}\\] where \\(x_{1i}\\) is a \\(k_{1}\\)-dimensional endogenous explanatory variables, \\(z_{1i}\\) is a \\(k_{2}\\)-dimensional exogenous explanatory variables with the intercept included. In addition, we have \\(z_{2i}\\), a \\(k_{3}\\)-dimensional excluded exogenous variables. Let \\(K=k_{1}+k_{2}\\) and \\(L=k_{2}+k_{3}\\). Denote \\(x_{i}=\\left(x_{1i}&#39;,z_{1i}&#39;\\right)&#39;\\) as a \\(K\\)-dimensional explanatory variable, and \\(z_{i}=\\left(z_{1i}&#39;,z_{2i}&#39;\\right)\\) as an \\(L\\)-dimensional exogenous vector. We call the exogenous variable instrument variables, or simply instruments. Let \\(\\beta=\\left(\\beta_{1}&#39;,\\beta_{2}&#39;\\right)&#39;\\) be a \\(K\\)-dimensional parameter of interest. From now on, we rewrite (\\[eq:basic\\_1\\]) as \\[y_{i}=x_{i}&#39;\\beta+\\epsilon_{i},\\label{eq:basic_2}\\] and we have a vector of instruments \\(z_{i}\\). Before estimating any structural econometric model, we must check identification. In the context of (\\[eq:basic\\_2\\]), identification requires that the true value \\(\\beta_{0}\\) is the only value on the parameters space that satisfies the moment condition \\[\\mathbb{E}\\left[z_{i}\\left(y_{i}-x_{i}&#39;\\beta\\right)\\right]=0_{L}.\\label{eq:moment}\\] The rank condition is sufficient and necessary for identification. \\(\\mathrm{rank}\\left(\\mathbb{E}\\left[z_{i}x_{i}&#39;\\right]\\right)=K\\). Note that \\(\\mathbb{E}\\left[x_{i}&#39;z_{i}\\right]\\) is a \\(K\\times L\\) matrix. The rank condition implies the order condition \\(L\\geq K\\), which says that the number of excluded instruments must be no fewer than the number of endogenous variables. The parameter in (\\[eq:moment\\]) is identified if and only if the rank condition holds. (The “if” direction). For any \\(\\tilde{\\beta}\\) such that \\(\\tilde{\\beta}\\neq\\beta_{0}\\), \\[\\begin{aligned} \\mathbb{E}\\left[z_{i}\\left(y_{i}-x_{i}&#39;\\tilde{\\beta}\\right)\\right] &amp; =\\mathbb{E}\\left[z_{i}\\left(y_{i}-x_{i}&#39;\\beta_{0}\\right)\\right]+\\mathbb{E}\\left[z_{i}x_{i}&#39;\\right]\\left(\\beta_{0}-\\tilde{\\beta}\\right)\\\\ &amp; =0_{L}+\\mathbb{E}\\left[z_{i}x_{i}&#39;\\right]\\left(\\beta_{0}-\\tilde{\\beta}\\right).\\end{aligned}\\] Because \\(\\mathrm{rank}\\left(\\mathbb{E}\\left[z_{i}x_{i}&#39;\\right]\\right)=K\\), we would have \\(\\mathbb{E}\\left[z_{i}x_{i}&#39;\\right]\\left(\\beta_{0}-\\tilde{\\beta}\\right)=0_{L}\\) if and only if \\(\\beta_{0}-\\tilde{\\beta}=0_{K}\\), which violates \\(\\tilde{\\beta}\\neq\\beta_{0}\\). Therefore \\(\\beta_{0}\\) is the unique value that satisfies (\\[eq:moment\\]). (The “only if” direction is left as an exercise. Hint: By contrapositiveness, if the rank condition fails, then the model is not identified. We can easily prove the claim by making an example.) 10.3 Sources of Endogeneity As econometricians mostly work with non-experimental data, we cannot overstate the importance of the endogeneity problem. We go over a few examples. We know that the first-difference (FD) estimator is consistent for (static) panel data model. Nevertheless, the FD estimator encounters difficulty in a dynamic panel model \\[y_{it}=\\beta_{1}+\\beta_{2}y_{i,t-1}+\\beta_{3}x_{it}+\\alpha_{i}+\\epsilon_{it},\\label{eq:dymPanel}\\] even if we assume \\[\\mathbb{E}\\left[\\epsilon_{is}|\\alpha_{i},x_{i1},\\ldots,x_{iT},y_{i,t-1},y_{i,t-2},\\ldots,y_{i0}\\right]=0,\\ \\ \\forall s\\geq t\\label{eq:dyn_mean_0}\\] When taking difference of the above equation (\\[eq:dymPanel\\]) for periods \\(t\\) and \\(t-1\\), we have \\[\\left(y_{it}-y_{i,t-1}\\right)=\\beta_{2}\\left(y_{it-1}-y_{i,t-2}\\right)+\\beta_{3}\\left(x_{it}-x_{i,t-1}\\right)+\\left(\\epsilon_{it}-\\epsilon_{i,t-1}\\right).\\label{eq:dyn_mean_1}\\] Under (\\[eq:dyn\\_mean\\_0\\]), \\(\\mathbb{E}\\left[\\left(x_{it}-x_{i,t-1}\\right)\\left(\\epsilon_{it}-\\epsilon_{i,t-1}\\right)\\right]=0\\), but \\[\\mathbb{E}\\left[\\left(y_{i,t-1}-y_{i,t-2}\\right)\\left(\\epsilon_{it}-\\epsilon_{i,t-1}\\right)\\right]=-\\mathbb{E}\\left[y_{i,t-1}\\epsilon_{i,t-1}\\right]=-\\mathbb{E}\\left[\\epsilon_{i,t-1}^{2}\\right]\\neq0.\\] Therefore the coefficients \\(\\beta_{2}\\) and \\(\\beta_{3}\\) cannot be identified from the linear regression model (\\[eq:dyn\\_mean\\_1\\]). Instruments for the above example is easy to find. Notice that the linear relationship (\\[eq:dymPanel\\]) implies \\[\\begin{aligned} &amp; &amp; \\mathbb{E}\\left[\\epsilon_{i,t}-\\epsilon_{i,t-1}|\\alpha_{i},x_{i1},\\ldots,x_{iT},\\epsilon_{i,t-2},\\epsilon_{i,t-3},\\ldots,\\epsilon_{i1},y_{i0}\\right]\\\\ &amp; = &amp; \\mathbb{E}\\left[\\epsilon_{i,t}-\\epsilon_{i,t-1}|\\alpha_{i},x_{i1},\\ldots,x_{iT},y_{i,t-2},y_{i,t-3},\\ldots,y_{i0}\\right]=0\\end{aligned}\\] according to the assumption (\\[eq:dyn\\_mean\\_0\\]). The above relationship gives orthogonal condition in the form \\[\\mathbb{E}\\left[\\left(\\epsilon_{i,t}-\\epsilon_{i,t-1}\\right)f\\left(\\epsilon_{i,t-2},\\epsilon_{i,t-3},\\ldots,\\epsilon_{i1}\\right)\\right]=0.\\] In other words, any function of \\(y_{i,t-2},y_{i,t-3},\\ldots,y_{i1}\\) is orthogonal to the error term \\(\\left(\\epsilon{}_{i,t-1}-\\epsilon_{i,t-2}\\right)\\). Here the excluded IVs are naturally generated from the model itself. Another classical source of endogeneity is the measurement error. Endogeneity also emerges when an explanatory variables is not directly observable but is replaced by a measurement with error. Suppose the true linear model is \\[y_{i}=\\beta_{1}+\\beta_{2}x_{i}^{*}+u_{i},\\label{eq:measurement_error}\\] with \\(\\mathbb{E}\\left[u_{i}|x_{i}^{*}\\right]=0\\). We cannot observe \\(x_{i}^{*}\\) but we observe \\(x_{i}\\), a measurement of \\(x_{i}^{*}\\), and they are linked by \\[x_{i}=x_{i}^{*}+v_{i}\\] with \\(\\mathbb{E}\\left[v_{i}|x_{i}^{*},u_{i}\\right]=0\\). Such a formulation of the measurement error is called the classical measurement error. Substitute out the unobservable \\(x_{i}^{*}\\) in (\\[eq:measurement\\_error\\]), \\[y_{i}=\\beta_{1}+\\beta_{2}\\left(x_{i}-v_{i}\\right)+u_{i}=\\beta_{1}+\\beta_{2}x_{i}+e_{i}\\label{eq:measurement_error2}\\] where \\(e_{i}=u_{i}-\\beta_{2}v_{i}\\). The correlation \\[\\mathbb{E}\\left[x_{i}e_{i}\\right]=\\mathbb{E}\\left[\\left(x_{i}^{*}+v_{i}\\right)\\left(u_{i}-\\beta_{2}v_{i}\\right)\\right]=-\\beta_{2}\\mathbb{E}\\left[v_{i}^{2}\\right]\\neq0.\\] OLS (\\[eq:measurement\\_error2\\]) would not deliver a consistent estimator. Alternatively, we can look at the above problem of classical measurement error from the expression of the linear projection coefficient. We know that in (\\[eq:measurement\\_error\\]) \\(\\beta_{2}^{\\mathrm{infeasible}}=\\mathrm{cov}\\left[x_{i}^{*},y_{i}\\right]/\\mathrm{var}\\left[x_{i}^{*}\\right].\\) In contrast, when we regression \\(y_{i}\\) on the observable \\(x_{i}\\) the corresponding linear projection coefficient is \\[\\beta_{2}^{\\mathrm{feasible}}=\\frac{\\mathrm{cov}\\left[x_{i},y_{i}\\right]}{\\mathrm{var}\\left[x_{i}\\right]}=\\frac{\\mathrm{cov}\\left[x_{i}^{*}+v_{i},y_{i}\\right]}{\\mathrm{var}\\left[x_{i}^{*}+v_{i}\\right]}=\\frac{\\mathrm{cov}\\left[x_{i}^{*},y_{i}\\right]}{\\mathrm{var}\\left[x_{i}^{*}\\right]+\\mathrm{var}\\left[v_{i}\\right]}.\\] It is clear that \\(|\\beta_{2}^{\\mathrm{feasible}}|\\leq|\\beta_{2}^{\\mathrm{infeasible}}|\\) and the equality holds only if \\(\\mathrm{var}\\left[v_{i}\\right]=0\\) (no measurement error). This is called the attenuation bias due to the measurement error. Next, we give two examples of equation systems, one from microeconomics and the other from macroeconomics. Let \\(p_{i}\\) and \\(q_{i}\\) be a good’s log-price and log-quantity on the \\(i\\)-th market, and they are iid across markets. We are interested in the demand curve \\[p_{i}=\\alpha_{d}-\\beta_{d}q_{i}+e_{di}\\label{eq:demand}\\] for some \\(\\beta_{d}\\geq0\\) and the supply curve \\[p_{i}=\\alpha_{s}+\\beta_{s}q_{i}+e_{si}\\label{eq:supply}\\] for some \\(\\beta_{s}\\geq0\\). We use a simple linear specification so that the coefficient \\(\\beta_{d}\\) can be interpreted as demand elasticity and \\(\\beta_{s}\\) as supply elasticity. Undergraduate microeconomics teaches the deterministic form but we add an error term to cope with the data. Can we learn the elasticities by regression \\(p_{i}\\) on \\(q_{i}\\)? The two equations can be written in a matrix form \\[\\begin{pmatrix}1 &amp; \\beta_{d}\\\\ 1 &amp; -\\beta_{s} \\end{pmatrix}\\begin{pmatrix}p_{i}\\\\ q_{i} \\end{pmatrix}=\\begin{pmatrix}\\alpha_{d}\\\\ \\alpha_{s} \\end{pmatrix}+\\begin{pmatrix}e_{di}\\\\ e_{si} \\end{pmatrix}.\\label{eq:structural}\\] Microeconomic terminology calls \\(\\left(p_{i},q_{i}\\right)\\) endogenous variables and \\(\\left(e_{di},e_{si}\\right)\\) exogenous variables. (\\[eq:structural\\]) is a structural equation because it is motivated from economic theory so that the coefficients bear economic meaning. If we rule out the trivial case \\(\\beta_{d}=\\beta_{s}=0\\), we can solve \\[\\begin{aligned} \\begin{pmatrix}p_{i}\\\\ q_{i} \\end{pmatrix} &amp; =\\begin{pmatrix}1 &amp; \\beta_{d}\\\\ 1 &amp; -\\beta_{s} \\end{pmatrix}^{-1}\\left[\\begin{pmatrix}\\alpha_{d}\\\\ \\alpha_{s} \\end{pmatrix}+\\begin{pmatrix}e_{di}\\\\ e_{si} \\end{pmatrix}\\right]\\nonumber \\\\ &amp; =\\frac{1}{\\beta_{s}+\\beta_{d}}\\begin{pmatrix}\\beta_{s} &amp; \\beta_{d}\\\\ 1 &amp; -1 \\end{pmatrix}\\left[\\begin{pmatrix}\\alpha_{d}\\\\ \\alpha_{s} \\end{pmatrix}+\\begin{pmatrix}e_{di}\\\\ e_{si} \\end{pmatrix}\\right].\\label{eq:reduced}\\end{aligned}\\] This equation (\\[eq:reduced\\]) is called the reduced form—the endogenous variables are expressed as explicit functions of the parameters and the exogenous variables. In particular, \\[q_{i}=\\left(\\alpha_{d}+e_{di}-\\alpha_{s}-e_{si}\\right)/\\left(\\beta_{s}+\\beta_{d}\\right)\\] so that the log-price is correlated with both \\(e_{si}\\) and \\(e_{di}\\). As \\(q_{i}\\) is endogenous (in the econometric sense) in either (\\[eq:demand\\]) or (\\[eq:supply\\]), neither the demand elasticity nor the supply elasticity is identified with \\(\\left(p_{i},q_{i}\\right)\\). Indeed, as \\[p_{i}=\\left(\\beta_{s}\\alpha_{d}+\\beta_{d}\\alpha_{s}+\\beta_{s}e_{di}+\\beta_{d}e_{si}\\right)/\\left(\\beta_{s}+\\beta_{d}\\right)\\] from (\\[eq:reduced\\]), the linear projection coefficient of \\(p_{i}\\) on \\(q_{i}\\) is \\[\\frac{\\mathrm{cov}\\left[p_{i},q_{i}\\right]}{\\mathrm{var}\\left[q_{i}\\right]}=\\frac{\\beta_{s}\\sigma_{d}^{2}-\\beta_{d}\\sigma_{s}^{2}+\\left(\\beta_{d}-\\beta_{s}\\right)\\sigma_{sd}}{\\beta_{d}^{2}\\sigma_{d}^{2}+\\beta_{d}\\sigma_{s}^{2}+2\\beta_{d}\\beta_{s}\\sigma_{sd}},\\] where \\(\\sigma_{d}^{2}=\\mathrm{var}\\left[e_{di}\\right]\\), \\(\\sigma_{s}^{2}=\\mathrm{var}\\left[e_{si}\\right]\\) and \\(\\sigma_{sd}=\\mathrm{cov}\\left[e_{di},e_{si}\\right]\\). This is a classical example of the demand-supply system. The structural parameter cannot be directly identified because the observed \\(\\left(p_{i},q_{i}\\right)\\) is the outcome of an equilibrium—the crossing of the demand curve and the supply curve. To identify the demand curve, we will need an instrument that shifts the supply curve only; and vice versa. This is a model borrowed from Hayashi (2000, p.193) but originated from Haavelmo (1943). An econometrician is interested in learning \\(\\beta_{2}\\), the marginal propensity of consumption, in the Keynesian-type equation \\[C_{i}=\\beta_{1}+\\beta_{2}Y_{i}+u_{i}\\label{eq:keynes}\\] where \\(C_{i}\\) is household consumption, \\(Y_{i}\\) is the GNP, and \\(u_{i}\\) is the unobservable error. However, \\(Y_{i}\\) and \\(C_{i}\\) are connected by an accounting equality (with no error) \\[Y_{i}=C_{i}+I_{i},\\] where \\(I_{i}\\) is investment. We assume \\(\\mathbb{E}\\left[u_{i}|I_{i}\\right]=0\\) as investment is determined in advance. In this example, \\(\\left(Y_{i}C_{i}\\right)\\) are endogenous and \\(\\left(I_{i},u_{i}\\right)\\) are exogenous. Put the two equations together as the structural form \\[\\begin{pmatrix}1 &amp; -\\beta_{2}\\\\ -1 &amp; 1 \\end{pmatrix}\\begin{pmatrix}C_{i}\\\\ Y_{i} \\end{pmatrix}=\\begin{pmatrix}\\beta_{1}\\\\ 0 \\end{pmatrix}+\\begin{pmatrix}u_{i}\\\\ I_{i} \\end{pmatrix}.\\] The corresponding reduced form is \\[\\begin{aligned} \\begin{pmatrix}C_{i}\\\\ Y_{i} \\end{pmatrix} &amp; =\\begin{pmatrix}1 &amp; -\\beta_{2}\\\\ -1 &amp; 1 \\end{pmatrix}^{-1}\\left[\\begin{pmatrix}\\beta_{1}\\\\ 0 \\end{pmatrix}+\\begin{pmatrix}u_{i}\\\\ I_{i} \\end{pmatrix}\\right]\\\\ &amp; =\\frac{1}{1-\\beta_{2}}\\begin{pmatrix}1 &amp; \\beta_{2}\\\\ 1 &amp; 1 \\end{pmatrix}\\left[\\begin{pmatrix}\\beta_{1}\\\\ 0 \\end{pmatrix}+\\begin{pmatrix}u_{i}\\\\ I_{i} \\end{pmatrix}\\right]\\\\ &amp; =\\frac{1}{1-\\beta_{2}}\\begin{pmatrix}\\beta_{1}+u_{i}+\\beta_{2}I_{i}\\\\ \\beta_{1}+u_{i}+I_{i} \\end{pmatrix}.\\end{aligned}\\] OLS (\\[eq:keynes\\]) will be inconsistent because in the reduced-form \\(Y_{i}=\\frac{1}{1-\\beta_{2}}\\left(\\beta_{1}+u_{i}+I_{i}\\right)\\) implies \\(\\mathbb{E}\\left[Y_{i}u_{i}\\right]=\\mathbb{E}\\left[u_{i}^{2}\\right]/\\left(1-\\beta_{2}\\right)\\neq0\\). 10.4 Summary Even though we often deal with a single equation model with potential endogenous variables, the underlying structural system may involve multiple equations. The simultaneous equation model is a classical econometric modeling approach, and it is still actively applied in structural economic studies. When our economic model is “structural”, we keep in mind a causal mechanism. Instead of identifying the causal effect by control group and treatment group as in Chapter 2, here we look at causality from the economic structural perspective. Historical notes: Instruments originally appeared in Philip Wright (1928) for identifying the coefficient of an endogenous variables. It is believed to be a collaborative idea with Philip’s son Sewall Wright. The demand and supply analysis is attributed to Working (1927), and the measurement error study is dated back to Fricsh (1934). Further reading: Causality is the holy grail of econometrics. Pearl and Mackenzie (2018) is a popular book with philosophical depth. It is a delight to read. (Chen, Hong, and Nekipelov 2011) is a survey for modern nonlinear measurement error models. References "],
["generalized-method-of-moments.html", "11 Generalized Method of Moments 11.1 Instrumental Regression 11.2 GMM Estimator 11.3 GMM in Nonlinear Model 11.4 Summary", " 11 Generalized Method of Moments Generalized method of moments (GMM) (Hansen 1982) is an estimation principle that extends method of moments. It seeks the parameter value that minimizes a quadratic form of the moments. It is particularly useful in estimating structural economic models in which moment conditions can be derived from underlying economic theory. GMM emerges as one of the most popular estimators in modern econometrics. It includes conventional methods like the two-stage least squares (2SLS) and the three-stage least square as special cases. 11.1 Instrumental Regression We first discuss estimation in a linear single structural equation \\[y_{i}=x_{i}&#39;\\beta+\\epsilon_{i}\\] with \\(K\\) regressors. Identification is a prerequisite for structural estimation. From now on we always assume that the model is identified: there is an \\(L\\times1\\) vector of instruments \\(z_{i}\\) such that \\(\\mathbb{E}\\left[z_{i}\\epsilon_{i}\\right]=0_{L}\\) and \\(\\Sigma:=\\mathbb{E}\\left[z_{i}x_{i}&#39;\\right]\\) is of full column rank. Denote \\(\\beta_{0}\\) as the root of the equation \\(E\\left[z_{i}\\left(y_{i}-x_{i}&#39;\\beta\\right)\\right]=0_{L}\\), which is uniquely identified. 11.1.1 Just-identification When \\(L=K\\), the instrumental regression model is just-identified, or exactly identified. The orthogonality condition implies \\[\\Sigma\\beta_{0}=\\mathbb{E}\\left[z_{i}y_{i}\\right],\\] and we can solve express \\(\\beta_{0}\\) as \\[\\beta_{0}=\\Sigma^{-1}\\mathbb{E}\\left[z_{i}y_{i}\\right]\\label{eq:just_beta}\\] in closed form. The closed-form solution naturally motivates an estimator in which we replace the population methods by the sample moments and this is a method-of-moments estimator. Nevertheless, we postpone the discussion of this estimator to the next section. 11.1.2 Over-identification When \\(L&gt;K\\), the model is over-identified. The orthogonality condition still implies \\[\\Sigma\\beta_{0}=\\mathbb{E}\\left[z_{i}y_{i}\\right],\\label{eq:moment2}\\] but \\(\\Sigma\\) is not a square matrix so we cannot write \\(\\beta_{0}\\) as that in (\\[eq:just\\_beta\\]). In order to express \\(\\beta_{0}\\) explicitly, we define a criterion function \\[Q\\left(\\beta\\right)=\\mathbb{E}\\left[z_{i}\\left(y_{i}-x_{i}\\beta\\right)\\right]&#39;W\\mathbb{E}\\left[z_{i}\\left(y_{i}-x_{i}\\beta\\right)\\right],\\] where \\(W\\) is an \\(L\\times L\\) positive-definite non-random symmetric matrix. (The choice of \\(W\\) will be discussed soon.) Because of the quadratic form, \\(Q\\left(\\beta\\right)\\geq0\\) for all \\(\\beta\\). Identification indicates that \\(Q\\left(\\beta\\right)=0\\) if and only if \\(\\beta=\\beta_{0}\\). Therefore we conclude \\[\\beta_{0}=\\arg\\min_{\\beta}Q\\left(\\beta\\right)\\] is the unique minimizer. Since \\(Q\\left(\\beta\\right)\\) is a smooth function of \\(\\beta\\), the minimizer \\(\\beta_{0}\\) can be characterized by the first-order condition \\[0_{K}=\\frac{\\partial}{\\partial\\beta}Q\\left(\\beta_{0}\\right)=-2\\Sigma&#39;W\\mathbb{E}\\left[z_{i}\\left(y_{i}-x_{i}\\beta_{0}\\right)\\right]\\] Rearranging the above equation, we have \\[\\Sigma&#39;W\\Sigma\\beta_{0}=\\Sigma&#39;W\\mathbb{E}\\left[z_{i}y_{i}\\right].\\] Under the rank condition \\(\\Sigma&#39;W\\Sigma\\) is invertible so that we can solve \\[\\beta_{0}=\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\Sigma&#39;W\\mathbb{E}\\left[z_{i}y_{i}\\right].\\label{eq:over_beta}\\] Because we have more moments (\\(L\\)) than the number of unknown parameters (\\(K\\)), we call it the generalized method of moments. The above equation can be derived by pre-multiplying \\(\\Sigma&#39;W\\) on the both sides of (\\[eq:moment2\\]) without referring to the minimization problem. Although we separate the discussion of the just-identified case and the over-identified case, the latter (\\[eq:over\\_beta\\]) actually takes (\\[eq:just\\_beta\\]) as a special case. In this sense, GMM is genuine generalization of the method of moments. to see this point, notice that when \\(L=K\\), given any \\(W\\) we have \\[\\begin{aligned} \\beta_{0} &amp; =\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\Sigma&#39;W\\mathbb{E}\\left[z_{i}y_{i}\\right]=\\Sigma^{-1}W^{-1}(\\Sigma&#39;)^{-1}\\Sigma&#39;W\\mathbb{E}\\left[z_{i}y_{i}\\right]\\\\ &amp; =\\Sigma^{-1}W^{-1}W\\mathbb{E}\\left[z_{i}y_{i}\\right]=\\Sigma^{-1}\\mathbb{E}\\left[z_{i}y_{i}\\right],\\end{aligned}\\] as \\(\\Sigma\\) is a square matrix. That is to say, in the just-identified case \\(W\\) plays no role because any choices of \\(W\\) lead to the same explicit solution of \\(\\beta_{0}\\). 11.2 GMM Estimator In practice, we use the sample moments to replace the corresponding population moments. The GMM estimator mimics its population formula. \\[\\begin{aligned} \\widehat{\\beta} &amp; = &amp; \\left(\\frac{1}{n}\\sum x_{i}z_{i}&#39;W\\frac{1}{n}\\sum z_{i}x_{i}&#39;\\right)^{-1}\\frac{1}{n}\\sum x_{i}z_{i}&#39;W\\frac{1}{n}\\sum z_{i}y_{i}\\\\ &amp; = &amp; \\left(\\frac{X&#39;Z}{n}W\\frac{Z&#39;X}{n}\\right)^{-1}\\frac{X&#39;Z}{n}W\\frac{Z&#39;y}{n}\\\\ &amp; = &amp; \\left(X&#39;ZWZ&#39;X\\right)^{-1}X&#39;ZWZ&#39;y.\\end{aligned}\\] Under just-identification, this expression includes the 2SLS estimator \\[\\hat{\\beta}=\\left(\\frac{Z&#39;X}{n}\\right)^{-1}\\frac{Z&#39;y}{n}=\\left(Z&#39;X\\right)^{-1}Z&#39;y\\] as a special case. The same GMM estimator \\(\\hat{\\beta}\\) can be obtained by minimizing \\[Q_{n}\\left(\\beta\\right)=\\left[\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\left(y_{i}-x_{i}\\beta\\right)\\right]&#39;W\\left[\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\left(y_{i}-x_{i}\\beta\\right)\\right]=\\frac{\\left(y-X\\beta\\right)&#39;Z}{n}W\\frac{Z&#39;\\left(y-X\\beta\\right)}{n},\\] or more concisely \\(\\hat{\\beta}=\\arg\\min_{\\beta}\\left(y-X\\beta\\right)&#39;ZWZ&#39;\\left(y-X\\beta\\right).\\) Now we check the asymptotic properties of \\(\\widehat{\\beta}\\). A few assumptions are in order. \\(Z&#39;X/n\\stackrel{\\mathrm{p}}{\\to}\\Sigma\\) and \\(Z&#39;\\epsilon/n\\stackrel{\\mathrm{p}}{\\to}0_{L}\\). A.1 assumes that we can apply a law of large numbers, so that that the sample moments \\(Z&#39;X/n\\) and \\(Z&#39;\\epsilon/n\\) converge in probability to their population counterparts. Under Assumption A.1, \\(\\widehat{\\beta}\\) is consistent. The step is similar to the consistency proof of OLS. \\[\\begin{aligned} \\widehat{\\beta} &amp; =\\left(X&#39;ZWZ&#39;X\\right)^{-1}X&#39;ZWZ&#39;\\left(X&#39;\\beta_{0}+\\epsilon\\right)\\\\ &amp; =\\beta_{0}+\\left(\\frac{X&#39;Z}{n}W\\frac{Z&#39;X}{n}\\right)^{-1}\\frac{X&#39;Z}{n}W\\frac{Z&#39;\\epsilon}{n}\\\\ &amp; \\stackrel{\\mathrm{p}}{\\to}\\beta_{0}+\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\Sigma&#39;W0=\\beta_{0}.\\qedhere\\end{aligned}\\] To check asymptotic normality, we assume that a central limit theorem can be applied. \\(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}z_{i}\\epsilon_{i}\\stackrel{d}{\\to}N\\left(0_{L},\\Omega\\right)\\), where \\(\\Omega=\\mathbb{E}\\left[z_{i}z_{i}&#39;\\epsilon_{i}^{2}\\right].\\) Under Assumptions A.1 and A.2, \\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{0}\\right)\\stackrel{d}{\\to}N\\left(0_{K},\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\Sigma&#39;W\\Omega W\\Sigma\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\right).\\label{eq:normality}\\] Multiply \\(\\widehat{\\beta}-\\beta_{0}\\) by the scaling factor \\(\\sqrt{n}\\), \\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{0}\\right)=\\left(\\frac{X&#39;Z}{n}W\\frac{Z&#39;X}{n}\\right)^{-1}\\frac{X&#39;Z}{n}W\\frac{Z&#39;\\epsilon}{\\sqrt{n}}=\\left(\\frac{X&#39;Z}{n}W\\frac{Z&#39;X}{n}\\right)^{-1}\\frac{X&#39;Z}{n}W\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}z_{i}&#39;\\epsilon_{i}.\\] The conclusion follows by the Slutsky’s theorem as \\[\\frac{X&#39;Z}{n}W\\frac{Z&#39;X}{n}\\stackrel{\\mathrm{p}}{\\to}\\Sigma&#39;W\\Sigma\\] and \\[\\frac{X&#39;Z}{n}W\\frac{1}{\\sqrt{n}}\\sum z_{i}&#39;\\epsilon_{i}\\stackrel{d}{\\to}\\Sigma&#39;W\\times N\\left(0,\\Omega\\right)\\sim N\\left(0,\\Sigma&#39;W\\Omega W\\Sigma\\right).\\qedhere\\] 11.2.1 Efficient GMM It is clear from (\\[eq:normality\\]) that the GMM estimator’s asymptotic variance depends on the choice of \\(W\\). Which \\(W\\) makes the asymptotic variance as small as possible? The answer is \\(W=\\Omega^{-1}\\), under which the efficient asymptotic variance is \\[\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}\\Sigma&#39;\\Omega^{-1}\\Omega\\Omega^{-1}\\Sigma\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}=\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}.\\] For any positive definite symmetric matrix \\(W\\), the difference \\[\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\Sigma&#39;W\\Omega W\\Sigma\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}-\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}\\] is positive semi-definite. To simplify notation, denote \\(A:=W\\Sigma\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\) and \\(B:=\\Omega^{-1}\\Sigma\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}\\) and then the difference of the two matrices becomes \\[\\begin{aligned} &amp; &amp; \\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\Sigma&#39;W\\Omega W\\Sigma\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}-\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}\\\\ &amp; = &amp; A&#39;\\Omega A-B&#39;\\Omega B\\\\ &amp; = &amp; \\left(A-B+B\\right)&#39;\\Omega\\left(A-B+B\\right)-B&#39;\\Omega B\\\\ &amp; = &amp; \\left(A-B\\right)&#39;\\Omega\\left(A-B\\right)+\\left(A-B\\right)&#39;\\Omega B+B&#39;\\Omega\\left(A-B\\right).\\end{aligned}\\] Notice that \\[\\begin{aligned} B&#39;\\Omega A &amp; =\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}\\Sigma&#39;\\Omega^{-1}\\Omega W\\Sigma\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\\\ &amp; =\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}\\Sigma&#39;W\\Sigma\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}=\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}=B&#39;\\Omega B,\\end{aligned}\\] which implies \\(B&#39;\\Omega\\left(A-B\\right)=0\\) and \\(\\left(A-B\\right)&#39;\\Omega B=0\\). We thus conclude that \\[\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\Sigma&#39;W\\Omega W\\Sigma\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}-\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}=\\left(A-B\\right)&#39;\\Omega\\left(A-B\\right)\\] is positive semi-definite. 11.2.2 Two-Step GMM The two-step GMM is one way to construct a feasible efficient GMM estimator. Choose any valid \\(W\\), say \\(W=I_{L}\\), to get a consistent (but inefficient in general) estimator \\(\\hat{\\beta}^{\\sharp}=\\hat{\\beta}^{\\sharp}\\left(W\\right)\\). Save the residual \\(\\widehat{\\epsilon}_{i}=y_{i}-x_{i}&#39;\\hat{\\beta}^{\\sharp}\\) and estimate the variance matrix \\(\\widehat{\\Omega}=\\frac{1}{n}\\sum z_{i}z_{i}&#39;\\widehat{\\epsilon}_{i}^{2}.\\) Notice that this \\(\\widehat{\\Omega}\\) is a consistent for \\(\\Omega\\). Set \\(W=\\widehat{\\Omega}^{-1}\\) and obtain the second estimator \\[\\widehat{\\beta}^{\\natural}=\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})=\\left(X&#39;Z\\widehat{\\Omega}^{-1}Z&#39;X\\right)^{-1}X&#39;Z\\widehat{\\Omega}^{-1}Z&#39;y.\\] This second estimator is asymptotic efficient. Show that if \\(\\widehat{\\Omega}\\stackrel{p}{\\to}\\Omega\\), then \\(\\sqrt{n}\\left(\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})-\\widehat{\\beta}\\left(\\Omega^{-1}\\right)\\right)\\stackrel{p}{\\to}0\\). In other words, the feasible estimator \\(\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})\\) is asymptotically equivalent to the infeasible efficient estimator \\(\\widehat{\\beta}\\left(\\Omega^{-1}\\right)\\). 11.2.3 Two Stage Least Squares If we further assume conditional homoskedasticity \\(\\mathbb{E}\\left[\\epsilon_{i}^{2}|z_{i}\\right]=\\sigma^{2}\\), then \\[\\Omega=\\mathbb{E}\\left[z_{i}z_{i}&#39;\\epsilon_{i}^{2}\\right]=\\mathbb{E}\\left[z_{i}z_{i}&#39;\\mathbb{E}\\left[\\epsilon_{i}^{2}|z_{i}\\right]\\right]=\\sigma^{2}\\mathbb{E}\\left[z_{i}z_{i}&#39;\\right].\\] In the first-step of the two-step GMM we can estimate the variance of the error term by \\(\\widehat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{\\epsilon}_{i}^{2}\\) and the variance matrix by \\(\\widehat{\\Omega}=\\widehat{\\sigma}^{2}\\frac{1}{n}\\sum_{i=1}^{n}z_{i}z_{i}&#39;=\\widehat{\\sigma}^{2}Z&#39;Z/n\\). When we plug this \\(W=\\widehat{\\Omega}^{-1}\\) into the GMM estimator, \\[\\begin{aligned} \\widehat{\\beta} &amp; = &amp; \\left(X&#39;Z\\left(\\widehat{\\sigma}^{2}\\frac{Z&#39;Z}{n}\\right)^{-1}Z&#39;X\\right)^{-1}X&#39;Z\\left(\\widehat{\\sigma}^{2}\\frac{Z&#39;Z}{n}\\right)^{-1}Z&#39;y\\\\ &amp; = &amp; \\left(X&#39;Z\\left(Z&#39;Z\\right)^{-1}Z&#39;X\\right)^{-1}X&#39;Z\\left(Z&#39;Z\\right)^{-1}Z&#39;y.\\end{aligned}\\] This is exactly the same expression of 2SLS for \\(L&gt;K\\). Therefore, 2SLS can be viewed as a special case of GMM with the weighting matrix \\(\\left(Z&#39;Z/n\\right)^{-1}\\). Under conditional homoskedasticity, 2SLS is the efficient estimator. 2SLS is inefficient in general cases of heteroskedasticity, despite its popularity. 2SLS gets its name because it can be obtained using two steps: first regress \\(X\\) on all instruments \\(Z\\), and then regress \\(y\\) on the fitted value along with the included exogenous variables. However, 2SLS can actually be obtained by one step using the above equation. It is a special case of GMM. If an efficient estimator is not too difficult to implement, an econometric theorist would prefer the efficient estimator to an inefficient estimator. The benefits of using the efficient estimator is not limited to more accurate coefficient estimation. Many specification tests, for example the \\(J\\)-statistic we will introduce soon, count on the efficient estimator to lead to a familiar \\(\\chi^{2}\\) distribution under null hypotheses. Otherwise their null asymptotic distributions will be non-standard and thereby critical values must be found by Monte Carlo simulations. 11.3 GMM in Nonlinear Model The principle of GMM can be used in models where the parameter enters the moment conditions nonlinearly. Let \\(g_{i}\\left(\\beta\\right)=g\\left(w_{i},\\beta\\right)\\mapsto\\mathbb{R}^{L}\\) be a function of the data \\(w_{i}\\) and the parameter \\(\\beta\\). If economic theory implies \\(\\mathbb{E}\\left[g_{i}\\left(\\beta\\right)\\right]=0\\), which the statisticians call the estimating equations, we can write the GMM population criterion function as \\[Q\\left(\\beta\\right)=\\mathbb{E}\\left[g_{i}\\left(\\beta\\right)\\right]&#39;W\\mathbb{E}\\left[g_{i}\\left(\\beta\\right)\\right]\\] Nonlinear models nest the linear model as a special case. For the linear IV model in the previous section, the data is \\(w_{i}=\\left(y_{i},x_{i},z_{i}\\right)\\), and the moment function is \\(g\\left(w_{i},\\beta\\right)=z_{i}&#39;\\left(y_{i}-x_{i}\\beta\\right)\\). In practice we use the sample moments to mimic the population moments in the criterion function \\[Q_{n}\\left(\\beta\\right)=\\left(\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\\left(\\beta\\right)\\right)&#39;W\\left(\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\\left(\\beta\\right)\\right).\\] The GMM estimator is defined as \\[\\hat{\\beta}=\\arg\\min_{\\beta}Q_{n}\\left(\\beta\\right).\\] In these nonlinear models, a closed-form solution is in general unavailable, while the asymptotic properties can still be established. We state these asymptotic properties without proofs. (a) If the model is identified, and \\[\\mathbb{P}\\left\\{ \\sup_{\\beta\\in\\mathcal{B}}\\big|\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\\left(\\beta\\right)-\\mathbb{E}\\left[g_{i}\\left(\\beta\\right)\\right]\\big|&gt;\\varepsilon\\right\\} \\to0\\] for any constant \\(\\varepsilon&gt;0\\) where the parametric space \\(\\mathcal{B}\\) is a closed set, then \\(\\hat{\\beta}\\stackrel{\\mathrm{p}}{\\to}\\beta.\\) (b) If in addition \\(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}g_{i}\\left(\\beta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)\\) and \\(\\Sigma=\\mathbb{E}\\left[\\frac{\\partial}{\\partial\\beta&#39;}g_{i}\\left(\\beta_{0}\\right)\\right]\\) is of full column rank, then \\[\\sqrt{n}\\left(\\hat{\\beta}-\\beta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\left(\\Sigma&#39;W\\Omega W\\Sigma\\right)\\left(\\Sigma&#39;W\\Sigma\\right)^{-1}\\right)\\] where \\(\\Omega=\\mathbb{E}\\left[g_{i}\\left(\\beta_{0}\\right)g_{i}\\left(\\beta_{0}\\right)&#39;\\right]\\). (c) If we choose \\(W=\\Omega^{-1}\\), then the GMM estimator is efficient, and the asymptotic variance becomes \\(\\left(\\Sigma&#39;\\Omega^{-1}\\Sigma\\right)^{-1}\\). The list of assumptions in the above statement is incomplete. We only lay out the key conditions but neglect some technical details. \\(Q_{n}\\left(\\beta\\right)\\) measures how close are the moments to zeros. It can serve as a test statistic with proper scaling. Under the null hypothesis \\(\\mathbb{E}\\left[g_{i}\\left(\\beta\\right)\\right]=0_{L}\\), this Sargan-Hansen \\(J\\)-test checks whether a moment condition is violated. The test statistic is \\[\\begin{aligned} J\\left(\\widehat{\\beta}\\right) &amp; =nQ_{n}\\left(\\widehat{\\beta}\\right)=n\\left(\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\\left(\\widehat{\\beta}\\right)\\right)&#39;\\widehat{\\Omega}^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\\left(\\widehat{\\beta}\\right)\\right)\\\\ &amp; =\\left(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}g_{i}\\left(\\widehat{\\beta}\\right)\\right)&#39;\\widehat{\\Omega}^{-1}\\left(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}g_{i}\\left(\\widehat{\\beta}\\right)\\right)\\end{aligned}\\] where \\(\\widehat{\\Omega}\\) is a consistent estimator of \\(\\Omega\\), and \\(\\widehat{\\beta}\\) is an efficient estimator, for example, the two-step GMM estimator \\(\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})\\). This statistic converges in distribution to a \\(\\chi^{2}\\) random variable with degree of freedom \\(L-K\\). That is, under the null, \\[J\\left(\\widehat{\\beta}\\right)\\stackrel{d}{\\to}\\chi^{2}\\left(L-K\\right).\\] If the null hypothesis is false, then the test statistic tends to be large and it is more likely to reject the null. 11.4 Summary The popularity of GMM in econometrics comes from the fact that economic theory is often not informative enough about the underlying parametric relationship amongst the variables. Instead, many economic assumptions suggest moment restrictions. From example, the efficient market hypothesis postulates that the future price movement \\(\\Delta p_{t+1}\\) cannot be predicted by available past information set \\(\\mathscr{I}_{t}\\) so that \\(\\mathbb{E}\\left[\\Delta p_{t+1}|\\mathscr{I}_{t}\\right]=0\\). It implies that any functions of the variables in the information set \\(\\mathscr{I}_{t}\\) are orthogonal to \\(\\Delta p_{t+1}\\). A plethora of moment conditions can be constructed in order to test the efficient market hypothesis. Conceptually simple though, GMM has many practical issues in reality. There has been vast econometric literature about issues of GMM and their remedies. Historical notes: 2SLS was attributed to Theil (1953). In the linear IV model, the \\(J\\)-statistic was proposed by Sargan (1958), and Hansen (1982) extended it to nonlinear models. Further reading: The quadratic form of GMM makes it difficult to accommodate many moments in the big data problems. Empirical likelihood is an alternative estimator to GMM to estimate models defined by moment restrictions. Shi (2016) solves the estimation problem of high-dimensional moments under the framework of empirical likelihood. Zhentao Shi. Dec 3, 2020. "]
]
