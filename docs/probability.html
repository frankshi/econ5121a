<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Probability | Econ5121</title>
  <meta name="description" content="nothing" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Probability | Econ5121" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="nothing" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Probability | Econ5121" />
  
  <meta name="twitter:description" content="nothing" />
  

<meta name="author" content="Zhentao Shi" />


<meta name="date" content="2022-01-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="conditional-expectation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#axiomatic-probability"><i class="fa fa-check"></i><b>2.2</b> Axiomatic Probability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="probability.html"><a href="probability.html#probability-space"><i class="fa fa-check"></i><b>2.2.1</b> Probability Space</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability.html"><a href="probability.html#random-variable"><i class="fa fa-check"></i><b>2.2.2</b> Random Variable</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability.html"><a href="probability.html#distribution-function"><i class="fa fa-check"></i><b>2.2.3</b> Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="probability.html"><a href="probability.html#expected-value"><i class="fa fa-check"></i><b>2.3</b> Expected Value</a><ul>
<li class="chapter" data-level="2.3.1" data-path="probability.html"><a href="probability.html#integration"><i class="fa fa-check"></i><b>2.3.1</b> Integration</a></li>
<li class="chapter" data-level="2.3.2" data-path="probability.html"><a href="probability.html#properties-of-expectations"><i class="fa fa-check"></i><b>2.3.2</b> Properties of Expectations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="probability.html"><a href="probability.html#multivariate-random-variable"><i class="fa fa-check"></i><b>2.4</b> Multivariate Random Variable</a><ul>
<li class="chapter" data-level="2.4.1" data-path="probability.html"><a href="probability.html#conditional-probability-and-bayes-theorem"><i class="fa fa-check"></i><b>2.4.1</b> Conditional Probability and Bayes’ Theorem</a></li>
<li class="chapter" data-level="2.4.2" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>2.4.2</b> Independence</a></li>
<li class="chapter" data-level="2.4.3" data-path="probability.html"><a href="probability.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>2.4.3</b> Law of Iterated Expectations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>2.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conditional-expectation.html"><a href="conditional-expectation.html"><i class="fa fa-check"></i><b>3</b> Conditional Expectation</a><ul>
<li class="chapter" data-level="3.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#linear-projection"><i class="fa fa-check"></i><b>3.1</b> Linear Projection</a><ul>
<li class="chapter" data-level="3.1.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#omitted-variable-bias"><i class="fa fa-check"></i><b>3.1.1</b> Omitted Variable Bias</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="conditional-expectation.html"><a href="conditional-expectation.html#causality"><i class="fa fa-check"></i><b>3.2</b> Causality</a><ul>
<li class="chapter" data-level="3.2.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#structure-and-identification"><i class="fa fa-check"></i><b>3.2.1</b> Structure and Identification</a></li>
<li class="chapter" data-level="3.2.2" data-path="conditional-expectation.html"><a href="conditional-expectation.html#treatment-effect"><i class="fa fa-check"></i><b>3.2.2</b> Treatment Effect</a></li>
<li class="chapter" data-level="3.2.3" data-path="conditional-expectation.html"><a href="conditional-expectation.html#ate-and-cef"><i class="fa fa-check"></i><b>3.2.3</b> ATE and CEF</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>3.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Least Squares: Linear Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html#estimator"><i class="fa fa-check"></i><b>4.1</b> Estimator</a></li>
<li class="chapter" data-level="4.2" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html#subvector"><i class="fa fa-check"></i><b>4.2</b> Subvector</a></li>
<li class="chapter" data-level="4.3" data-path="least-squares-linear-algebra.html"><a href="least-squares-linear-algebra.html#goodness-of-fit"><i class="fa fa-check"></i><b>4.3</b> Goodness of Fit</a></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html"><i class="fa fa-check"></i><b>5</b> Least Squares: Finite Sample Theory</a><ul>
<li class="chapter" data-level="5.1" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#maximum-likelihood"><i class="fa fa-check"></i><b>5.1</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="5.2" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#likelihood-estimation-for-regression"><i class="fa fa-check"></i><b>5.2</b> Likelihood Estimation for Regression</a></li>
<li class="chapter" data-level="5.3" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#finite-sample-distribution"><i class="fa fa-check"></i><b>5.3</b> Finite Sample Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#mean-and-variancemean-and-variance"><i class="fa fa-check"></i><b>5.4</b> Mean and Variance<span id="mean-and-variance" label="mean-and-variance"><span class="math display">\[mean-and-variance\]</span></span></a></li>
<li class="chapter" data-level="5.5" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="5.6" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>5.6</b> Summary</a></li>
<li class="chapter" data-level="5.7" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#appendix"><i class="fa fa-check"></i><b>5.7</b> Appendix</a><ul>
<li class="chapter" data-level="5.7.1" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#joint-normal-distribution"><i class="fa fa-check"></i><b>5.7.1</b> Joint Normal Distribution</a></li>
<li class="chapter" data-level="5.7.2" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#basus-theorem-subsecbasus-theoremsubsecbasus-theorem-labelsubsecbasus-theorem"><i class="fa fa-check"></i><b>5.7.2</b> Basu’s Theorem* [<span class="math display">\[subsec:Basu\&#39;s-Theorem\]</span>]{#subsec:Basu’s-Theorem label=“subsec:Basu’s-Theorem”}</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html"><i class="fa fa-check"></i><b>6</b> Basic Asymptotic Theory</a><ul>
<li class="chapter" data-level="6.1" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#modes-of-convergence"><i class="fa fa-check"></i><b>6.1</b> Modes of Convergence</a></li>
<li class="chapter" data-level="6.2" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#law-of-large-numbers"><i class="fa fa-check"></i><b>6.2</b> Law of Large Numbers</a><ul>
<li class="chapter" data-level="6.2.1" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#cherbyshev-lln"><i class="fa fa-check"></i><b>6.2.1</b> Cherbyshev LLN</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="6.4" data-path="basic-asymptotic-theory.html"><a href="basic-asymptotic-theory.html#tools-for-transformations"><i class="fa fa-check"></i><b>6.4</b> Tools for Transformations</a></li>
<li class="chapter" data-level="6.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html"><i class="fa fa-check"></i><b>7</b> Asymptotic Properties of Least Squares</a><ul>
<li class="chapter" data-level="7.1" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#consistency"><i class="fa fa-check"></i><b>7.1</b> Consistency</a></li>
<li class="chapter" data-level="7.2" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#asymptotic-distribution"><i class="fa fa-check"></i><b>7.2</b> Asymptotic Distribution</a></li>
<li class="chapter" data-level="7.3" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#asymptotic-inference"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Inference</a></li>
<li class="chapter" data-level="7.4" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#consistency-of-feasible-variance-estimator"><i class="fa fa-check"></i><b>7.4</b> Consistency of Feasible Variance Estimator</a><ul>
<li class="chapter" data-level="7.4.1" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#homoskedasticity"><i class="fa fa-check"></i><b>7.4.1</b> Homoskedasticity</a></li>
<li class="chapter" data-level="7.4.2" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#heteroskedasticity"><i class="fa fa-check"></i><b>7.4.2</b> Heteroskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#appendix"><i class="fa fa-check"></i><b>7.6</b> Appendix</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html"><i class="fa fa-check"></i><b>8</b> Asymptotic Properties of MLE</a><ul>
<li class="chapter" data-level="8.1" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#examples-of-mle"><i class="fa fa-check"></i><b>8.1</b> Examples of MLE</a></li>
<li class="chapter" data-level="8.2" data-path="asymptotic-properties-of-least-squares.html"><a href="asymptotic-properties-of-least-squares.html#consistency"><i class="fa fa-check"></i><b>8.2</b> Consistency</a></li>
<li class="chapter" data-level="8.3" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#asymptotic-normality"><i class="fa fa-check"></i><b>8.3</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="8.4" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#information-matrix-equality"><i class="fa fa-check"></i><b>8.4</b> Information Matrix Equality</a></li>
<li class="chapter" data-level="8.5" data-path="asymptotic-properties-of-mle.html"><a href="asymptotic-properties-of-mle.html#cramer-rao-lower-bound"><i class="fa fa-check"></i><b>8.5</b> Cramer-Rao Lower Bound</a></li>
<li class="chapter" data-level="8.6" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>9</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#testing"><i class="fa fa-check"></i><b>9.1</b> Testing</a><ul>
<li class="chapter" data-level="9.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#decision-rule-and-errors"><i class="fa fa-check"></i><b>9.1.1</b> Decision Rule and Errors</a></li>
<li class="chapter" data-level="9.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#optimality"><i class="fa fa-check"></i><b>9.1.2</b> Optimality</a></li>
<li class="chapter" data-level="9.1.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#likelihood-ratio-test-and-wilks-theorem"><i class="fa fa-check"></i><b>9.1.3</b> Likelihood-Ratio Test and Wilks’ theorem</a></li>
<li class="chapter" data-level="9.1.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#score-test"><i class="fa fa-check"></i><b>9.1.4</b> Score Test</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#confidence-intervalconfidence-interval"><i class="fa fa-check"></i><b>9.2</b> Confidence Interval<span id="confidence-interval" label="confidence-interval"><span class="math display">\[confidence-interval\]</span></span></a></li>
<li class="chapter" data-level="9.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#bayesian-credible-set"><i class="fa fa-check"></i><b>9.3</b> Bayesian Credible Set</a></li>
<li class="chapter" data-level="9.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#applications-in-ols"><i class="fa fa-check"></i><b>9.4</b> Applications in OLS</a><ul>
<li class="chapter" data-level="9.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wald-test"><i class="fa fa-check"></i><b>9.4.1</b> Wald Test</a></li>
<li class="chapter" data-level="9.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#lagrangian-multiplier-test"><i class="fa fa-check"></i><b>9.4.2</b> Lagrangian Multiplier Test</a></li>
<li class="chapter" data-level="9.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#likelihood-ratio-test-for-regression"><i class="fa fa-check"></i><b>9.4.3</b> Likelihood-Ratio Test for Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
<li class="chapter" data-level="9.6" data-path="least-squares-finite-sample-theory.html"><a href="least-squares-finite-sample-theory.html#appendix"><i class="fa fa-check"></i><b>9.6</b> Appendix</a><ul>
<li class="chapter" data-level="9.6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#neyman-pearson-lemma"><i class="fa fa-check"></i><b>9.6.1</b> Neyman-Pearson Lemma</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>10</b> Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="panel-data.html"><a href="panel-data.html#fixed-effect"><i class="fa fa-check"></i><b>10.1</b> Fixed Effect</a></li>
<li class="chapter" data-level="10.2" data-path="panel-data.html"><a href="panel-data.html#random-effect"><i class="fa fa-check"></i><b>10.2</b> Random Effect</a></li>
<li class="chapter" data-level="10.3" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>10.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>11</b> Endogeneity</a><ul>
<li class="chapter" data-level="11.1" data-path="endogeneity.html"><a href="endogeneity.html#identification"><i class="fa fa-check"></i><b>11.1</b> Identification</a></li>
<li class="chapter" data-level="11.2" data-path="endogeneity.html"><a href="endogeneity.html#instruments"><i class="fa fa-check"></i><b>11.2</b> Instruments</a></li>
<li class="chapter" data-level="11.3" data-path="endogeneity.html"><a href="endogeneity.html#sources-of-endogeneity"><i class="fa fa-check"></i><b>11.3</b> Sources of Endogeneity</a></li>
<li class="chapter" data-level="11.4" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html"><i class="fa fa-check"></i><b>12</b> Generalized Method of Moments</a><ul>
<li class="chapter" data-level="12.1" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#instrumental-regression"><i class="fa fa-check"></i><b>12.1</b> Instrumental Regression</a><ul>
<li class="chapter" data-level="12.1.1" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#just-identification"><i class="fa fa-check"></i><b>12.1.1</b> Just-identification</a></li>
<li class="chapter" data-level="12.1.2" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#over-identification"><i class="fa fa-check"></i><b>12.1.2</b> Over-identification</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#gmm-estimator"><i class="fa fa-check"></i><b>12.2</b> GMM Estimator</a><ul>
<li class="chapter" data-level="12.2.1" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#efficient-gmm"><i class="fa fa-check"></i><b>12.2.1</b> Efficient GMM</a></li>
<li class="chapter" data-level="12.2.2" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#two-step-gmm"><i class="fa fa-check"></i><b>12.2.2</b> Two-Step GMM</a></li>
<li class="chapter" data-level="12.2.3" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#two-stage-least-squares"><i class="fa fa-check"></i><b>12.2.3</b> Two Stage Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html#gmm-in-nonlinear-model"><i class="fa fa-check"></i><b>12.3</b> GMM in Nonlinear Model</a></li>
<li class="chapter" data-level="12.4" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econ5121</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability" class="section level1">
<h1><span class="header-section-number">2</span> Probability</h1>
<p>For the convenience of online teaching in the fall semester of 2020, the
layout is modified with wide margins and line space for note taking.</p>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>With the advent of big data, computer scientists have come up with a
plethora of new algorithms that are aimed at revealing patterns from
data. <em>Machine learning</em> and <em>artificial intelligence</em> become buzz words
that attract public attention. They defeated best human Go players,
automated manufacturers, powered self-driving vehicles, recognized human
faces, and recommended online purchases. Some of these industrial
successes are based on statistical theory, and statistical theory is
based on probability theory. Although this probabilistic approach is not
the only perspective to understand the behavior of machine learning and
artificial intelligence, it offers one of the most promising paradigms
to rationalize existing algorithms and engineer new ones.</p>
<p>Economics has been an empirical social science since Adam Smith
(1723–1790). Many numerical observations and anecdotes were scattered
in his <em>Wealth of Nations</em> published in 1776. Ragnar Frisch (1895–1973)
and Jan Tinbergen (1903--1994), two pioneer econometricians, were
awarded in 1969 the first Nobel Prize in economics. Econometrics
provides quantitative insights about economic data. It flourishes in
real-world management practices, from households and firms up to
governance at the global level. Today, the big data revolution is
pumping fresh energy into research and exercises of econometric methods.
The mathematical foundation of econometric theory is built on
probability theory as well.</p>
</div>
<div id="axiomatic-probability" class="section level2">
<h2><span class="header-section-number">2.2</span> Axiomatic Probability</h2>
<p>Human beings are awed by uncertainty in daily life. In the old days,
Egyptians consulted oracles, Hebrews inquired prophets, and Chinese
counted on diviners to interpret tortoise shell or bone cracks.
Fortunetellers are abundant in today’s Hong Kong.</p>
<p>Probability theory is a philosophy about uncertainty. Over centuries,
mathematicians strove to contribute to the understanding of randomness.
As measure theory matured in the early 20th century, Andrey Kolmogorov
(1903-1987) built the edifice of modern probability theory in his
monograph published in 1933. The formal mathematical language is a
system that allows rigorous explorations which have made fruitful
advancements, and is now widely accepted in academic and industrial
research.</p>
<p>In this lecture, we will briefly introduce the axiomatic probability
theory along with familiar results covered in undergraduate <em>probability
and statistics</em>. This lecture note is at the level</p>
<ul>
<li><p>Hansen (2020): Introduction to Econometrics, or</p></li>
<li><p>Stachurski (2016): A Primer in Econometric Theory, or</p></li>
<li><p>Casella and Berger (2002): Statistical Inference (second edition)</p></li>
</ul>
<p>Interested readers may want to read this textbook for more examples.</p>
<div id="probability-space" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Probability Space</h3>
<p>A <em>sample space</em> <span class="math inline">\(\Omega\)</span> is a collection of all possible outcomes. It
is a set of things. An <em>event</em> <span class="math inline">\(A\)</span> is a subset of <span class="math inline">\(\Omega\)</span>. It is
something of interest on the sample space. A <span class="math inline">\(\sigma\)</span>-<em>field</em>, denoted
by <span class="math inline">\(\mathcal{F}\)</span>, is a collection of events such that</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\emptyset\in\mathcal{F}\)</span>;</p></li>
<li><p>if an event <span class="math inline">\(A\in\mathcal{F}\)</span>, then <span class="math inline">\(A^{c}\in\mathcal{F}\)</span>;</p></li>
<li><p>if <span class="math inline">\(A_{i}\in\mathcal{F}\)</span> for <span class="math inline">\(i\in\mathbb{N}\)</span>, then
<span class="math inline">\(\bigcup_{i\in\mathbb{N}}A_{i}\in\mathcal{F}\)</span>.</p></li>
</ol>
<p>Implications: (a) Since <span class="math inline">\(\Omega=\emptyset^{c}\in\mathcal{F}\)</span>, we have
<span class="math inline">\(\Omega\in\mathcal{F}\)</span>. (b) If <span class="math inline">\(A_{i}\in\mathcal{F}\)</span> for
<span class="math inline">\(i\in\mathbb{N}\)</span>, then <span class="math inline">\(A_{i}^{c}\in\mathcal{F}\)</span> for <span class="math inline">\(i\in\mathbb{N}\)</span>.
Thus, if <span class="math inline">\(\bigcup_{i\in\mathbb{N}}A_{i}^{c}\in\mathcal{F}\)</span> , then
<span class="math inline">\(\bigcap_{i\in\mathbb{N}}A_{i}=(\bigcup_{i\in\mathbb{N}}A_{i}^{c})^{c}\in\mathcal{F}\)</span>.</p>
<div class="rem">
<ul>
<li>1.1*. Intuitively, a <span class="math inline">\(\sigma\)</span>-field is a pool which is closed for
countable sets to conduct union, difference, and intersection
operations. These are algebraic operations of sets. <span class="math inline">\(\sigma\)</span>-field is
also called <span class="math inline">\(\sigma\)</span>-algebra.</li>
</ul>
</div>
<div class="example">
<p>** 1.1**. Let <span class="math inline">\(\Omega=\{1,2,3,4,5,6\}\)</span>. Some examples of
<span class="math inline">\(\sigma\)</span>-<em>fields</em> include</p>
<ul>
<li><p><span class="math inline">\(\mathcal{F}_{1}=\{\emptyset,\{1,2,3\},\{4,5,6\},\Omega\}\)</span>;</p></li>
<li><p><span class="math inline">\(\mathcal{F}_{2}=\{\emptyset,\{1,3\},\{2,4,5,6\},\Omega\}\)</span>.</p></li>
<li><p>Counterexample:
<span class="math inline">\(\mathcal{F}_{3}=\{\emptyset,\{1,2\},\{4,6\},\Omega\}\)</span> is not a
<span class="math inline">\(\sigma\)</span>-<em>field</em> since <em><span class="math inline">\(\{1,2,4,6\}=\{1,2\}\bigcup\{4,6\}\)</span></em> does
not belong to <span class="math inline">\(\mathcal{F}_{3}\)</span>.</p></li>
</ul>
</div>
<p>The <span class="math inline">\(\sigma\)</span>-field can be viewed as a well-organized structure built on
the ground of the sample space. The pair
<span class="math inline">\(\left(\Omega,\mathcal{F}\right)\)</span> is called a <em>measure space</em>.</p>
<p>Let <span class="math inline">\(\mathcal{G}=\{B_{1},B_{2},\ldots\}\)</span> be an arbitrary collection of
sets, not necessarily a <span class="math inline">\(\sigma\)</span>-field. We say <span class="math inline">\(\mathcal{F}\)</span> is the
smallest <span class="math inline">\(\sigma\)</span>-field generated by <span class="math inline">\(\mathcal{G}\)</span> if
<span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span>, and
<span class="math inline">\(\mathcal{F}\subseteq\mathcal{\tilde{F}}\)</span> for any <span class="math inline">\(\mathcal{\tilde{F}}\)</span>
such that <span class="math inline">\(\mathcal{G}\subseteq\mathcal{\tilde{F}}\)</span>. A <em>Borel
<span class="math inline">\(\sigma\)</span>-field</em> <span class="math inline">\(\mathcal{R}\)</span> is the smallest <span class="math inline">\(\sigma\)</span>-field generated
by the open sets on the real line <span class="math inline">\(\mathbb{R}\)</span>.</p>
<div class="example">
<p>** 1.2**. Let <span class="math inline">\(\Omega=\{1,2,3,4,5,6\}\)</span> and <span class="math inline">\(A=\{\{1\},\{1,3\}\}\)</span>. Then
the smallest <span class="math inline">\(\sigma\)</span>-field generated by <span class="math inline">\(A\)</span> is
<span class="math display">\[\sigma(A)=\{\emptyset,\{1\},\{1,3\},\{3\},\{2,4,5,6\},\{2,3,4,5,6\},\{1,2,4,5,6\},\Omega\}.\]</span></p>
</div>
<p>A function <span class="math inline">\(\mu:(\Omega,\mathcal{F})\mapsto\left[0,\infty\right]\)</span> is
called a <em>measure</em> if it satisfies</p>
<ol style="list-style-type: decimal">
<li><p>(positiveness) <span class="math inline">\(\mu\left(A\right)\geq0\)</span> for all <span class="math inline">\(A\in\mathcal{F}\)</span>;</p></li>
<li><p>(countable additivity) if <span class="math inline">\(A_{i}\in\mathcal{F}\)</span>, <span class="math inline">\(i\in\mathbb{N}\)</span>,
are mutually disjoint, then
<span class="math display">\[\mu\left(\bigcup_{i\in\mathbb{N}}A_{i}\right)=\sum_{i\in\mathbb{N}}\mu\left(A_{i}\right).\]</span></p></li>
</ol>
<p>Measure can be understand as weight or length. In particular, we call
<span class="math inline">\(\mu\)</span> a <em>probability measure</em> if <span class="math inline">\(\mu\left(\Omega\right)=1\)</span>. A
probability measure is often denoted as <span class="math inline">\(P\)</span>. The triple
<span class="math inline">\(\left(\Omega,\mathcal{F},P\right)\)</span> is called a <em>probability space</em>.</p>
<p>So far we have answered the question: “What is a mathematically
well-defined probability?”, but we have not yet answered “How to assign
the probability?” There are two major schools of thinking on probability
assignment. One is <em>frequentist</em>, who considers probability as the
average chance of occurrence if a large number of experiments are
carried out. The other is <em>Bayesian</em>, who deems probability as a
subjective brief. The principles of these two schools are largely
incompatible, while each school has merits and difficulties, which will
be elaborated when discussing hypothesis testing.</p>
</div>
<div id="random-variable" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Random Variable</h3>
<p>The terminology <em>random variable</em> is a historic relic which belies its
modern definition of a deterministic mapping. It is a link between two
measurable spaces such that any event in the <span class="math inline">\(\sigma\)</span>-field installed on
the range can be traced back to an event in the <span class="math inline">\(\sigma\)</span>-field installed
on the domain.</p>
<p>Formally, a function <span class="math inline">\(X:\Omega\mapsto\mathbb{R}\)</span> is
<span class="math inline">\(\left(\Omega,\mathcal{F}\right)\backslash\left(\mathbb{R},\mathcal{R}\right)\)</span>
<em>measurable</em> if
<span class="math display">\[X^{-1}\left(B\right)=\left\{ \omega\in\Omega:X\left(\omega\right)\in B\right\} \in\mathcal{F}\]</span>
for any <span class="math inline">\(B\in\mathcal{R}.\)</span> <em>Random variable</em> is an alternative, and
somewhat romantic, name for a measurable function. The <em><span class="math inline">\(\sigma\)</span>-field
generated by the random variable</em> <span class="math inline">\(X\)</span> is defined as
<span class="math inline">\(\sigma\left(X\right)=\left\{ X^{-1}\left(B\right):B\in\mathcal{R}\right\}\)</span>.</p>
<p>We say a measurable is a <em>discrete random variable</em> if the set
<span class="math inline">\(\left\{ X\left(\omega\right):\omega\in\Omega\right\}\)</span> is finite or
countable. We say it is a <em>continuous random variable</em> if the set
<span class="math inline">\(\left\{ X\left(\omega\right):\omega\in\Omega\right\}\)</span> is uncountable.</p>
<p>A measurable function connects two measurable spaces. No probability is
involved in its definition yet. While if a probability measure <span class="math inline">\(P\)</span> is
installed on <span class="math inline">\((\Omega,\mathcal{F})\)</span>, the measurable function <span class="math inline">\(X\)</span> will
induce a probability measure on <span class="math inline">\((\mathbb{R},\mathcal{R})\)</span>. It is easy
to verify that <span class="math inline">\(P_{X}:(\mathbb{R},\mathcal{R})\mapsto\left[0,1\right]\)</span>
is also a probability measure if defined as
<span class="math display">\[P_{X}\left(B\right)=P\left(X^{-1}\left(B\right)\right)\]</span> for any
<span class="math inline">\(B\in\mathcal{R}\)</span>. This <span class="math inline">\(P_{X}\)</span> is called the probability measure
<em>induced</em> by the measurable function <span class="math inline">\(X\)</span>. The induced probability
measure <span class="math inline">\(P_{X}\)</span> is an offspring of the parent probability measure <span class="math inline">\(P\)</span>
though the channel of <span class="math inline">\(X\)</span>.</p>
</div>
<div id="distribution-function" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Distribution Function</h3>
<p>We go back to some terms that we have learned in a undergraduate
probability course. A <em>(cumulative) distribution function</em>
<span class="math inline">\(F:\mathbb{R}\mapsto[0,1]\)</span> is defined as
<span class="math display">\[F\left(x\right)=P\left(X\leq x\right)=P\left(\{X\leq x\}\right)=P\left(\left\{ \omega\in\Omega:X\left(\omega\right)\leq x\right\} \right).\]</span>
It is often abbreviated as CDF, and it has the following properties.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\lim_{x\to-\infty}F\left(x\right)=0\)</span>,</p></li>
<li><p><span class="math inline">\(\lim_{x\to\infty}F\left(x\right)=1\)</span>,</p></li>
<li><p>non-decreasing,</p></li>
<li><p>right-continuous <span class="math inline">\(\lim_{y\to x^{+}}F\left(y\right)=F\left(x\right).\)</span></p></li>
</ol>
<div class="xca">
<p>** 1.1**. Draw the CDF of a binary distribution; that is, <span class="math inline">\(X=1\)</span> with
probability <span class="math inline">\(p\in\left(0,1\right)\)</span> and <span class="math inline">\(X=0\)</span> with probability <span class="math inline">\(1-p\)</span>.</p>
</div>
<p>For continuous distribution, if there exists a function <span class="math inline">\(f\)</span> such that
for all <span class="math inline">\(x\)</span>,
<span class="math display">\[F\left(x\right)=\int_{-\infty}^{x}f\left(y\right)\mathrm{d}y,\]</span> then
<span class="math inline">\(f\)</span> is called the <em>probability density function</em> of <span class="math inline">\(X\)</span>, often
abbreviated as PDF. It is easy to show that <span class="math inline">\(f\left(x\right)\geq0\)</span> and
<span class="math inline">\(\int_{a}^{b}f\left(x\right)dx=F\left(b\right)-F\left(a\right)\)</span>.</p>
<div class="example">
<p>** 1.3**. We have learned many parametric distributions like the binary
distribution, the Poisson distribution, the uniform distribution, the
exponential distribution, the normal distribution, <span class="math inline">\(\chi^{2}\)</span>, <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>
distributions and so on. They are parametric distributions, meaning that
the CDF or PDF can be completely characterized by very few parameters.</p>
</div>
</div>
</div>
<div id="expected-value" class="section level2">
<h2><span class="header-section-number">2.3</span> Expected Value</h2>
<div id="integration" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Integration</h3>
<p>Integration is one of the most fundamental operations in mathematical
analysis. We have studied Riemann’s integral in the undergraduate
calculus. Riemann’s integral is intuitive, but Lebesgue integral is a
more general approach to defining integration. Lebesgue integral is
constructed by the following steps. <span class="math inline">\(X\)</span> is called a <em>simple function</em> on
a measurable space <span class="math inline">\(\left(\Omega,\mathcal{F}\right)\)</span> if
<span class="math inline">\(X=\sum_{i}a_{i}\cdot1\left\{ A_{i}\right\}\)</span> and this summation is
finite, where <span class="math inline">\(a_{i}\in\mathbb{R}\)</span> and
<span class="math inline">\(\{A_{i}\in\mathcal{F}\}_{i\in\mathbb{N}}\)</span> is a partition of <span class="math inline">\(\Omega\)</span>. A
simple function is measurable.</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(\left(\Omega,\mathcal{F},\mu\right)\)</span> be a measure space. The
integral of the simple function <span class="math inline">\(X\)</span> with respect to <span class="math inline">\(\mu\)</span> is
<span class="math display">\[\int X\mathrm{d}\mu=\sum_{i}a_{i}\mu\left(A_{i}\right).\]</span> Unlike
the Rieman integral, this definition of integration does not
partition the domain into splines of equal length. Instead, it
tracks the distinctive values of the function and the corresponding
measure.</p></li>
<li><p>Let <span class="math inline">\(X\)</span> be a non-negative measurable function. The integral of <span class="math inline">\(X\)</span>
with respect to <span class="math inline">\(\mu\)</span> is
<span class="math display">\[\int X\mathrm{d}\mu=\sup\left\{ \int Y\mathrm{d}\mu:0\leq Y\leq X,\text{ }Y\text{ is simple}\right\} .\]</span></p></li>
<li><p>Let <span class="math inline">\(X\)</span> be a measurable function. Define
<span class="math inline">\(X^{+}=\max\left\{ X,0\right\}\)</span> and
<span class="math inline">\(X^{-}=-\min\left\{ X,0\right\}\)</span>. Both <span class="math inline">\(X^{+}\)</span> and <span class="math inline">\(X^{-}\)</span> are
non-negative functions. The integral of <span class="math inline">\(X\)</span> with respect to <span class="math inline">\(\mu\)</span> is
<span class="math display">\[\int X\mathrm{d}\mu=\int X^{+}\mathrm{d}\mu-\int X^{-}\mathrm{d}\mu.\]</span></p></li>
</ol>
<p>The Step 1 above defines the integral of a simple function. Step 2
defines the integral of a non-negative function as the approximation of
steps functions from below. Step 3 defines the integral of a general
function as the difference of the integral of two non-negative parts.</p>
<div class="rem">
<ul>
<li>1.2*. The integrand that highlights the difference between the
Lebesgue integral and Riemann integral is the Dirichelet function on the
unit interval <span class="math inline">\(1\left\{ x\in\mathbb{Q}\cap[0,1]\right\}\)</span>. It is not
Riemann-integrable whereas its Lebesgue integral. is well defined and
<span class="math inline">\(\int1\left\{ x\in\mathbb{Q}\cap[0,1]\right\} dx=0\)</span>.</li>
</ul>
</div>
<p>If the measure <span class="math inline">\(\mu\)</span> is a probability measure <span class="math inline">\(P\)</span>, then the integral
<span class="math inline">\(\int X\mathrm{d}P\)</span> is called the <em>expected value,</em> or <em>expectation,</em> of
<span class="math inline">\(X\)</span>. We often use the notation <span class="math inline">\(E\left[X\right]\)</span>, instead of
<span class="math inline">\(\int X\mathrm{d}P\)</span>, for convenience.</p>
<p>Expectation provides the average of a random variable, despite that we
cannot foresee the realization of a random variable in a particular
trial (otherwise the study of uncertainty is trivial). In the
frequentist’s view, the expectation is the average outcome if we carry
out a large number of independent trials.</p>
<p>If we know the probability mass function of a discrete random variable,
its expectation is calculated as
<span class="math inline">\(E\left[X\right]=\sum_{x}xP\left(X=x\right)\)</span>, which is the integral of a
simple function. If a continuous random variable has a PDF <span class="math inline">\(f(x)\)</span>, its
expectation can be computed as
<span class="math inline">\(E\left[X\right]=\int xf\left(x\right)\mathrm{d}x\)</span>. These two
expressions are unified as <span class="math inline">\(E[X]=\int X\mathrm{d}P\)</span> by the Lebesgue
integral.</p>
</div>
<div id="properties-of-expectations" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Properties of Expectations</h3>
<p>Here are some properties of mathematical expectations.</p>
<ul>
<li><p>The probability of an event <span class="math inline">\(A\)</span> is the expectation of an indicator
function.
<span class="math inline">\(E\left[1\left\{ A\right\} \right]=1\times P(A)+0\times P(A^{c})=P\left(A\right)\)</span>.</p></li>
<li><p><span class="math inline">\(E\left[X^{r}\right]\)</span> is call the <span class="math inline">\(r\)</span>-moment of <span class="math inline">\(X\)</span>. The <em>mean</em> of a
random variable is the first moment <span class="math inline">\(\mu=E\left[X\right]\)</span>, and the
second <em>centered</em> moment is called the <em>variance</em>
<span class="math inline">\(\mathrm{var}\left[X\right]=E\left[\left(X-\mu\right)^{2}\right]\)</span>.
The third centered moment <span class="math inline">\(E\left[\left(X-\mu\right)^{3}\right]\)</span>,
called <em>skewness</em>, is a measurement of the symmetry of a random
variable, and the fourth centered moment
<span class="math inline">\(E\left[\left(X-\mu\right)^{4}\right]\)</span>, called <em>kurtosis</em>, is a
measurement of the tail thickness.</p></li>
<li><p>Moments do not always exist. For example, the mean of the Cauchy
distribution does not exist, and the variance of the <span class="math inline">\(t(2)\)</span>
distribution does not exist.</p></li>
<li><p><span class="math inline">\(E[\cdot]\)</span> is a linear operation. If <span class="math inline">\(\phi(\cdot)\)</span> is a linear
function, then <span class="math inline">\(E[\phi(X)]=\phi(E[X]).\)</span></p></li>
<li><p><em>Jensen’s inequality</em> is an important fact. A function
<span class="math inline">\(\varphi(\cdot)\)</span> is convex if
<span class="math inline">\(\varphi(ax_{1}+(1-a)x_{2})\leq a\varphi(x_{1})+(1-a)\varphi(x_{2})\)</span>
for all <span class="math inline">\(x_{1},x_{2}\)</span> in the domain and <span class="math inline">\(a\in[0,1]\)</span>. For instance,
<span class="math inline">\(x^{2}\)</span> is a convex function. Jensen’s inequality says that if
<span class="math inline">\(\varphi\left(\cdot\right)\)</span> is a convex function, then
<span class="math inline">\(\varphi\left(E\left[X\right]\right)\leq E\left[\varphi\left(X\right)\right].\)</span></p></li>
<li><p><em>Markov inequality</em> is another simple but important fact. If
<span class="math inline">\(E\left[\left|X\right|^{r}\right]\)</span> exists, then
<span class="math inline">\(P\left(\left|X\right|&gt;\epsilon\right)\leq E\left[\left|X\right|^{r}\right]/\epsilon^{r}\)</span>
for all <span class="math inline">\(r\geq1\)</span>. <em>Chebyshev inequality</em>
<span class="math inline">\(P\left(\left|X\right|&gt;\epsilon\right)\leq E\left[X^{2}\right]/\epsilon^{2}\)</span>
is a special case of the Markov inequality when <span class="math inline">\(r=2\)</span>.</p></li>
<li><p>The distribution of a random variable is completely characterized by
its CDF or PDF. A moment is a function of the distribution. To back
out the underlying distribution from moments, we need to know the
moment-generating function (mgf) <span class="math inline">\(M_{X}(t)=E[e^{tX}]\)</span> for
<span class="math inline">\(t\in\mathbb{R}\)</span> whenever the expectation exists. The <span class="math inline">\(r\)</span>th moment
can be computed from mgf as
<span class="math display">\[E[X^{r}]=\frac{\mathrm{d}^{r}M_{X}(t)}{\mathrm{d}t^{r}}\big\vert_{t=0}.\]</span>
Just like moments, mgf does not always exist.</p></li>
</ul>
</div>
</div>
<div id="multivariate-random-variable" class="section level2">
<h2><span class="header-section-number">2.4</span> Multivariate Random Variable</h2>
<p>A bivariate random variable is a measurable function
<span class="math inline">\(X:\Omega\mapsto\mathbb{R}^{2}\)</span>, and more generally a multivariate
random variable is a measurable function
<span class="math inline">\(X:\Omega\mapsto\mathbb{R}^{n}\)</span>. We can define the <em>joint CDF</em> as
<span class="math inline">\(F\left(x_{1},\ldots,x_{n}\right)=P\left(X_{1}\leq x_{1},\ldots,X_{n}\leq x_{n}\right)\)</span>.
Joint PDF is defined similarly.</p>
<p>It is sufficient to introduce the joint distribution, conditional
distribution and marginal distribution in the simple bivariate case, and
these definitions can be extended to multivariate distributions. Suppose
a bivariate random variable <span class="math inline">\((X,Y)\)</span> has a joint density
<span class="math inline">\(f(\cdot,\cdot)\)</span>. The <em>conditional density</em> can be roughly written as
<span class="math inline">\(f\left(y|x\right)=f\left(x,y\right)/f\left(x\right)\)</span> if we do not
formally deal with the case <span class="math inline">\(f(x)=0\)</span>. The <em>marginal density</em>
<span class="math inline">\(f\left(y\right)=\int f\left(x,y\right)dx\)</span> integrates out the coordinate
that is not interested.</p>
<div id="conditional-probability-and-bayes-theorem" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Conditional Probability and Bayes’ Theorem</h3>
<p>In a probability space <span class="math inline">\((\Omega,\mathcal{F},P)\)</span>, for two events
<span class="math inline">\(A_{1},A_{2}\in\mathcal{F}\)</span> the <em>conditional probability</em> is
<span class="math display">\[P\left(A_{1}|A_{2}\right)=\frac{P\left(A_{1}A_{2}\right)}{P\left(A_{2}\right)}\]</span>
if <span class="math inline">\(P(A_{2})&gt;0\)</span>. In the definition of conditional probability, <span class="math inline">\(A_{2}\)</span>
plays the role of the outcome space so that <span class="math inline">\(P(A_{1}A_{2})\)</span> is
standardized by the total mass <span class="math inline">\(P(A_{2})\)</span>. If <span class="math inline">\(P(A_{2})=0\)</span>, the
conditional probability can still be valid in some cases, but we need to
introduce the <em>dominance</em> between two measures, which we do not
elaborate here.</p>
<p>Since <span class="math inline">\(A_{1}\)</span> and <span class="math inline">\(A_{2}\)</span> are symmetric, we also have
<span class="math inline">\(P(A_{1}A_{2})=P(A_{2}|A_{1})P(A_{1})\)</span>. It implies
<span class="math display">\[P(A_{1}|A_{2})=\frac{P\left(A_{2}|A_{1}\right)P\left(A_{1}\right)}{P\left(A_{2}\right)}\]</span>
This formula is the <em>Bayes’ Theorem</em>.</p>
</div>
<div id="independence" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Independence</h3>
<p>We say two events <span class="math inline">\(A_{1}\)</span> and <span class="math inline">\(A_{2}\)</span> are <em>independent</em> if
<span class="math inline">\(P(A_{1}A_{2})=P(A_{1})P(A_{2})\)</span>. If <span class="math inline">\(P(A_{2})\neq0\)</span>, it is equivalent
to <span class="math inline">\(P(A_{1}|A_{2})=P(A_{1})\)</span>. In words, knowing <span class="math inline">\(A_{2}\)</span> does not change
the probability of <span class="math inline">\(A_{1}\)</span>.</p>
<p>Regarding the independence of two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are
<em>independent</em> if
<span class="math inline">\(P\left(X\in B_{1},Y\in B_{2}\right)=P\left(X\in B_{1}\right)P\left(Y\in B_{2}\right)\)</span>
for any two Borel sets <span class="math inline">\(B_{1}\)</span> and <span class="math inline">\(B_{2}\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(E[XY]=E[X]E[Y]\)</span>. The expectation
of their product is the product of their expectations.</p>
</div>
<div id="law-of-iterated-expectations" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Law of Iterated Expectations</h3>
<p>Given a probability space <span class="math inline">\(\left(\Omega,\mathcal{F},P\right)\)</span>, a <em>sub
<span class="math inline">\(\sigma\)</span>-algebra</em> <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> and a
<span class="math inline">\(\mathcal{F}\)</span>-measurable function <span class="math inline">\(Y\)</span> with <span class="math inline">\(E\left|Y\right|&lt;\infty\)</span>, the
<em>conditional expectation</em> <span class="math inline">\(E\left[Y|\mathcal{G}\right]\)</span> is defined as a
<span class="math inline">\(\mathcal{G}\)</span>-measurable function such that
<span class="math display">\[\int_{A}Y\mathrm{d}P=\int_{A}E\left[Y|\mathcal{G}\right]\mathrm{d}P\]</span>
for all <span class="math inline">\(A\in\mathcal{G}\)</span>. Here <span class="math inline">\(\mathcal{G}\)</span> is a coarse <span class="math inline">\(\sigma\)</span>-field
and <span class="math inline">\(\mathcal{F}\)</span> is a finer <span class="math inline">\(\sigma\)</span>-field.</p>
<p>Taking <span class="math inline">\(A=\Omega\)</span>, we have
<span class="math inline">\(E\left[Y\right]=\int Y\mathrm{d}P=\int E\left[Y|\mathcal{G}\right]\mathrm{d}P=E\left[E\left[Y|\mathcal{G}\right]\right]\)</span>.
The <em>law of iterated expectation</em>
<span class="math display">\[E\left[Y\right]=E\left[E\left[Y|\mathcal{G}\right]\right]\]</span> is a
trivial fact which follows this definition of the conditional
expectation. In the bivariate case, if the conditional density exists,
the conditional expectation can be computed as
<span class="math inline">\(E\left[Y|X\right]=\int yf\left(y|X\right)\mathrm{d}y\)</span>, where the
conditioning variable
<span class="math inline">\(E\left[\cdot|X\right]=E\left[\cdot|\sigma\left(X\right)\right]\)</span> is a
concise notation for the smallest <span class="math inline">\(\sigma\)</span>-field generated by <span class="math inline">\(X\)</span>. The
law of iterated expectation implies
<span class="math inline">\(E\left[E\left[Y|X\right]\right]=E\left[Y\right]\)</span>.</p>
<p>Below are some properties of conditional expectations</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(E\left[E\left[Y|X_{1},X_{2}\right]|X_{1}\right]=E\left[Y|X_{1}\right];\)</span></p></li>
<li><p><span class="math inline">\(E\left[E\left[Y|X_{1}\right]|X_{1},X_{2}\right]=E\left[Y|X_{1}\right];\)</span></p></li>
<li><p><span class="math inline">\(E\left[h\left(X\right)Y|X\right]=h\left(X\right)E\left[Y|X\right].\)</span></p></li>
</ol>
</div>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">2.5</span> Summary</h2>
<p>If it is your first encounter of measure theory, the new definitions
here may seem overwhelmingly abstract. A natural question is that: “I
earned high grade in my undergraduate probability and statistics; do I
really need the fancy mathematics in this lecture to do well in
econometrics?” The answer is yes and no. <em>No</em> is in the sense that if
you want to use econometric methods, instead of grasp the underlying
theory, then the axiomatic probability does not add much to your
weaponry. You can be an excellent economist or applied econometrician
without knowing measure theoretic probability. <em>Yes</em> is in the sense
that without measure theory, we cannot even formally define conditional
expectation, which will be the subject of our next lecture and is a core
concept of econometrics. Moreover, the pillars of asymptotic theory —
law of large numbers and central limit theorem — can only be made
accurate with this foundation. If you are aspired to work on econometric
theory, you will meet and use measure theory so often in your future
study and finally it becomes part of your muscle memory.</p>
<p>In this course, we try to keep a balance manner. On the one hand, many
econometrics topics can be presented with elementary mathematics.
Whenever possible, econometrics should reach wider audience with a plain
appearance, instead of intimidating people by arcane languages. On the
other hand, we introduce these concepts in this lecture and will invoke
them in the discussion of asymptotic theory later. Your investment in
advanced mathematics will not be wasted in vain.</p>
<p><strong>Historical notes:</strong> Measure theory was established in the early 20th
century by a constellation of French/German mathematicians, represented
by Émile Borel, Henri Lebesgue, Johann Radon, etc. Generations of
Russian mathematicians such as Andrey Markov and Andrey Kolmogorov made
fundamental contributions in mathematizing seemingly abstract concepts
of uncertainty and randomness. Their names are immortalized by the Borel
set, the Lebesgue integral, the Radon measure, Markov chain,
Kolmogorov’s zero–one law and many other terminologies named after
them.</p>
<p>Fascinating questions about probability attracted great economists.
Francis Edgeworth (1845–1926) wrote extensively on probability and
statistics. John Maynard Keynes (1883–1946) published <em>A Treatise on
Probability</em> in 1921 which mixed probability and philosophy, although
this piece of work was not as influential as his <em>General Theory of
Employment, Interest and Money</em> in 1936 which later revolutionized
economics.</p>
<p>Today, the technology of collecting data and the processing data is
unbelievably cheaper than that 100 years ago. Unfortunately, the cost of
learning mathematics and developing mathematics has not been
significantly lowered over one century. Only a small handful of talents,
like you, enjoy the privilege and luxury to appreciate the ideas of
these great minds.</p>
<p><strong>Further reading</strong>: <span class="citation">Doob (<a href="#ref-doob1996development" role="doc-biblioref">1996</a>)</span> summarized the development of
axiomatic probability in the first half of the 20th century.</p>
<p><code>Zhentao Shi. Sep 12, 2020.</code></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-doob1996development">
<p>Doob, Joseph L. 1996. “The Development of Rigor in Mathematical Probability (1900–1950).” <em>The American Mathematical Monthly</em> 103 (7): 586–95.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conditional-expectation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
