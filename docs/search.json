[{"path":"index.html","id":"preface","chapter":"1 Preface","heading":"1 Preface","text":"bs4_book style also includes .rmdnote callout block\nlike one.{r collapse=TRUE}` head(beaver1, n = 5)","code":""},{"path":"probability.html","id":"probability","chapter":"2 Probability","heading":"2 Probability","text":"convenience online teaching fall semester 2020, \nlayout modified wide margins line space note taking.","code":""},{"path":"probability.html","id":"introduction","chapter":"2 Probability","heading":"2.1 Introduction","text":"advent big data, computer scientists come \nplethora new algorithms aimed revealing patterns \ndata. Machine learning artificial intelligence become buzz words\nattract public attention. defeated best human Go players,\nautomated manufacturers, powered self-driving vehicles, recognized human\nfaces, recommended online purchases. industrial\nsuccesses based statistical theory, statistical theory \nbased probability theory. Although probabilistic approach \nperspective understand behavior machine learning \nartificial intelligence, offers one promising paradigms\nrationalize existing algorithms engineer new ones.Economics empirical social science since Adam Smith\n(1723–1790). Many numerical observations anecdotes scattered\nWealth Nations published 1776. Ragnar Frisch (1895–1973)\nJan Tinbergen (1903--1994), two pioneer econometricians, \nawarded 1969 first Nobel Prize economics. Econometrics\nprovides quantitative insights economic data. flourishes \nreal-world management practices, households firms \ngovernance global level. Today, big data revolution \npumping fresh energy research exercises econometric methods.\nmathematical foundation econometric theory built \nprobability theory well.","code":""},{"path":"probability.html","id":"axiomatic-probability","chapter":"2 Probability","heading":"2.2 Axiomatic Probability","text":"Human beings awed uncertainty daily life. old days,\nEgyptians consulted oracles, Hebrews inquired prophets, Chinese\ncounted diviners interpret tortoise shell bone cracks.\nFortunetellers abundant today’s Hong Kong.Probability theory philosophy uncertainty. centuries,\nmathematicians strove contribute understanding randomness.\nmeasure theory matured early 20th century, Andrey Kolmogorov\n(1903-1987) built edifice modern probability theory \nmonograph published 1933. formal mathematical language \nsystem allows rigorous explorations made fruitful\nadvancements, now widely accepted academic industrial\nresearch.lecture, briefly introduce axiomatic probability\ntheory along familiar results covered undergraduate probability\nstatistics. lecture note levelHansen (2020): Introduction Econometrics, orHansen (2020): Introduction Econometrics, orStachurski (2016): Primer Econometric Theory, orStachurski (2016): Primer Econometric Theory, orCasella Berger (2002): Statistical Inference (second edition)Casella Berger (2002): Statistical Inference (second edition)Interested readers may want read textbook examples.","code":""},{"path":"probability.html","id":"probability-space","chapter":"2 Probability","heading":"2.2.1 Probability Space","text":"sample space \\(\\Omega\\) collection possible outcomes. \nset things. event \\(\\) subset \\(\\Omega\\). \nsomething interest sample space. \\(\\sigma\\)-field, denoted\n\\(\\mathcal{F}\\), collection events \\(\\emptyset\\\\mathcal{F}\\);\\(\\emptyset\\\\mathcal{F}\\);event \\(\\\\mathcal{F}\\), \\(^{c}\\\\mathcal{F}\\);event \\(\\\\mathcal{F}\\), \\(^{c}\\\\mathcal{F}\\);\\(A_{}\\\\mathcal{F}\\) \\(\\\\mathbb{N}\\), \n\\(\\bigcup_{\\\\mathbb{N}}A_{}\\\\mathcal{F}\\).\\(A_{}\\\\mathcal{F}\\) \\(\\\\mathbb{N}\\), \n\\(\\bigcup_{\\\\mathbb{N}}A_{}\\\\mathcal{F}\\).Implications: () Since \\(\\Omega=\\emptyset^{c}\\\\mathcal{F}\\), \n\\(\\Omega\\\\mathcal{F}\\). (b) \\(A_{}\\\\mathcal{F}\\) \n\\(\\\\mathbb{N}\\), \\(A_{}^{c}\\\\mathcal{F}\\) \\(\\\\mathbb{N}\\).\nThus, \\(\\bigcup_{\\\\mathbb{N}}A_{}^{c}\\\\mathcal{F}\\) , \n\\(\\bigcap_{\\\\mathbb{N}}A_{}=(\\bigcup_{\\\\mathbb{N}}A_{}^{c})^{c}\\\\mathcal{F}\\).1.1*. Intuitively, \\(\\sigma\\)-field pool closed \ncountable sets conduct union, difference, intersection\noperations. algebraic operations sets. \\(\\sigma\\)-field \nalso called \\(\\sigma\\)-algebra.Example 2.1  ** 1.1**. Let \\(\\Omega=\\{1,2,3,4,5,6\\}\\). examples \n\\(\\sigma\\)-fields include\\(\\mathcal{F}_{1}=\\{\\emptyset,\\{1,2,3\\},\\{4,5,6\\},\\Omega\\}\\);\\(\\mathcal{F}_{1}=\\{\\emptyset,\\{1,2,3\\},\\{4,5,6\\},\\Omega\\}\\);\\(\\mathcal{F}_{2}=\\{\\emptyset,\\{1,3\\},\\{2,4,5,6\\},\\Omega\\}\\).\\(\\mathcal{F}_{2}=\\{\\emptyset,\\{1,3\\},\\{2,4,5,6\\},\\Omega\\}\\).Counterexample:\n\\(\\mathcal{F}_{3}=\\{\\emptyset,\\{1,2\\},\\{4,6\\},\\Omega\\}\\) \n\\(\\sigma\\)-field since \\(\\{1,2,4,6\\}=\\{1,2\\}\\bigcup\\{4,6\\}\\) \nbelong \\(\\mathcal{F}_{3}\\).Counterexample:\n\\(\\mathcal{F}_{3}=\\{\\emptyset,\\{1,2\\},\\{4,6\\},\\Omega\\}\\) \n\\(\\sigma\\)-field since \\(\\{1,2,4,6\\}=\\{1,2\\}\\bigcup\\{4,6\\}\\) \nbelong \\(\\mathcal{F}_{3}\\).\\(\\sigma\\)-field can viewed well-organized structure built \nground sample space. pair\n\\(\\left(\\Omega,\\mathcal{F}\\right)\\) called measure space.Let \\(\\mathcal{G}=\\{B_{1},B_{2},\\ldots\\}\\) arbitrary collection \nsets, necessarily \\(\\sigma\\)-field. say \\(\\mathcal{F}\\) \nsmallest \\(\\sigma\\)-field generated \\(\\mathcal{G}\\) \n\\(\\mathcal{G}\\subseteq\\mathcal{F}\\), \n\\(\\mathcal{F}\\subseteq\\mathcal{\\tilde{F}}\\) \\(\\mathcal{\\tilde{F}}\\)\n\\(\\mathcal{G}\\subseteq\\mathcal{\\tilde{F}}\\). Borel\n\\(\\sigma\\)-field \\(\\mathcal{R}\\) smallest \\(\\sigma\\)-field generated\nopen sets real line \\(\\mathbb{R}\\).Example 2.2  ** 1.2**. Let \\(\\Omega=\\{1,2,3,4,5,6\\}\\) \\(=\\{\\{1\\},\\{1,3\\}\\}\\). \nsmallest \\(\\sigma\\)-field generated \\(\\) \n\\[\\sigma()=\\{\\emptyset,\\{1\\},\\{1,3\\},\\{3\\},\\{2,4,5,6\\},\\{2,3,4,5,6\\},\\{1,2,4,5,6\\},\\Omega\\}.\\]function \\(\\mu:(\\Omega,\\mathcal{F})\\mapsto\\left[0,\\infty\\right]\\) \ncalled measure satisfies(positiveness) \\(\\mu\\left(\\right)\\geq0\\) \\(\\\\mathcal{F}\\);(positiveness) \\(\\mu\\left(\\right)\\geq0\\) \\(\\\\mathcal{F}\\);(countable additivity) \\(A_{}\\\\mathcal{F}\\), \\(\\\\mathbb{N}\\),\nmutually disjoint, \n\\[\\mu\\left(\\bigcup_{\\\\mathbb{N}}A_{}\\right)=\\sum_{\\\\mathbb{N}}\\mu\\left(A_{}\\right).\\](countable additivity) \\(A_{}\\\\mathcal{F}\\), \\(\\\\mathbb{N}\\),\nmutually disjoint, \n\\[\\mu\\left(\\bigcup_{\\\\mathbb{N}}A_{}\\right)=\\sum_{\\\\mathbb{N}}\\mu\\left(A_{}\\right).\\]Measure can understand weight length. particular, call\n\\(\\mu\\) probability measure \\(\\mu\\left(\\Omega\\right)=1\\). \nprobability measure often denoted \\(P\\). triple\n\\(\\left(\\Omega,\\mathcal{F},P\\right)\\) called probability space.far answered question: “mathematically\nwell-defined probability?”, yet answered “assign\nprobability?” two major schools thinking probability\nassignment. One frequentist, considers probability \naverage chance occurrence large number experiments \ncarried . Bayesian, deems probability \nsubjective brief. principles two schools largely\nincompatible, school merits difficulties, \nelaborated discussing hypothesis testing.","code":""},{"path":"probability.html","id":"random-variable","chapter":"2 Probability","heading":"2.2.2 Random Variable","text":"terminology random variable historic relic belies \nmodern definition deterministic mapping. link two\nmeasurable spaces event \\(\\sigma\\)-field installed \nrange can traced back event \\(\\sigma\\)-field installed\ndomain.Formally, function \\(X:\\Omega\\mapsto\\mathbb{R}\\) \n\\(\\left(\\Omega,\\mathcal{F}\\right)\\backslash\\left(\\mathbb{R},\\mathcal{R}\\right)\\)\nmeasurable \n\\[X^{-1}\\left(B\\right)=\\left\\{ \\omega\\\\Omega:X\\left(\\omega\\right)\\B\\right\\} \\\\mathcal{F}\\]\n\\(B\\\\mathcal{R}.\\) Random variable alternative, \nsomewhat romantic, name measurable function. \\(\\sigma\\)-field\ngenerated random variable \\(X\\) defined \n\\(\\sigma\\left(X\\right)=\\left\\{ X^{-1}\\left(B\\right):B\\\\mathcal{R}\\right\\}\\).say measurable discrete random variable set\n\\(\\left\\{ X\\left(\\omega\\right):\\omega\\\\Omega\\right\\}\\) finite \ncountable. say continuous random variable set\n\\(\\left\\{ X\\left(\\omega\\right):\\omega\\\\Omega\\right\\}\\) uncountable.measurable function connects two measurable spaces. probability \ninvolved definition yet. probability measure \\(P\\) \ninstalled \\((\\Omega,\\mathcal{F})\\), measurable function \\(X\\) \ninduce probability measure \\((\\mathbb{R},\\mathcal{R})\\). easy\nverify \\(P_{X}:(\\mathbb{R},\\mathcal{R})\\mapsto\\left[0,1\\right]\\)\nalso probability measure defined \n\\[P_{X}\\left(B\\right)=P\\left(X^{-1}\\left(B\\right)\\right)\\] \n\\(B\\\\mathcal{R}\\). \\(P_{X}\\) called probability measure\ninduced measurable function \\(X\\). induced probability\nmeasure \\(P_{X}\\) offspring parent probability measure \\(P\\)\nthough channel \\(X\\).","code":""},{"path":"probability.html","id":"distribution-function","chapter":"2 Probability","heading":"2.2.3 Distribution Function","text":"go back terms learned undergraduate\nprobability course. (cumulative) distribution function\n\\(F:\\mathbb{R}\\mapsto[0,1]\\) defined \n\\[F\\left(x\\right)=P\\left(X\\leq x\\right)=P\\left(\\{X\\leq x\\}\\right)=P\\left(\\left\\{ \\omega\\\\Omega:X\\left(\\omega\\right)\\leq x\\right\\} \\right).\\]\noften abbreviated CDF, following properties.\\(\\lim_{x\\-\\infty}F\\left(x\\right)=0\\),\\(\\lim_{x\\-\\infty}F\\left(x\\right)=0\\),\\(\\lim_{x\\\\infty}F\\left(x\\right)=1\\),\\(\\lim_{x\\\\infty}F\\left(x\\right)=1\\),non-decreasing,non-decreasing,right-continuous \\(\\lim_{y\\x^{+}}F\\left(y\\right)=F\\left(x\\right).\\)right-continuous \\(\\lim_{y\\x^{+}}F\\left(y\\right)=F\\left(x\\right).\\)** 1.1**. Draw CDF binary distribution; , \\(X=1\\) \nprobability \\(p\\\\left(0,1\\right)\\) \\(X=0\\) probability \\(1-p\\).continuous distribution, exists function \\(f\\) \n\\(x\\),\n\\[F\\left(x\\right)=\\int_{-\\infty}^{x}f\\left(y\\right)\\mathrm{d}y,\\] \n\\(f\\) called probability density function \\(X\\), often\nabbreviated PDF. easy show \\(f\\left(x\\right)\\geq0\\) \n\\(\\int_{}^{b}f\\left(x\\right)dx=F\\left(b\\right)-F\\left(\\right)\\).Example 2.3  ** 1.3**. learned many parametric distributions like binary\ndistribution, Poisson distribution, uniform distribution, \nexponential distribution, normal distribution, \\(\\chi^{2}\\), \\(t\\), \\(F\\)\ndistributions . parametric distributions, meaning \nCDF PDF can completely characterized parameters.","code":""},{"path":"probability.html","id":"expected-value","chapter":"2 Probability","heading":"2.3 Expected Value","text":"","code":""},{"path":"probability.html","id":"integration","chapter":"2 Probability","heading":"2.3.1 Integration","text":"Integration one fundamental operations mathematical\nanalysis. studied Riemann’s integral undergraduate\ncalculus. Riemann’s integral intuitive, Lebesgue integral \ngeneral approach defining integration. Lebesgue integral \nconstructed following steps. \\(X\\) called simple function \nmeasurable space \\(\\left(\\Omega,\\mathcal{F}\\right)\\) \n\\(X=\\sum_{}a_{}\\cdot1\\left\\{ A_{}\\right\\}\\) summation \nfinite, \\(a_{}\\\\mathbb{R}\\) \n\\(\\{A_{}\\\\mathcal{F}\\}_{\\\\mathbb{N}}\\) partition \\(\\Omega\\). \nsimple function measurable.Let \\(\\left(\\Omega,\\mathcal{F},\\mu\\right)\\) measure space. \nintegral simple function \\(X\\) respect \\(\\mu\\) \n\\[\\int X\\mathrm{d}\\mu=\\sum_{}a_{}\\mu\\left(A_{}\\right).\\] Unlike\nRieman integral, definition integration \npartition domain splines equal length. Instead, \ntracks distinctive values function corresponding\nmeasure.Let \\(\\left(\\Omega,\\mathcal{F},\\mu\\right)\\) measure space. \nintegral simple function \\(X\\) respect \\(\\mu\\) \n\\[\\int X\\mathrm{d}\\mu=\\sum_{}a_{}\\mu\\left(A_{}\\right).\\] Unlike\nRieman integral, definition integration \npartition domain splines equal length. Instead, \ntracks distinctive values function corresponding\nmeasure.Let \\(X\\) non-negative measurable function. integral \\(X\\)\nrespect \\(\\mu\\) \n\\[\\int X\\mathrm{d}\\mu=\\sup\\left\\{ \\int Y\\mathrm{d}\\mu:0\\leq Y\\leq X,\\text{ }Y\\text{ simple}\\right\\} .\\]Let \\(X\\) non-negative measurable function. integral \\(X\\)\nrespect \\(\\mu\\) \n\\[\\int X\\mathrm{d}\\mu=\\sup\\left\\{ \\int Y\\mathrm{d}\\mu:0\\leq Y\\leq X,\\text{ }Y\\text{ simple}\\right\\} .\\]Let \\(X\\) measurable function. Define\n\\(X^{+}=\\max\\left\\{ X,0\\right\\}\\) \n\\(X^{-}=-\\min\\left\\{ X,0\\right\\}\\). \\(X^{+}\\) \\(X^{-}\\) \nnon-negative functions. integral \\(X\\) respect \\(\\mu\\) \n\\[\\int X\\mathrm{d}\\mu=\\int X^{+}\\mathrm{d}\\mu-\\int X^{-}\\mathrm{d}\\mu.\\]Let \\(X\\) measurable function. Define\n\\(X^{+}=\\max\\left\\{ X,0\\right\\}\\) \n\\(X^{-}=-\\min\\left\\{ X,0\\right\\}\\). \\(X^{+}\\) \\(X^{-}\\) \nnon-negative functions. integral \\(X\\) respect \\(\\mu\\) \n\\[\\int X\\mathrm{d}\\mu=\\int X^{+}\\mathrm{d}\\mu-\\int X^{-}\\mathrm{d}\\mu.\\]Step 1 defines integral simple function. Step 2\ndefines integral non-negative function approximation \nsteps functions . Step 3 defines integral general\nfunction difference integral two non-negative parts.1.2*. integrand highlights difference \nLebesgue integral Riemann integral Dirichelet function \nunit interval \\(1\\left\\{ x\\\\mathbb{Q}\\cap[0,1]\\right\\}\\). \nRiemann-integrable whereas Lebesgue integral. well defined \n\\(\\int1\\left\\{ x\\\\mathbb{Q}\\cap[0,1]\\right\\} dx=0\\).measure \\(\\mu\\) probability measure \\(P\\), integral\n\\(\\int X\\mathrm{d}P\\) called expected value, expectation, \n\\(X\\). often use notation \\(E\\left[X\\right]\\), instead \n\\(\\int X\\mathrm{d}P\\), convenience.Expectation provides average random variable, despite \nforesee realization random variable particular\ntrial (otherwise study uncertainty trivial). \nfrequentist’s view, expectation average outcome carry\nlarge number independent trials.know probability mass function discrete random variable,\nexpectation calculated \n\\(E\\left[X\\right]=\\sum_{x}xP\\left(X=x\\right)\\), integral \nsimple function. continuous random variable PDF \\(f(x)\\), \nexpectation can computed \n\\(E\\left[X\\right]=\\int xf\\left(x\\right)\\mathrm{d}x\\). two\nexpressions unified \\(E[X]=\\int X\\mathrm{d}P\\) Lebesgue\nintegral.","code":""},{"path":"probability.html","id":"properties-of-expectations","chapter":"2 Probability","heading":"2.3.2 Properties of Expectations","text":"properties mathematical expectations.probability event \\(\\) expectation indicator\nfunction.\n\\(E\\left[1\\left\\{ \\right\\} \\right]=1\\times P()+0\\times P(^{c})=P\\left(\\right)\\).probability event \\(\\) expectation indicator\nfunction.\n\\(E\\left[1\\left\\{ \\right\\} \\right]=1\\times P()+0\\times P(^{c})=P\\left(\\right)\\).\\(E\\left[X^{r}\\right]\\) call \\(r\\)-moment \\(X\\). mean \nrandom variable first moment \\(\\mu=E\\left[X\\right]\\), \nsecond centered moment called variance\n\\(\\mathrm{var}\\left[X\\right]=E\\left[\\left(X-\\mu\\right)^{2}\\right]\\).\nthird centered moment \\(E\\left[\\left(X-\\mu\\right)^{3}\\right]\\),\ncalled skewness, measurement symmetry random\nvariable, fourth centered moment\n\\(E\\left[\\left(X-\\mu\\right)^{4}\\right]\\), called kurtosis, \nmeasurement tail thickness.\\(E\\left[X^{r}\\right]\\) call \\(r\\)-moment \\(X\\). mean \nrandom variable first moment \\(\\mu=E\\left[X\\right]\\), \nsecond centered moment called variance\n\\(\\mathrm{var}\\left[X\\right]=E\\left[\\left(X-\\mu\\right)^{2}\\right]\\).\nthird centered moment \\(E\\left[\\left(X-\\mu\\right)^{3}\\right]\\),\ncalled skewness, measurement symmetry random\nvariable, fourth centered moment\n\\(E\\left[\\left(X-\\mu\\right)^{4}\\right]\\), called kurtosis, \nmeasurement tail thickness.Moments always exist. example, mean Cauchy\ndistribution exist, variance \\(t(2)\\)\ndistribution exist.Moments always exist. example, mean Cauchy\ndistribution exist, variance \\(t(2)\\)\ndistribution exist.\\(E[\\cdot]\\) linear operation. \\(\\phi(\\cdot)\\) linear\nfunction, \\(E[\\phi(X)]=\\phi(E[X]).\\)\\(E[\\cdot]\\) linear operation. \\(\\phi(\\cdot)\\) linear\nfunction, \\(E[\\phi(X)]=\\phi(E[X]).\\)Jensen’s inequality important fact. function\n\\(\\varphi(\\cdot)\\) convex \n\\(\\varphi(ax_{1}+(1-)x_{2})\\leq \\varphi(x_{1})+(1-)\\varphi(x_{2})\\)\n\\(x_{1},x_{2}\\) domain \\(\\[0,1]\\). instance,\n\\(x^{2}\\) convex function. Jensen’s inequality says \n\\(\\varphi\\left(\\cdot\\right)\\) convex function, \n\\(\\varphi\\left(E\\left[X\\right]\\right)\\leq E\\left[\\varphi\\left(X\\right)\\right].\\)Jensen’s inequality important fact. function\n\\(\\varphi(\\cdot)\\) convex \n\\(\\varphi(ax_{1}+(1-)x_{2})\\leq \\varphi(x_{1})+(1-)\\varphi(x_{2})\\)\n\\(x_{1},x_{2}\\) domain \\(\\[0,1]\\). instance,\n\\(x^{2}\\) convex function. Jensen’s inequality says \n\\(\\varphi\\left(\\cdot\\right)\\) convex function, \n\\(\\varphi\\left(E\\left[X\\right]\\right)\\leq E\\left[\\varphi\\left(X\\right)\\right].\\)Markov inequality another simple important fact. \n\\(E\\left[\\left|X\\right|^{r}\\right]\\) exists, \n\\(P\\left(\\left|X\\right|>\\epsilon\\right)\\leq E\\left[\\left|X\\right|^{r}\\right]/\\epsilon^{r}\\)\n\\(r\\geq1\\). Chebyshev inequality\n\\(P\\left(\\left|X\\right|>\\epsilon\\right)\\leq E\\left[X^{2}\\right]/\\epsilon^{2}\\)\nspecial case Markov inequality \\(r=2\\).Markov inequality another simple important fact. \n\\(E\\left[\\left|X\\right|^{r}\\right]\\) exists, \n\\(P\\left(\\left|X\\right|>\\epsilon\\right)\\leq E\\left[\\left|X\\right|^{r}\\right]/\\epsilon^{r}\\)\n\\(r\\geq1\\). Chebyshev inequality\n\\(P\\left(\\left|X\\right|>\\epsilon\\right)\\leq E\\left[X^{2}\\right]/\\epsilon^{2}\\)\nspecial case Markov inequality \\(r=2\\).distribution random variable completely characterized \nCDF PDF. moment function distribution. back\nunderlying distribution moments, need know \nmoment-generating function (mgf) \\(M_{X}(t)=E[e^{tX}]\\) \n\\(t\\\\mathbb{R}\\) whenever expectation exists. \\(r\\)th moment\ncan computed mgf \n\\[E[X^{r}]=\\frac{\\mathrm{d}^{r}M_{X}(t)}{\\mathrm{d}t^{r}}\\big\\vert_{t=0}.\\]\nJust like moments, mgf always exist.distribution random variable completely characterized \nCDF PDF. moment function distribution. back\nunderlying distribution moments, need know \nmoment-generating function (mgf) \\(M_{X}(t)=E[e^{tX}]\\) \n\\(t\\\\mathbb{R}\\) whenever expectation exists. \\(r\\)th moment\ncan computed mgf \n\\[E[X^{r}]=\\frac{\\mathrm{d}^{r}M_{X}(t)}{\\mathrm{d}t^{r}}\\big\\vert_{t=0}.\\]\nJust like moments, mgf always exist.","code":""},{"path":"probability.html","id":"multivariate-random-variable","chapter":"2 Probability","heading":"2.4 Multivariate Random Variable","text":"bivariate random variable measurable function\n\\(X:\\Omega\\mapsto\\mathbb{R}^{2}\\), generally multivariate\nrandom variable measurable function\n\\(X:\\Omega\\mapsto\\mathbb{R}^{n}\\). can define joint CDF \n\\(F\\left(x_{1},\\ldots,x_{n}\\right)=P\\left(X_{1}\\leq x_{1},\\ldots,X_{n}\\leq x_{n}\\right)\\).\nJoint PDF defined similarly.sufficient introduce joint distribution, conditional\ndistribution marginal distribution simple bivariate case, \ndefinitions can extended multivariate distributions. Suppose\nbivariate random variable \\((X,Y)\\) joint density\n\\(f(\\cdot,\\cdot)\\). conditional density can roughly written \n\\(f\\left(y|x\\right)=f\\left(x,y\\right)/f\\left(x\\right)\\) \nformally deal case \\(f(x)=0\\). marginal density\n\\(f\\left(y\\right)=\\int f\\left(x,y\\right)dx\\) integrates coordinate\ninterested.","code":""},{"path":"probability.html","id":"conditional-probability-and-bayes-theorem","chapter":"2 Probability","heading":"2.4.1 Conditional Probability and Bayes’ Theorem","text":"probability space \\((\\Omega,\\mathcal{F},P)\\), two events\n\\(A_{1},A_{2}\\\\mathcal{F}\\) conditional probability \n\\[P\\left(A_{1}|A_{2}\\right)=\\frac{P\\left(A_{1}A_{2}\\right)}{P\\left(A_{2}\\right)}\\]\n\\(P(A_{2})>0\\). definition conditional probability, \\(A_{2}\\)\nplays role outcome space \\(P(A_{1}A_{2})\\) \nstandardized total mass \\(P(A_{2})\\). \\(P(A_{2})=0\\), \nconditional probability can still valid cases, need \nintroduce dominance two measures, \nelaborate .Since \\(A_{1}\\) \\(A_{2}\\) symmetric, also \n\\(P(A_{1}A_{2})=P(A_{2}|A_{1})P(A_{1})\\). implies\n\\[P(A_{1}|A_{2})=\\frac{P\\left(A_{2}|A_{1}\\right)P\\left(A_{1}\\right)}{P\\left(A_{2}\\right)}\\]\nformula Bayes’ Theorem.","code":""},{"path":"probability.html","id":"independence","chapter":"2 Probability","heading":"2.4.2 Independence","text":"say two events \\(A_{1}\\) \\(A_{2}\\) independent \n\\(P(A_{1}A_{2})=P(A_{1})P(A_{2})\\). \\(P(A_{2})\\neq0\\), equivalent\n\\(P(A_{1}|A_{2})=P(A_{1})\\). words, knowing \\(A_{2}\\) change\nprobability \\(A_{1}\\).Regarding independence two random variables, \\(X\\) \\(Y\\) \nindependent \n\\(P\\left(X\\B_{1},Y\\B_{2}\\right)=P\\left(X\\B_{1}\\right)P\\left(Y\\B_{2}\\right)\\)\ntwo Borel sets \\(B_{1}\\) \\(B_{2}\\).\\(X\\) \\(Y\\) independent, \\(E[XY]=E[X]E[Y]\\). expectation\nproduct product expectations.","code":""},{"path":"probability.html","id":"law-of-iterated-expectations","chapter":"2 Probability","heading":"2.4.3 Law of Iterated Expectations","text":"Given probability space \\(\\left(\\Omega,\\mathcal{F},P\\right)\\), sub\n\\(\\sigma\\)-algebra \\(\\mathcal{G}\\subseteq\\mathcal{F}\\) \n\\(\\mathcal{F}\\)-measurable function \\(Y\\) \\(E\\left|Y\\right|<\\infty\\), \nconditional expectation \\(E\\left[Y|\\mathcal{G}\\right]\\) defined \n\\(\\mathcal{G}\\)-measurable function \n\\[\\int_{}Y\\mathrm{d}P=\\int_{}E\\left[Y|\\mathcal{G}\\right]\\mathrm{d}P\\]\n\\(\\\\mathcal{G}\\). \\(\\mathcal{G}\\) coarse \\(\\sigma\\)-field\n\\(\\mathcal{F}\\) finer \\(\\sigma\\)-field.Taking \\(=\\Omega\\), \n\\(E\\left[Y\\right]=\\int Y\\mathrm{d}P=\\int E\\left[Y|\\mathcal{G}\\right]\\mathrm{d}P=E\\left[E\\left[Y|\\mathcal{G}\\right]\\right]\\).\nlaw iterated expectation\n\\[E\\left[Y\\right]=E\\left[E\\left[Y|\\mathcal{G}\\right]\\right]\\] \ntrivial fact follows definition conditional\nexpectation. bivariate case, conditional density exists,\nconditional expectation can computed \n\\(E\\left[Y|X\\right]=\\int yf\\left(y|X\\right)\\mathrm{d}y\\), \nconditioning variable\n\\(E\\left[\\cdot|X\\right]=E\\left[\\cdot|\\sigma\\left(X\\right)\\right]\\) \nconcise notation smallest \\(\\sigma\\)-field generated \\(X\\). \nlaw iterated expectation implies\n\\(E\\left[E\\left[Y|X\\right]\\right]=E\\left[Y\\right]\\).properties conditional expectations\\(E\\left[E\\left[Y|X_{1},X_{2}\\right]|X_{1}\\right]=E\\left[Y|X_{1}\\right];\\)\\(E\\left[E\\left[Y|X_{1},X_{2}\\right]|X_{1}\\right]=E\\left[Y|X_{1}\\right];\\)\\(E\\left[E\\left[Y|X_{1}\\right]|X_{1},X_{2}\\right]=E\\left[Y|X_{1}\\right];\\)\\(E\\left[E\\left[Y|X_{1}\\right]|X_{1},X_{2}\\right]=E\\left[Y|X_{1}\\right];\\)\\(E\\left[h\\left(X\\right)Y|X\\right]=h\\left(X\\right)E\\left[Y|X\\right].\\)\\(E\\left[h\\left(X\\right)Y|X\\right]=h\\left(X\\right)E\\left[Y|X\\right].\\)","code":""},{"path":"probability.html","id":"summary","chapter":"2 Probability","heading":"2.5 Summary","text":"first encounter measure theory, new definitions\nmay seem overwhelmingly abstract. natural question : “\nearned high grade undergraduate probability statistics; \nreally need fancy mathematics lecture well \neconometrics?” answer yes . sense \nwant use econometric methods, instead grasp underlying\ntheory, axiomatic probability add much \nweaponry. can excellent economist applied econometrician\nwithout knowing measure theoretic probability. Yes sense\nwithout measure theory, even formally define conditional\nexpectation, subject next lecture core\nconcept econometrics. Moreover, pillars asymptotic theory —\nlaw large numbers central limit theorem — can made\naccurate foundation. aspired work econometric\ntheory, meet use measure theory often future\nstudy finally becomes part muscle memory.course, try keep balance manner. one hand, many\neconometrics topics can presented elementary mathematics.\nWhenever possible, econometrics reach wider audience plain\nappearance, instead intimidating people arcane languages. \nhand, introduce concepts lecture invoke\ndiscussion asymptotic theory later. investment \nadvanced mathematics wasted vain.Historical notes: Measure theory established early 20th\ncentury constellation French/German mathematicians, represented\nÉmile Borel, Henri Lebesgue, Johann Radon, etc. Generations \nRussian mathematicians Andrey Markov Andrey Kolmogorov made\nfundamental contributions mathematizing seemingly abstract concepts\nuncertainty randomness. names immortalized Borel\nset, Lebesgue integral, Radon measure, Markov chain,\nKolmogorov’s zero–one law many terminologies named \n.Fascinating questions probability attracted great economists.\nFrancis Edgeworth (1845–1926) wrote extensively probability \nstatistics. John Maynard Keynes (1883–1946) published Treatise \nProbability 1921 mixed probability philosophy, although\npiece work influential General Theory \nEmployment, Interest Money 1936 later revolutionized\neconomics.Today, technology collecting data processing data \nunbelievably cheaper 100 years ago. Unfortunately, cost \nlearning mathematics developing mathematics \nsignificantly lowered one century. small handful talents,\nlike , enjoy privilege luxury appreciate ideas \ngreat minds.reading: Doob (1996) summarized development \naxiomatic probability first half 20th century.Zhentao Shi. Sep 12, 2020.","code":""},{"path":"conditional-expectation.html","id":"conditional-expectation","chapter":"3 Conditional Expectation","heading":"3 Conditional Expectation","text":"Notation: note, \\(y\\) scale random variable, \n\\(x=\\left(x_{1},\\ldots,x_{K}\\right)'\\) \\(K\\times1\\) random vector.\nThroughout course, vector column vector, .e. one-column\nmatrix.Machine learning big basket contains regression models. \nmotivate conditional expectation model perspective \nprediction. view regression supervised learning. Supervised\nlearning uses function \\(x\\), say, \\(g\\left(x\\right)\\), predict \\(y\\).\n\\(x\\) perfectly predict \\(y\\); otherwise relationship \ndeterministic. prediction error \\(y-g\\left(x\\right)\\) depends \nchoice \\(g\\). numerous possible choices \\(g\\). one \nbest? Notice question concerned \nunderlying data generating process (DGP) joint distribution \n\\(\\left(y,x\\right)\\). want find general rule achieve accurate\nprediction \\(y\\) given \\(x\\), matter pair variables \ngenerated.answer question, need decide criterion compare\ndifferent \\(g\\). criterion called loss function\n\\(L\\left(y,g\\left(x\\right)\\right)\\). particularly convenient one \nquadratic loss, defined \n\\[L\\left(y,g\\left(x\\right)\\right)=\\left(y-g\\left(x\\right)\\right)^{2}.\\]\nSince data random, \\(L\\left(y,g\\left(x\\right)\\right)\\) also\nrandom. “Random” means uncertainty: sometimes happens, \nsometimes happens. get rid uncertainty, average \nloss function respect joint distribution \n\\(\\left(y,x\\right)\\) \n\\(R\\left(y,g\\left(x\\right)\\right)=E\\left[L\\left(y,g\\left(x\\right)\\right)\\right]\\),\ncalled risk. Risk deterministic quality. \nquadratic loss function, corresponding risk \n\\[R\\left(y,g\\left(x\\right)\\right)=E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right],\\]\ncalled mean squared error (MSE). MSE widely used\nrisk measure, although exist many alternative measures, \nexample mean absolute error (MAE)\n\\(E\\left[\\left|y-g\\left(x\\right)\\right|\\right]\\). popularity MSE\ncomes convenience analysis closed-form, MAE \nenjoy due nondifferentiability. similar choice\nutility functions economics. functional forms\nutility, example CRRA, CARA, . popular\nlead close-form solutions easy handle. Now\nquest narrowed : optimal choice \\(g\\) \nminimize MSE?\\[prop:CEF\\] conditional mean function\n(CEF)\n\\(m\\left(x\\right)=E\\left[y|x\\right]=\\int yf\\left(y|x\\right)\\mathrm{d}y\\)\nminimizes MSE.prove proposition, first discuss properties\nconditional mean function. Obviously\n\\[y=m\\left(x\\right)+\\left(y-m\\left(x\\right)\\right)=m\\left(x\\right)+\\epsilon,\\]\n\\(\\epsilon:=y-m\\left(x\\right)\\) called regression error.\nequation holds \\(\\left(y,x\\right)\\) following joint\ndistribution, long \\(E\\left[y|x\\right]\\) exists. error term\n\\(\\epsilon\\) satisfies properties:\\(E\\left[\\epsilon|x\\right]=E\\left[y-m\\left(x\\right)|x\\right]=E\\left[y|x\\right]-m(x)=0\\),\\(E\\left[\\epsilon|x\\right]=E\\left[y-m\\left(x\\right)|x\\right]=E\\left[y|x\\right]-m(x)=0\\),\\(E\\left[\\epsilon\\right]=E\\left[E\\left[\\epsilon|x\\right]\\right]=E\\left[0\\right]=0\\),\\(E\\left[\\epsilon\\right]=E\\left[E\\left[\\epsilon|x\\right]\\right]=E\\left[0\\right]=0\\),function \\(h\\left(x\\right)\\), \n\\[E\\left[h\\left(x\\right)\\epsilon\\right]=E\\left[E\\left[h\\left(x\\right)\\epsilon|x\\right]\\right]=E\\left[h(x)E\\left[\\epsilon|x\\right]\\right]=0.\\label{eq:uncorr}\\]function \\(h\\left(x\\right)\\), \n\\[E\\left[h\\left(x\\right)\\epsilon\\right]=E\\left[E\\left[h\\left(x\\right)\\epsilon|x\\right]\\right]=E\\left[h(x)E\\left[\\epsilon|x\\right]\\right]=0.\\label{eq:uncorr}\\]last property implies \\(\\epsilon\\) uncorrelated \nfunction \\(x\\). particular, \\(h\\) identity function\n\\(h\\left(x\\right)=x\\), \n\\(E\\left[x\\epsilon\\right]=\\mathrm{cov}\\left(x,\\epsilon\\right)=0\\).optimality CEF can confirmed “guess--verify.” \narbitrary \\(g\\left(x\\right)\\), MSE can decomposed three terms\n\\[\\begin{aligned}\n&  & E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right]\\\\\n& = & E\\left[\\left(y-m(x)+m(x)-g(x)\\right)^{2}\\right]\\\\\n& = & E\\left[\\left(y-m\\left(x\\right)\\right)^{2}\\right]+2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]+E\\left[\\left(m\\left(x\\right)-g\\left(x\\right)\\right)^{2}\\right].\\end{aligned}\\]\nfirst term irrelevant \\(g\\left(x\\right)\\). second term\n\\[\\begin{aligned}2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right] & =2E\\left[\\epsilon\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]=0\\end{aligned}\\]\ninvoking (\\[eq:uncorr\\]) \n\\(h\\left(x\\right)=m\\left(x\\right)-g\\left(x\\right)\\). second term \nirrelevant \\(g\\left(x\\right)\\). third term, obviously, \nminimized \\(g\\left(x\\right)=m\\left(x\\right)\\).perspective far deviates many econometric textbooks \nassume dependent variable \\(y\\) generated \n\\(g\\left(x\\right)+\\epsilon\\) unknown function\n\\(g\\left(\\cdot\\right)\\) error term \\(\\epsilon\\) \n\\(E\\left[\\epsilon|x\\right]=0\\). Instead, take predictive approach\nregardless DGP. observe \\(y\\) \\(x\\) solely\ninterested seeking function \\(g\\left(x\\right)\\) predict \\(y\\) \naccurately possible MSE criterion.","code":""},{"path":"conditional-expectation.html","id":"linear-projection","chapter":"3 Conditional Expectation","heading":"3.1 Linear Projection","text":"CEF \\(m(x)\\) function minimizes MSE. However,\n\\(m\\left(x\\right)=E\\left[y|x\\right]\\) complex function \\(x\\), \ndepends joint distribution \\(\\left(y,x\\right)\\), mostly\nunknown practice. Now let us make prediction task even simpler.\nminimize MSE within linear functions form \n\\(h\\left(x\\right)=h\\left(x;b\\right)=x'b\\) \\(b\\\\mathbb{R}^{K}\\)? \nminimization problem \n\\[\\min_{b\\\\mathbb{R}^{K}}E\\left[\\left(y-x'b\\right)^{2}\\right].\\label{eq:linear_MSE}\\]\nTake first-order condition MSE\n\\[\\frac{\\partial}{\\partial b}E\\left[\\left(y-x'b\\right)^{2}\\right]=E\\left[\\frac{\\partial}{\\partial b}\\left(y-x'b\\right)^{2}\\right]=-2E\\left[x\\left(y-x'b\\right)\\right],\\]\nfirst equality holds \n\\(E\\left[\\left(y-x'b\\right)^{2}\\right]<\\infty\\) expectation\npartial differentiation interchangeable, second equality\nhods chain rule linearity expectation. Set first\norder condition 0 solve\n\\[\\beta=\\arg\\min_{b\\\\mathbb{R}^{K}}E\\left[\\left(y-x'b\\right)^{2}\\right]\\]\nclosed-form\n\\[\\beta=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xy\\right]\\] \n\\(E\\left[xx'\\right]\\) invertible. Notice \\(b\\) arbitrary\n\\(K\\)-vector, \\(\\beta\\) optimizer. function \\(x'\\beta\\) \ncalled best linear projection (BLP) \\(y\\) \\(x\\), vector\n\\(\\beta\\) called linear projection coefficient.linear function restrictive one might thought. can\nused produce nonlinear (random variables) effect \nre-define \\(x\\). example, \n\\[y=x_{1}\\beta_{1}+x_{2}\\beta_{2}+x_{1}^{2}\\beta_{3}+e,\\] \n\\(\\frac{\\partial}{\\partial x_{1}}m\\left(x_{1},x_{2}\\right)=\\beta_{1}+2x_{1}\\beta_{3}\\),\nnonlinear \\(x_{1}\\), still linear parameter\n\\(\\beta=\\left(\\beta_{1},\\beta_{2},\\beta_{3}\\right)\\) define set \nnew regressors \n\\(\\left(\\tilde{x}_{1},\\tilde{x}_{2},\\tilde{x}_{3}\\right)=\\left(x_{1},x_{2},x_{1}^{2}\\right)\\).\\(\\left(y,x\\right)\\) jointly normal form \\[\\begin{pmatrix}y\\\\\nx\n\\end{pmatrix}\\sim\\mathrm{N}\\left(\\begin{pmatrix}\\mu_{y}\\\\\n\\mu_{x}\n\\end{pmatrix},\\begin{pmatrix}\\sigma_{y}^{2} & \\rho\\sigma_{y}\\sigma_{x}\\\\\n\\rho\\sigma_{y}\\sigma_{x} & \\sigma_{x}^{2}\n\\end{pmatrix}\\right)\\] \\(\\rho\\) correlation coefficient, \n\\[E\\left[y|x\\right]=\\mu_{y}+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\left(x-\\mu_{x}\\right)=\\left(\\mu_{y}-\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\mu_{x}\\right)+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}x,\\]\nliner function \\(x\\). example, CEF linear.Even though general \\(m\\left(x\\right)\\neq x'\\beta\\), linear form\n\\(x'\\beta\\) still useful approximating \\(m\\left(x\\right)\\). ,\n\\(\\beta=\\arg\\min\\limits _{b\\\\mathbb{R}^{K}}E\\left[\\left(m(x)-x'b\\right)^{2}\\right]\\).first-order condition gives\n\\(\\frac{\\partial}{\\partial b}E\\left[\\left(m(x)-x'b\\right)^{2}\\right]=-2E[x(m(x)-x'b)]=0\\).\nRearrange terms obtain \\(E[x\\cdot m(x)]=E[xx']b\\). \\(E[xx']\\)\ninvertible, solve\n\\[\\left(E\\left[xx'\\right]\\right){}^{-1}E[x\\cdot m(x)]=\\left(E\\left[xx'\\right]\\right){}^{-1}E[E[xy|x]]=\\left(E\\left[xx'\\right]\\right){}^{-1}E[xy]=\\beta.\\]\nThus \\(\\beta\\) also best linear approximation \\(m\\left(x\\right)\\)\nMSE.may rewrite linear regression model, linear projection\nmodel, \\[\\begin{array}[t]{c}\ny=x'\\beta+e\\\\\nE[xe]=0,\n\\end{array}\\] \\(e=y-x'\\beta\\) called linear projection\nerror, distinguished \\(\\epsilon=y-m(x).\\)Show () \\(E\\left[xe\\right]=0\\). (b) \\(x\\) contains constant, \n\\(E\\left[e\\right]=0\\).","code":""},{"path":"conditional-expectation.html","id":"omitted-variable-bias","chapter":"3 Conditional Expectation","heading":"3.1.1 Omitted Variable Bias","text":"write long regression \n\\[y=x_{1}'\\beta_{1}+x_{2}'\\beta_{2}+\\beta_{3}+e_{\\beta},\\] \nshort regression \\[y=x_{1}'\\gamma_{1}+\\gamma_{2}+e_{\\gamma},\\]\n\\(e_{\\beta}\\) \\(e_{\\gamma}\\) projection errors,\nrespectively. \\(\\beta_{1}\\) long regression parameter \ninterest, omitting \\(x_{2}\\) short regression render\nomitted variable bias (meaning \\(\\gamma_{1}\\neq\\beta_{1}\\)) unless\n\\(x_{1}\\) \\(x_{2}\\) uncorrelated.first demean variables two regressions, \nequivalent project effect constant. long\nregression becomes\n\\[\\tilde{y}=\\tilde{x}_{1}'\\beta_{1}+\\tilde{x}_{2}'\\beta_{2}+\\tilde{e}_{\\beta},\\]\nshort regression becomes\n\\[\\tilde{y}=\\tilde{x}_{1}'\\gamma_{1}+\\tilde{e}_{\\gamma},\\] tilde\ndenotes demeaned variable.Show \\(\\tilde{e}_{\\beta}=e_{\\beta}\\) \\(\\tilde{e}_{\\gamma}=e_{\\gamma}\\).demeaning, cross-moment equals covariance. short\nregression coefficient\n\\[\\begin{aligned}\\gamma_{1} & =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{y}\\right]\\\\\n& =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\left(\\tilde{x}_{1}'\\beta_{1}+\\tilde{x}_{2}'\\beta_{2}+\\tilde{e}_{\\beta}\\right)\\right]\\\\\n& =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2}\\\\\n& =\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2},\n\\end{aligned}\\] third line holds \n\\(E\\left[\\tilde{x}_{1}\\tilde{e}_{\\beta}\\right]=0\\). Therefore,\n\\(\\gamma_{1}=\\beta_{1}\\) \n\\(E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2}=0\\), demands\neither \\(E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]=0\\) \\(\\beta_{2}=0\\).Show \n\\(E\\left[\\left(y-x_{1}'\\beta_{1}-x_{2}'\\beta_{2}-\\beta_{3}\\right)^{2}\\right]\\leq E\\left[\\left(y-x_{1}'\\gamma_{1}-\\gamma_{2}\\right)^{2}\\right]\\).Obviously prefer run long regression attain \\(\\beta_{1}\\) \npossible, general model short regression \nachieves larger variance projection error. However, sometimes\n\\(x_{2}\\) unobservable long regression unavailable. \nexample omitted variable bias ubiquitous applied econometrics.\nIdeally like directly observe regressors reality\nhand. aware potential\nconsequence data ideal wished. \nshort regression available, cases able sign \nbias, meaning can argue whether \\(\\gamma_{1}\\) bigger \nsmaller \\(\\beta_{1}\\) based knowledge.","code":""},{"path":"conditional-expectation.html","id":"causality","chapter":"3 Conditional Expectation","heading":"3.2 Causality","text":"","code":""},{"path":"conditional-expectation.html","id":"structure-and-identification","chapter":"3 Conditional Expectation","heading":"3.2.1 Structure and Identification","text":"Unlike physical laws Einstein’s mass–energy equivalence\n\\(E=mc^{2}\\) Newton’s universal gravitation \\(F=Gm_{1}m_{2}/r^{2}\\),\neconomic phenomena can rarely summarized minimalistic\nstyle. using experiments verify physical laws, scientists often\nmanage come smart design signal--noise ratio \nhigh small disturbances kept negligible level. \ncontrary, economic laws fit laboratory experimentation.\nworse, subjects economic studies — human beings — \nheterogeneous many features hard control. People\ndistinctive cultural family backgrounds respond \nissue differently researchers can little homogenize . \nsignal--noise ratios economic laws often significantly lower\nphysical laws, mainly due lack laboratory\nsetting heterogeneous nature subjects.Educational return demand-supply system two classical topics\neconometrics. person’s incomes determined many random\nfactors academic career path impossible \nexhaustively observe control. observable prices quantities\noutcomes equilibrium demand supply affect .Generations thinkers debating definitions causality.\neconomics, accepted definition structural causality.\nStructural causality thought experiment. assumes \nDGP produces observational data. can use data recover\nDGP features DGP, learned causality \nimplications causality.key issue resolve looking realized sample \nidentification. say model DGP identified \npossible parameter model consideration generates\ndistinctive features observable data. model \n-identified one parameter model can generate\nexact features observable data. words, model\n-identified observable data trace back \nunique parameter model. correctly specified model \nprerequisite discussion identification. reality, \nmodels wrong. Thus talking identification, \nindulged imaginary world. thought experiment still\nunique distinguish true parameter data generating\nprocess, identification fails. determine true\nmodel matter large sample .","code":""},{"path":"conditional-expectation.html","id":"treatment-effect","chapter":"3 Conditional Expectation","heading":"3.2.2 Treatment Effect","text":"narrow framework relationship \\(y\\) \\(x\\).\nOne question particular interest treatment effect. treatment\neffect much \\(y\\) change change variable interest,\nsay \\(d\\), one unit keeping variables (including \nunobservable variables) . Latin phrase ceteris paribus\nmeans “keep things constant.”2020 covid-19 pandemic, Hong Kong’s unemployment rate rose \nhigh-level consumption collapsed. order boost economy,\nHong Kong residents qualified receiving 10,000 HKD cash\nallowance government. interested learning much\n10,000 HKD allowance increase people’s consumption. \nindividual, imagine two parallel worlds: one cash allowance\none without. difference consumption world \nallowance, denoted \\(Y\\left(1\\right)\\), world without \nallowance, denoted \\(Y\\left(0\\right)\\), treatment effect \nparticular person. thought experiment called potential\noutcome framework.However, reality one one scenario happens, echoes \nsaying ancient Greek philosopher Heraclitus (553 BC--475 BC) “\nstep river twice.” individual treatment effect\noperational (operational means can computed data \npopulation level), one one outcome realized.\nmany people available, can define average treatment effect\n(ATE) \n\\[ATE=E\\left[Y\\left(1\\right)-Y\\left(0\\right)\\right]=E\\left[Y\\left(1\\right)\\right]-E\\left[Y\\left(0\\right)\\right].\\]\nNotice \\(E\\left[Y\\left(1\\right)\\right]\\) \n\\(E\\left[Y\\left(0\\right)\\right]\\) still operational \nobserve companion variable\n\\[D=1\\left\\{ \\mbox{treatment received}\\right\\} .\\] \nindividual’s treatment status observable,\n\\(E\\left[Y\\left(1\\right)|D=1\\right]\\) \n\\(E\\left[Y\\left(0\\right)|D=0\\right]\\) operational data.two potential outcomes\n\\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\) independent \nassignment \\(D\\), \n\\(E\\left[Y\\left(1\\right)\\right]=E\\left[Y\\left(1\\right)|D=1\\right]\\) \n\\(E\\left[Y\\left(0\\right)\\right]=E\\left[Y\\left(0\\right)|D=0\\right]\\) \nATE can estimated data operational way \n\\[ATE=E\\left[Y\\left(1\\right)|D=1\\right]-E\\left[Y\\left(0\\right)|D=0\\right].\\]\nTherefore, evaluate ATE ideally like use lottery \nrandomly decide people receive treatment (treatment\ngroup, \\(D=1\\)) others (control group, \\(D=0\\)).control variables, can also define finer\ntreatment effect conditional \\(x\\):\n\\[ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|x\\right]-E\\left[Y\\left(0\\right)|x\\right].\\]\nATE average effect population individuals \nhypothetical give treatment, keeping factors \\(x\\)\nconstant. conditioning \\(x\\), treatment \\(D\\) independent \n\\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\), ATE becomes\noperational:\n\\[ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|D=1,x\\right]-E\\left[Y\\left(0\\right)|D=0,x\\right]\\]\nimportant condition\n\\(\\left(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\perp D\\right)|x\\) \ncalled conditional independence assumption (CIA).CIA plausible full independence. Consider example\n\\(Y\\left(1\\right)=x+u\\left(1\\right)\\), \\(Y\\left(0\\right)=x+u\\left(0\\right)\\)\n\\(D=1\\left\\{ x+u_{d}\\geq0\\right\\}\\). \n\\(\\left(\\left(u\\left(0\\right),u\\left(1\\right)\\right)\\perp u_{d}\\right)|x\\),\nCIA satisfied. Nevertheless\n\\(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\) \\(D\\) statistically\ndependent, since \\(x\\) involved random variables.","code":""},{"path":"conditional-expectation.html","id":"ate-and-cef","chapter":"3 Conditional Expectation","heading":"3.2.3 ATE and CEF","text":"previous section treatment \\(D\\) binary. Now consider \ncontinuous treatment \\(D\\). Suppose DGP, structural model, \n\\(Y=h\\left(D,x,u\\right)\\) \\(D\\) \\(x\\) observable \\(u\\) \nunobservable. natural define ATE continuous treatment\n(Hansen’s book Chapter 2.30 calls average causal effect) \n\\[ATE\\left(d,x\\right)=E\\left[\\lim_{\\Delta\\to0}\\frac{h\\left(d+\\Delta,x,u\\right)-h\\left(d,x,u\\right)}{\\Delta}\\right]=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right],\\]\ncontinuous differentiability \\(h\\left(d,x,u\\right)\\) \\(d\\)\nimplicitly assumed. Unlike binary treatment case, \\(d\\)\nexplicitly shows \\(ATE\\left(d,x\\right)\\) effect can vary\ndifferent values \\(d\\). ATE average effect \npopulation individuals hypothetical move \\(D\\) tiny bit around\n\\(d\\), keeping factors \\(x\\) constant.previous sections, focused CEF \\(m\\left(d,x\\right)\\),\n\\(d\\) added \\(x\\) additional variable interest. \nintend model underlying economic mechanism\n\\(h\\left(D,x,u\\right)\\), may complex. Can learn \n\\(ATE\\left(d,x\\right)\\) bears structural causal interpretation,\nmechanical \\(m\\left(d,x\\right)\\) merely cares best\nprediction? answer positive CIA: \\(\\left(u\\perp D\\right)|x\\).\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial d}m\\left(d,x\\right) & =\\frac{\\partial}{\\partial d}E\\left[y|d,x\\right]=\\frac{\\partial}{\\partial d}E\\left[h\\left(d,x,u\\right)|d,x\\right]=\\frac{\\partial}{\\partial d}\\int h\\left(d,x,u\\right)f\\left(u|d,x\\right)du\\\\\n& =\\int\\frac{\\partial}{\\partial d}\\left[h\\left(d,x,u\\right)f\\left(u|d,x\\right)\\right]du\\\\\n& =\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du+\\int h\\left(d,x,u\\right)\\left[\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)\\right]du,\\end{aligned}\\]\nsecond line implicitly assumes interchangeability \nintegral partial derivative. CIA,\n\\(\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)=0\\) second term\ndrops . Thus\n\\[\\frac{\\partial}{\\partial d}m\\left(d,x\\right)=\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]=ATE\\left(d,x\\right).\\]\nimportant result. says CIA holds, can learn \ncausal effect \\(d\\) \\(y\\) partial derivative CEF conditional\n\\(x\\). particular, assume linear CEF\n\\(m\\left(d,x\\right)=\\beta_{d}d+\\beta_{x}'x\\), causal effect \ncoefficient \\(\\beta_{d}\\).CIA key condition links CEF causal effect. CIA\ninnocuous assumption. applications, causal results \ncredible can convincing defend CIA.Let factories’ output Cobb-Douglas function\n\\(Y=AK^{\\alpha}L^{\\beta}\\), capital level \\(K\\) labor \\(L\\) \nwell output \\(Y\\) observable, “technology” \\(\\) \nunobservable. Take logarithm sides equation:\n\\[y=u+\\alpha k+\\beta l\\label{eq:causal}\\] \\(y=\\log Y\\), \\(u=\\log \\),\n\\(k=\\log K\\) \\(l=\\log L\\). Suppose \\(\\begin{pmatrix}u\\\\ k\\\\ l \\end{pmatrix}\\sim N\\left(\\begin{pmatrix}1\\\\ 1\\\\ 1 \\end{pmatrix},\\begin{pmatrix}1 & 0.5 & 0\\\\ 0.5 & 1 & 0\\\\ 0 & 0 & 1 \\end{pmatrix}\\right)\\) \\(\\alpha=\\beta=1/2\\) make true DGP. \\(u\\)\n\\(k\\) correlated, factories larger scale can afford\nrobots facilitate automation.partial derivative CEF use \\(k\\) treatment\nvariable fixed labor level \\(l\\)? (Hint: CEF linear\nfunction thanks joint normality.)partial derivative CEF use \\(k\\) treatment\nvariable fixed labor level \\(l\\)? (Hint: CEF linear\nfunction thanks joint normality.)coincide \\(\\alpha=1/2\\), coefficient causal\nmodel (\\[eq:causal\\])? (Hint: , CIA violated.)coincide \\(\\alpha=1/2\\), coefficient causal\nmodel (\\[eq:causal\\])? (Hint: , CIA violated.)Sometimes applied researchers assume brute force \n\\(y=m\\left(d,x\\right)+u\\) DGP \\(E\\left[u|d,x\\right]=0\\), \n\\(d\\) variable interest \\(x\\) vector control\nvariables. assumptions,\n\\[ATE\\left(d,x\\right)=E\\left[\\frac{\\partial}{\\partial d}\\left(m\\left(d,x\\right)+u\\right)|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d}+\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d},\\]\nsecond equality holds \n\\(\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=E\\left[\\frac{\\partial}{\\partial d}u|d,x\\right]\\).\nfirst glance, seems mean independence assumption\n\\(E\\left[u|d,x\\right]=0\\), weaker CIA, implies \nequivalence \\(ATE\\left(d,x\\right)\\) \n\\(\\partial m\\left(d,x\\right)/\\partial d\\) . However, slight\nweakening achieved strong assumption DGP\n\\(h\\left(d,x,u\\right)\\) follows additive separable form\n\\(m\\left(d,x\\right)+u\\). Without economic theory defend choice \nassumed DGP \\(y=m\\left(d,x\\right)+u\\), best \nreduced-form approach.structural approach models economic mechanism, guided \neconomic theory. reduced-form approach convenient can\ndocument stylized facts suitable economic theory immediately\navailable. constant debates pros cons two\napproaches; see Journal Economic Perspectives Vol. 24, . 2 Spring\n2010. macroeconomics, -called Phillips curve, attributed \n.W. Phillips negative correlation inflation \nunemployment, stylized fact learned reduced-form approach.\nLucas critique (Lucas 1976) exposed lack \nmicrofoundation advocated modeling deep parameters \ninvariant policy changes. latter structural approach.\nIronically, 40 years passed since Lucas critique,\nequations little microfoundation still dominate analytical\napparatus central bankers.","code":""},{"path":"conditional-expectation.html","id":"summary-1","chapter":"3 Conditional Expectation","heading":"3.3 Summary","text":"lecture, cover conditional mean function causality.\nfaced pair random variable \\(\\left(y,x\\right)\\)\ndrawn joint distribution, CEF best predictor. \ngo structural causality treatment \\(d\\) \ndependent variable \\(y\\), CIA can find equivalence \nATE partial derivative CEF. analyses conducted \npopulation. touched sample yet.Historical notes: Regressions conditional expectations \nconcepts statistics imported econometrics early\ntime. Researchers Cowles Commission (now Cowles Foundation \nResearch Economics) — Jacob Marschak (1898–1977), Tjalling\nKoopmans (1910–1985, Nobel Prize 1975), Trygve Haavelmo (1911–1999,\nNobel Prize 1989) colleagues — trailblazers \neconometric structural approach.potential outcome framework peculiar economics. \nwidely used fields biostatistics medical studies.\ninitiated Jerzy Neyman (1894–1981) extended Donald\nB. Rubin (1943– ), Professor Statistics Tsinghua University.reading: Lewbel (2019) offers comprehensive\nsummary identification econometrics. Accounting applied\nfield many claimed causal inference drawn simple regressions;\nencouraging hear Gow, Larcker, Reiss (2016) reflect causality \npractices.Zhentao Shi. Sep 17, 2020","code":""},{"path":"least-squares-linear-algebra.html","id":"least-squares-linear-algebra","chapter":"4 Least Squares: Linear Algebra","heading":"4 Least Squares: Linear Algebra","text":"最小二乘法是计量经济学中最基本的估计方法，简单而透明。完全弄懂它，为未来研究更加复杂的线性估计量铺平道路。即使是非线性的估计量，它们的行为在一个点线性展开后，和线性估计量大同小异。在这一讲中，我们将会学习一系列最小二层估计的线性代数性质。Ordinary least squares (OLS) basic estimation technique \neconometrics. simple transparent. Understanding thoroughly\npaves way study sophisticated linear estimators. Moreover,\nmany nonlinear estimators resemble behavior linear estimators \nneighborhood true value. lecture, learn series \nfacts linear algebra operation.最小二乘估计是通用的统计方法，它不仅运用于计量经济学中也广泛的应用于其他统计分支。\n但我们必须认清，最小二层古迹是一个纯粹的线性代数操作。它只能揭示相关性，并不能说明英国性。\n只有经济理论，才能提供有关因果的假说。数据只能用来检验假说或者量化效果。manipulate Leopold Kronecker’s famous saying “God made integers;\nelse works man”, say “Gauss made OLS; else \nworks applied researchers.” Popularity OLS goes far beyond \ndismal science. aware OLS pure statistical \nsupervised machine learning method reveals correlation instead \ncausality. Rather, economic theory hypothesizes causality data \ncollected test theory quantify effect.数学标记：\\(y_{}\\)是标量。\\(x_{}=\\left(x_{i1},\\ldots,x_{iK}\\right)'\\)是一个\\(K\\times1\\) 向量。\n\\(Y=\\left(y_{1},\\ldots,y_{n}\\right)'\\)是一个\\(n\\times1\\)向量。\n\\[\nX=\\left[\\begin{array}{c}\nx_{1}'\\\\\nx_{2}'\\\\\n\\vdots\\\\\nx_{n}'\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\nx_{11} & x_{12} & \\cdots & x_{1K}\\\\\nx_{21} & x_{22} & \\cdots & x_{2K}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{22} & \\cdots & x_{nK}\n\\end{array}\\right]\n\\]\n是\\(n\\times K\\)矩阵。\\(I_{n}\\)是\\(n\\times n\\)单位矩阵.","code":""},{"path":"least-squares-linear-algebra.html","id":"estimator","chapter":"4 Least Squares: Linear Algebra","heading":"4.1 Estimator","text":"我们已经在前一讲中学习了线性映射模型。线性映射函数是？\nlearned linear projection model, projection\ncoefficient \\(\\beta\\) regression\n\\[\n\\begin{aligned}y & =x'\\beta+e\\end{aligned}\n\\]\ncan written as可以被写成\n\\[\n\\beta=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xy\\right].\\label{eq:pop_OLS}\n\\]\n我们从\\(\\left(y,x\\right)\\)的联合分布中取出一对观测值，记作\\(\\left(y_{},x_{}\\right)\\)\n重复n次之后，有n个观测值。，\\(=1,\\ldots,n\\) repeated\nexperiments. possess sample\n我们就得到了一个样本\\(\\left(y_{},x_{}\\right)_{=1}^{n}\\).1.1*. \\(\\left(y_{},x_{}\\right)\\)\n样本到底是谁寄的？还是固定的呢？\n我们在观测之前，随机变量的值是不确定的。\n观测之后，他们的值就定下来了。\nrandom deterministic? \nmake observation, treated random variables whose\nrealized values uncertain. \\(\\left(y_{},x_{}\\right)\\) treated \nrandom talk statistical properties — statistical\nproperties fixed number meaningless. make \nobservation, become deterministic values vary anymore.在实际操作中，我们手中只有一些给定的数字。（当然，现在的大数据也可以将文本，照片声音和图像处理成为数据，这些数据在计算机当中用零和一来表示。）我们把这些数据扔给计算机，让计算机给出一个结果。在统计上。我们认为这些数字是从一个概率分布上得出的思想实验。思想实验是一个学术用语，说白了，它就是一个故事。在公理体系的概率论当中，这个故事，在数学上是自洽的。但是数学本身是一个套套逻辑，而不是科学。概率模型的科学价值在于它在多大程度上能够毕竟事实的真相以及他是不是能够帮我们预测一些真相？在这门课的研究当中，我们假设数据来自于某种机制。我们把这种机制当成真相。在线性回归当中。Xy的联合分布就是真相。而我们想要研究线性映射系数beta。这个线性映射函数是真相的蕴含（implication）。1.2. reality, hand fixed numbers (recently, words,\nphotos, audio clips, video clips, etc., can represented \ndigital formats 0 1) feed computational operation,\noperation return one numbers. statistical\ninterpretation numbers drawn probabilistic\nthought experiments. thought experiment* academic jargon \nstory plain language. axiomatic approach probability\ntheory, stories mathematical consistent coherent. \nmathematics tautological system, science. scientific value\nprobability model depends close truth \nimplications truth. course, suppose data \ngenerated mechanism, taken truth. \nlinear regression model example, joint distribution \n\\(\\left(y,x\\right)\\) truth, interested linear\nprojection coefficient \\(\\beta\\), implication truth \n(\\[eq:pop_OLS\\]).sample mean natural estimator population mean. Replace\npopulation mean \\(E\\left[\\cdot\\right]\\) \n(\\[eq:pop_OLS\\]) sample mean\n\\(\\frac{1}{n}\\sum_{=1}^{n}\\cdot\\), resulting estimator \n\\[\\begin{aligned}\n\\widehat{\\beta} & =\\left(\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\right)^{-1}\\frac{1}{n}\\sum_{=1}^{n}x_{}y_{}\\\\\n& =\\left(\\frac{X'X}{n}\\right)^{-1}\\frac{X'y}{n}=\\left(X'X\\right)^{-1}X'y\\end{aligned}\\]\n\\(X'X\\) invertible. one way motivate OLS estimator.Alternatively, can derive OLS estimator minimizing sum\nsquared residuals \\(\\sum_{=1}^{n}\\left(y_{}-x_{}'b\\right)^{2}\\), \nequivalently\n\\[\nQ\\left(b\\right)=\\frac{1}{2n}\\sum_{=1}^{n}\\left(y_{}-x_{}'b\\right)^{2}=\\frac{1}{2n}\\left(Y-Xb\\right)'\\left(Y-Xb\\right)=\\frac{1}{2n}\\left\\Vert Y-Xb\\right\\Vert ^{2},\n\\]\nfactor \\(\\frac{1}{2n}\\) nonrandom change \nminimizer, \\(\\left\\Vert \\cdot\\right\\Vert\\) Euclidean norm \nvector. Solve first-order condition\n\\[\n\\frac{\\partial}{\\partial b}Q\\left(b\\right)=\\left[\\begin{array}{c}\n\\partial Q\\left(b\\right)/\\partial b_{1}\\\\\n\\partial Q\\left(b\\right)/\\partial b_{2}\\\\\n\\vdots\\\\\n\\partial Q\\left(b\\right)/\\partial b_{K}\n\\end{array}\\right]=-\\frac{1}{n}X'\\left(Y-Xb\\right)=0.\n\\]\nnecessary\ncondition optimality gives exactly \n\\(\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y\\). Moreover, second-order\ncondition\n\\[\n\\frac{\\partial^{2}}{\\partial b\\partial b'}Q\\left(b\\right)=\\left[\\begin{array}{cccc}\n\\frac{\\partial^{2}}{\\partial b_{1}^{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{1}}Q\\left(b\\right)\\\\\n\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}^{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{2}}Q\\left(b\\right)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{K}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{K}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}^{2}}Q\\left(b\\right)\n\\end{array}\\right]=\\frac{1}{n}X'X\n\\]\nshows \\(Q\\left(b\\right)\\) \nconvex \\(b\\) due positive semi-definite matrix \\(X'X/n\\). (\nfunction \\(Q\\left(b\\right)\\) strictly convex \\(b\\) \\(X'X/n\\) \npositive definite.)1.3. derivation OLS presume \\(K\\) columns \\(X\\)\nlinearly independent, means \\(K\\times1\\) vector\n\\(b\\) \\(b\\neq0_{K}\\) \\(Xb=0_{n}\\). Linear independence \ncolumns implies \\(n\\geq K\\) invertibility \\(X'X/n\\). Linear\nindependence violated regressors perfectly collinear,\nexample use dummy variables indicate categorical\nvariables put categories regression. Modern\neconometrics software automatically detects reports perfect\ncollinearity. treacherous nearly collinear*, meaning \nminimal eigenvalue \\(X'X/n\\) close 0, though exactly\nequal 0. talk consequence near collinearity \nchapter asymptotic theory.definitions properties OLS estimator.Fitted value: \\(\\widehat{Y}=X\\widehat{\\beta}\\).Fitted value: \\(\\widehat{Y}=X\\widehat{\\beta}\\).Projection matrix: \\(P_{X}=X\\left(X'X\\right)^{-1}X\\); Residual maker\nmatrix: \\(M_{X}=I_{n}-P_{X}\\).Projection matrix: \\(P_{X}=X\\left(X'X\\right)^{-1}X\\); Residual maker\nmatrix: \\(M_{X}=I_{n}-P_{X}\\).\\(P_{X}X=X\\); \\(X'P_{X}=X'\\).\\(P_{X}X=X\\); \\(X'P_{X}=X'\\).\\(M_{X}X=0_{n\\times K}\\); \\(X'M_{X}=0_{K\\times n}\\).\\(M_{X}X=0_{n\\times K}\\); \\(X'M_{X}=0_{K\\times n}\\).\\(P_{X}M_{X}=M_{X}P_{X}=0_{n\\times n}\\).\\(P_{X}M_{X}=M_{X}P_{X}=0_{n\\times n}\\).\\(AA=\\), call idempotent matrix. \\(P_{X}\\) \n\\(M_{X}\\) idempotent. eigenvalues idempotent matrix must\neither 1 0.\\(AA=\\), call idempotent matrix. \\(P_{X}\\) \n\\(M_{X}\\) idempotent. eigenvalues idempotent matrix must\neither 1 0.\\(\\mathrm{rank}\\left(P_{X}\\right)=K\\), \n\\(\\mathrm{rank}\\left(M_{X}\\right)=n-K\\) (See Appendix \nchapter).\\(\\mathrm{rank}\\left(P_{X}\\right)=K\\), \n\\(\\mathrm{rank}\\left(M_{X}\\right)=n-K\\) (See Appendix \nchapter).Residual:\n\\(\\widehat{e}=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y=M_{X}\\left(X\\beta+e\\right)=M_{X}e\\).\nNotice \\(\\widehat{e}\\) \\(e\\) two different objects.Residual:\n\\(\\widehat{e}=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y=M_{X}\\left(X\\beta+e\\right)=M_{X}e\\).\nNotice \\(\\widehat{e}\\) \\(e\\) two different objects.\\(X'\\widehat{e}=X'M_{X}e=0_{K}\\).\\(X'\\widehat{e}=X'M_{X}e=0_{K}\\).\\(\\sum_{=1}^{n}\\widehat{e}_{}=0\\) \\(x_{}\\) contains constant.\n(\\(X'\\widehat{e}=\\left[\\begin{array}{cccc} 1 & 1 & \\cdots & 1\\\\ \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\\\\ \\cdots & \\cdots & \\ddots & \\vdots\\\\ \\heartsuit & \\heartsuit & \\cdots & \\heartsuit \\end{array}\\right]\\left[\\begin{array}{c} \\widehat{e}_{1}\\\\ \\widehat{e}_{2}\\\\ \\vdots\\\\ \\widehat{e}_{n} \\end{array}\\right]=\\left[\\begin{array}{c} 0\\\\ 0\\\\ \\vdots\\\\ 0 \\end{array}\\right]\\) , first row implies\n\\(\\sum_{=1}^{n}\\widehat{e}_{}=0\\). “\\(\\heartsuit\\)” indicates \nentries irrelevant purpose.)\\(\\sum_{=1}^{n}\\widehat{e}_{}=0\\) \\(x_{}\\) contains constant.(\\(X'\\widehat{e}=\\left[\\begin{array}{cccc} 1 & 1 & \\cdots & 1\\\\ \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\\\\ \\cdots & \\cdots & \\ddots & \\vdots\\\\ \\heartsuit & \\heartsuit & \\cdots & \\heartsuit \\end{array}\\right]\\left[\\begin{array}{c} \\widehat{e}_{1}\\\\ \\widehat{e}_{2}\\\\ \\vdots\\\\ \\widehat{e}_{n} \\end{array}\\right]=\\left[\\begin{array}{c} 0\\\\ 0\\\\ \\vdots\\\\ 0 \\end{array}\\right]\\) , first row implies\n\\(\\sum_{=1}^{n}\\widehat{e}_{}=0\\). “\\(\\heartsuit\\)” indicates \nentries irrelevant purpose.)operation OLS bears natural geometric interpretation. Notice\n\\(\\mathcal{X}=\\left\\{ Xb:b\\\\mathbb{R}^{K}\\right\\}\\) linear space\nspanned \\(K\\) columns \n\\(X=\\left[X_{\\cdot1},\\ldots,X_{\\cdot K}\\right]\\), \n\\(K\\)-dimension columns linearly independent. OLS estimator\nminimizer \n\\(\\min_{b\\\\mathbb{R}^{K}}\\left\\Vert Y-Xb\\right\\Vert\\) (Square \nEuclidean norm change minimizer \\(^{2}\\) \nmonotonic transformation \\(\\geq0\\)). words,\n\\(X\\widehat{\\beta}\\) point \\(\\mathcal{X}\\) \nclosest vector \\(Y\\) terms Euclidean norm.relationship \\(Y=X\\widehat{\\beta}+\\widehat{e}\\) decomposes \\(Y\\) \ntwo orthogonal vectors \\(X\\widehat{\\beta}\\) \\(\\widehat{e}\\) \n\\(\\left\\langle X\\widehat{\\beta},\\widehat{e}\\right\\rangle =\\widehat{\\beta}'X'\\widehat{e}=0_{K}^{\\prime}\\),\n\\(\\left\\langle \\cdot,\\cdot\\right\\rangle\\) inner product \ntwo vectors. Therefore \\(X\\widehat{\\beta}\\) projection \\(Y\\)\nonto \\(\\mathcal{X}\\), \\(\\widehat{e}\\) corresponding projection\nresiduals. Pythagorean theorem implies\n\\[\\left\\Vert Y\\right\\Vert ^{2}=\\Vert X\\widehat{\\beta}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\\]Example 4.1  ** 1.1**. simple simulated example demonstrate \nproperties OLS. Given\n\\(\\left(x_{1i},x_{2i},x_{3i},e_{}\\right)^{\\prime}\\sim N\\left(0_{4},I_{4}\\right)\\),\ndependent variable \\(y_{}\\) generated \n\\[\ny_{}=0.5+2\\cdot x_{1i}-1\\cdot x_{2i}+e_{}\n\\]researcher \nknow \\(x_{3i}\\) redundant, regresses \\(y_{}\\) \n\\(\\left(1,x_{1i},x_{2i},x_{3i}\\right)\\).estimated coefficient \\(\\widehat{\\beta}\\) ( 0.315, 1.955, -0.852,\n0.151). close true value, accurate due \nsmall sample size.","code":""},{"path":"least-squares-linear-algebra.html","id":"subvector","chapter":"4 Least Squares: Linear Algebra","heading":"4.2 Subvector","text":"Frish-Waugh-Lovell (FWL) theorem algebraic fact \nformula subvector OLS estimator. derive FWL theorem\nneed use inverse partitioned matrix. positive\ndefinite symmetric matrix \\(=\\begin{pmatrix}A_{11} & A_{12}\\\\ A_{12}' & A_{22} \\end{pmatrix}\\), inverse can written \n\\[^{-1}=\\begin{pmatrix}\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & -\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1}A_{12}A_{22}^{-1}\\\\\n-A_{22}^{-1}A_{12}'\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & \\left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\\right)^{-1}\n\\end{pmatrix}.\\] context OLS estimator, let\n\\(X=\\left(\\begin{array}{cc} X_{1} & X_{2}\\end{array}\\right)\\)\\[\\begin{aligned}\n\\begin{pmatrix}\\widehat{\\beta}_{1}\\\\\n\\widehat{\\beta}_{2}\n\\end{pmatrix} & =\\widehat{\\beta}=(X'X)^{-1}X'Y\\\\\n& =\\left(\\begin{pmatrix}X_{1}'\\\\\nX_{2}'\n\\end{pmatrix}\\begin{pmatrix}X_{1} & X_{2}\\end{pmatrix}\\right)^{-1}\\begin{pmatrix}X_{1}'Y\\\\\nX_{2}'Y\n\\end{pmatrix}\\\\\n& =\\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\\\\nX_{2}'X_{1} & X_{2}'X_{2}\n\\end{pmatrix}^{-1}\\begin{pmatrix}X_{1}'Y\\\\\nX_{2}'Y\n\\end{pmatrix}\\\\\n& =\\begin{pmatrix}\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1} & -\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}\\\\\n\\heartsuit & \\heartsuit\n\\end{pmatrix}\\begin{pmatrix}X_{1}'Y\\\\\nX_{2}'Y\n\\end{pmatrix}.\\end{aligned}\\]subvector \\[\\begin{aligned}\n\\widehat{\\beta}_{1} & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}X_{2}'Y\\\\\n& =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'P_{X_{2}}Y\\\\\n& =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}\\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\\right)\\\\\n& =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'M_{X_{2}}Y.\\end{aligned}\\]Notice \\(\\widehat{\\beta}_{1}\\) can obtained following:Regress \\(Y\\) \\(X_{2}\\), obtain residual \\(\\tilde{Y}\\);Regress \\(Y\\) \\(X_{2}\\), obtain residual \\(\\tilde{Y}\\);Regress \\(X_{1}\\) \\(X_{2}\\), obtain residual \\(\\tilde{X}_{1}\\);Regress \\(X_{1}\\) \\(X_{2}\\), obtain residual \\(\\tilde{X}_{1}\\);Regress \\(\\tilde{Y}\\) \\(\\tilde{X}_{1}\\), obtain OLS estimates\n\\(\\widehat{\\beta}_{1}\\).Regress \\(\\tilde{Y}\\) \\(\\tilde{X}_{1}\\), obtain OLS estimates\n\\(\\widehat{\\beta}_{1}\\).Similar derivation can also carried population linear\nprojection. See Hansen (2020) \\[E\\] Chapter 2.22-23.","code":""},{"path":"least-squares-linear-algebra.html","id":"goodness-of-fit","chapter":"4 Least Squares: Linear Algebra","heading":"4.3 Goodness of Fit","text":"Consider regression intercept\n\\(Y=X_{1}\\beta_{1}+\\beta_{2}+e.\\) OLS estimator gives\n\\[Y=\\widehat{Y}+\\widehat{e}=\\left(X_{1}\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}\\right)+\\widehat{e}.\\label{eq:decomp_1}\\]\nApplying FWL theorem \\(X_{2}=\\iota\\), \\(\\iota\\) (Greek\nletter, iota) \\(n\\times1\\) vector 1’s. \n\\(M_{X_{2}}=M_{\\iota}=I_{n}-\\frac{1}{n}\\iota\\iota'\\). Notice \\(M_{\\iota}\\)\ndemeaner \\(M_{\\iota}z=z-\\bar{z}\\). subtract vector\nmean \\(\\bar{z}=\\frac{1}{n}\\sum_{=1}^{n}z_{}\\) original vector\n\\(z\\). three-step procedure becomesRegress \\(Y\\) \\(\\iota\\), residual \\(M_{\\iota}Y\\);Regress \\(Y\\) \\(\\iota\\), residual \\(M_{\\iota}Y\\);Regress \\(X_{1}\\) \\(\\iota\\), residual \\(M_{\\iota}X_{1}\\);Regress \\(X_{1}\\) \\(\\iota\\), residual \\(M_{\\iota}X_{1}\\);Regress \\(M_{\\iota}Y\\) \\(M_{\\iota}X_{1}\\), OLS estimates \nexactly \\(\\widehat{\\beta}_{1}\\) \n(\\[eq:decomp_1\\]).Regress \\(M_{\\iota}Y\\) \\(M_{\\iota}X_{1}\\), OLS estimates \nexactly \\(\\widehat{\\beta}_{1}\\) \n(\\[eq:decomp_1\\]).last step gives decomposition\n\\[M_{\\iota}Y=M_{\\iota}X_{1}\\widehat{\\beta}_{1}+\\tilde{e},\\label{eq:decomp_2}\\]\nPythagorean theorem implies\n\\[\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}=\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\\]** 1.1**. Show \\(\\widehat{e}\\) \n(\\[eq:decomp_1\\]) exactly \\(\\tilde{e}\\) \n(\\[eq:decomp_2\\]).R-squared popular measure goodness--fit linear\nregression. (-sample) R-squared\n\\[R^{2}=\\frac{\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}=1-\\frac{\\left\\Vert \\tilde{e}\\right\\Vert ^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}.\\]\nwell defined constant included regressors.** 1.2**. Show\n\\[R^{2}=\\frac{\\widehat{Y}'M_{\\iota}\\widehat{Y}}{Y'M_{\\iota}Y}=\\frac{\\sum_{=1}^{n}\\left(\\widehat{y_{}}-\\overline{y}\\right)^{2}}{\\sum_{=1}^{n}\\left(y_{}-\\overline{y}\\right)^{2}}\\]\ndecomposition\n(\\[eq:decomp_1\\]). words, ratio \nsample variance \\(\\widehat{Y}\\) sample variance \\(Y\\).magnitude R-squared varies different contexts. macro models\nlagged dependent variables, unusually observe\nR-squared larger 90%. cross sectional regressions often\n20%.** 1.3**. Consider short regression “regress \\(y_{}\\) \\(x_{1i}\\)” \nlong regression “regress \\(y_{}\\) \\(\\left(x_{1i},x_{2i}\\right)\\)”.\nGiven dataset \\(\\left(Y,X_{1},X_{2}\\right)\\), show \nR-squared short regression larger long\nregression. words, can always (weakly) increase \\(R^{2}\\) \nadding regressors.Conventionally consider regressions number regressors\n\\(K\\) much smaller sample size \\(n\\). era big data, can\nhappen potential regressors sample size.** 1.4**. Show \\(R^{2}=1\\) \\(K\\geq n\\). (\\(K>n\\), matrix \\(X'X\\)\nmust rank deficient. can generalize definition OLS fitting \nvector minimizes \\(\\left\\Vert Y-Xb\\right\\Vert ^{2}\\) though \nminimizer unique.new dataset \\(\\left(Y^{\\mathrm{new}},X^{\\mathrm{new}}\\right)\\), \n--sample (OOS) R-squared \n\\[OOS\\ R^{2}=\\frac{\\widehat{\\beta}^{\\prime}X^{\\mathrm{new}\\prime}M_{\\iota}X^{\\mathrm{new}}\\widehat{\\beta}}{Y^{\\mathrm{new}\\prime}M_{\\iota}Y^{\\mathrm{new}}}.\\]\nOOS R-squred measures goodness fit new dataset given \ncoefficient estimated original data. financial market\nshorter-term predictive models, person may easily become rich \ncan systematically achieve 2% OOS R-squared.","code":"##\n## Call:\n## lm(formula = Y ~ X)\n##\n## Residuals:\n## ALL 5 residuals are 0: no residual degrees of freedom!\n##\n## Coefficients: (2 not defined because of singularities)\n##             Estimate Std. Error t value Pr(>|t|)\n## (Intercept)  -0.2229         NA      NA       NA\n## X1           -0.6422         NA      NA       NA\n## X2            0.1170         NA      NA       NA\n## X3            1.1844         NA      NA       NA\n## X4            0.5883         NA      NA       NA\n## X5                NA         NA      NA       NA\n## X6                NA         NA      NA       NA\n##\n## Residual standard error: NaN on 0 degrees of freedom\n## Multiple R-squared:      1,  Adjusted R-squared:    NaN\n## F-statistic:   NaN on 4 and 0 DF,  p-value: NA"},{"path":"least-squares-linear-algebra.html","id":"summary-2","chapter":"4 Least Squares: Linear Algebra","heading":"4.4 Summary","text":"linear algebraic properties holds finite sample matter \ndata taken fixed numbers random variables. Gauss Markov\ntheorem holds two crucial assumptions: linear CEF \nhomoskedasticity.高斯说，他在1795年就想出了最小二乘法的操作。他用三个点来预测了。矮行星位置。高斯没有在1809年之前把文章发表出来。而勒让德发表了同样的方法。今天，人们通常将最小二乘法归功于高斯。因为大家觉得像高斯这样的数学巨人没有必要。撒一个谎，来偷取勒让德的发现。Historical notes: Carl Friedrich Gauss (1777–1855) claimed \ncome operation OLS 1795. three data points\nhand, Gauss successfully applied method predict location\ndwarf planet Ceres 1801. Gauss publish work\nOLS 1809, Adrien-Marie Legendre (1752–1833) presented \nmethod 1805. Today people tend attribute OLS Gauss, assuming\ngiant like Gauss need tell lie steal Legendre’s\ndiscovery.","code":""},{"path":"least-squares-finite-sample-theory.html","id":"least-squares-finite-sample-theory","chapter":"5 Least Squares: Finite Sample Theory","heading":"5 Least Squares: Finite Sample Theory","text":"continue properties OLS. show OLS coincides \nmaximum likelihood estimator error term follows normal\ndistribution. derive finite-sample exact distribution can\nused statistical inference. Gauss-Markov theorem justifies\noptimality OLS classical assumptions.Suppose data generated parametric model. Statistical\nestimation looks unknown parameter observed data. \nprinciple ideology proper way estimation. \nhistory statistics, principles widely accepted. Among\nMaximum Likelihood important fundamental. \nmaximum likelihood principle entails unknown parameter \nfound maximizer log-likelihood function.","code":""},{"path":"least-squares-finite-sample-theory.html","id":"maximum-likelihood","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.1 Maximum Likelihood","text":"chapter, first give introduction maximum likelihood\nestimation. Consider random sample \n\\(Z=\\left(z_{1},z_{2},\\ldots,z_{n}\\right)\\) drawn parametric\ndistribution density \\(f_{z}\\left(z_{};\\theta\\right)\\), \n\\(z_{}\\) either scalar random variable random vector. \nparametric distribution completely characterized \nfinite-dimensional parameter \\(\\theta\\). know \\(\\theta\\) belongs \nparameter space \\(\\Theta\\). use data estimate \\(\\theta\\).log-likelihood observing entire sample \\(Z\\) \n\\[L_{n}\\left(\\theta;Z\\right):=\\log\\left(\\prod_{=1}^{n}f_{z}\\left(z_{};\\theta\\right)\\right)=\\sum_{=1}^{n}\\log f_{z}\\left(z_{};\\theta\\right).\\label{eq:raw_likelihood}\\]\nreality sample \\(Z\\) given \\(\\theta\\\\Theta\\) can\nevaluate \\(L_{n}\\left(\\theta;Z\\right)\\). maximum likelihood estimator\n\n\\[\\widehat{\\theta}_{MLE}:=\\arg\\max_{\\theta\\\\Theta}L_{n}\\left(\\theta;Z\\right).\\]\nmaximizing log-likelihood function desirable? intuitive\nexplanation \\(\\widehat{\\theta}_{MLE}\\) makes observing \\(Z\\) \n“likely” entire parametric space.formal justification requires explicitly defined distance.\nSuppose true parameter value generates data \n\\(\\theta_{0}\\), true distribution \n\\(f_{z}\\left(z_{};\\theta_{0}\\right)\\). generic point\n\\(\\theta\\\\Theta\\) produces \\(f_{z}\\left(z_{};\\theta\\right)\\). measure\ndifference, introduce Kullback-Leibler divergence, \nKullback-Leibler distance, defined logarithms expected\nlog-likelihood ratio \\[\\begin{aligned}\nD_{f}\\left(\\theta_{0}\\Vert\\theta\\right) & =D\\left(f_{z}\\left(z_{};\\theta_{0}\\right)\\Vert f_{z}\\left(z_{};\\theta\\right)\\right):=E_{\\theta_{0}}\\left[\\log\\frac{f_{z}\\left(z_{};\\theta_{0}\\right)}{f_{z}\\left(z_{};\\theta\\right)}\\right]\\\\\n& =E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{};\\theta_{0}\\right)\\right]-E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{};\\theta\\right)\\right].\\end{aligned}\\]\ncall “distance” non-negative, although \nsymmetric \n\\(D_{f}\\left(\\theta_{1}\\Vert\\theta_{2}\\right)\\neq D_{f}\\left(\\theta_{2}\\Vert\\theta_{1}\\right)\\)\nsatisfy triangle inequality. see\n\\(D_{f}\\left(\\theta_{0}\\Vert\\theta\\right)\\) non-negative, notice \n\\(-\\log\\left(\\cdot\\right)\\) strictly convex Jensen’s\ninequality \\[\\begin{aligned}\nE_{\\theta_{0}}\\left[\\log\\frac{f_{z}\\left(z_{};\\theta_{0}\\right)}{f_{z}\\left(z_{};\\theta\\right)}\\right] & =E_{\\theta_{0}}\\left[-\\log\\frac{f_{z}\\left(z_{};\\theta\\right)}{f_{z}\\left(z_{};\\theta_{0}\\right)}\\right]\\geq-\\log\\left(E_{\\theta_{0}}\\left[\\frac{f_{z}\\left(z_{};\\theta\\right)}{f_{z}\\left(z_{};\\theta_{0}\\right)}\\right]\\right)\\\\\n& =-\\log\\left(\\int\\frac{f_{z}\\left(z_{};\\theta\\right)}{f_{z}\\left(z_{};\\theta_{0}\\right)}f_{z}\\left(z_{};\\theta_{0}\\right)dz_{}\\right)=-\\log\\left(\\int f_{z}\\left(z_{};\\theta\\right)dz_{}\\right)\\\\\n& =-\\log1=0,\\end{aligned}\\] \n\\(\\int f_{z}\\left(z_{};\\theta\\right)dz_{}=1\\) pdf. equality\nholds \n\\(f_{z}\\left(z_{};\\theta\\right)=f_{z}\\left(z_{};\\theta_{0}\\right)\\)\nalmost everywhere. Furthermore, one--one mapping \n\\(\\theta\\) \\(f_{z}\\left(z_{};\\theta\\right)\\) \\(\\Theta\\)\n(identification), \n\\(\\theta_{0}=\\arg\\min_{\\theta\\\\Theta}D_{f}\\left(\\theta_{0}\\Vert\\theta\\right)\\)\nunique solution.information theory,\n\\(-E_{\\theta_{0}}\\left[\\log f_{z}\\left(z_{};\\theta_{0}\\right)\\right]\\) \nentropy continuous distribution \n\\(f_{z}\\left(z_{};\\theta_{0}\\right)\\). Entropy measures uncertainty\nrandom variable; larger value, chaotic \nrandom variable. Kullback-Leibler distance relative entropy\ndistribution \\(f_{z}\\left(z_{};\\theta_{0}\\right)\\) \n\\(f_{z}\\left(z_{};\\theta\\right)\\). measures inefficiency \nassuming distribution \\(f_{z}\\left(z_{};\\theta\\right)\\) \ntrue distribution indeed \\(f_{z}\\left(z_{};\\theta_{0}\\right)\\).\n(Cover Thomas 2006, 19)Consider Gaussian location model \\(z_{}\\sim N\\left(\\mu,1\\right)\\),\n\\(\\mu\\) unknown parameter estimated. likelihood \nobserving \\(z_{}\\) \n\\(f_{z}\\left(z_{};\\mu\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{}-\\mu\\right)^{2}\\right)\\).\nlikelihood observing sample \\(Z\\) \n\\[f_{Z}\\left(Z;\\mu\\right)=\\prod_{=1}^{n}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{}-\\mu\\right)^{2}\\right)\\]\nlog-likelihood \n\\[L_{n}\\left(\\mu;Z\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}\\sum_{=1}^{n}\\left(z_{}-\\mu\\right)^{2}.\\]\n(averaged) log-likelihood function \\(n\\) observations \n\\[\\begin{aligned}\n\\ell_{n}\\left(\\mu\\right) & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2n}\\sum_{=1}^{n}\\left(z_{}-\\mu\\right)^{2}.\\end{aligned}\\]\nwork averaged log-likelihood \\(\\ell_{n}\\), instead \n(raw) log-likelihood \\(L_{n}\\), make directly comparable \nexpected log density \\[\\begin{aligned}\nE_{\\mu_{0}}\\left[\\log f_{z}\\left(z;\\mu\\right)\\right] & =E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\\\\n& =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{}-\\mu\\right)^{2}\\right]\\\\\n& =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(\\left(z_{}-\\mu_{0}\\right)+\\left(\\mu_{0}-\\mu\\right)\\right)^{2}\\right]\\\\\n& =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{}-\\mu_{0}\\right)^{2}\\right]-E_{\\mu_{0}}\\left[z_{}-\\mu_{0}\\right]\\left(\\mu_{0}-\\mu\\right)-\\frac{1}{2}\\left(\\mu_{0}-\\mu\\right)^{2}\\\\\n& =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}-\\frac{1}{2}\\left(\\mu-\\mu_{0}\\right)^{2}.\\end{aligned}\\]\nfirst equality holds random sampling. Obviously,\n\\(\\ell_{n}\\left(\\mu\\right)\\) maximized \n\\(\\bar{z}=\\frac{1}{n}\\sum_{=1}^{n}z_{}\\) \n\\(E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\) maximized \n\\(\\mu=\\mu_{0}\\). Kullback-Leibler divergence example \n\\[D\\left(\\mu_{0}\\Vert\\mu\\right)=E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu_{0}\\right)\\right]-E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]=\\frac{1}{2}\\left(\\mu-\\mu_{0}\\right)^{2},\\]\n\n\\(-E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu_{0}\\right)\\right]=\\frac{1}{2}\\left(\\log\\left(2\\pi\\right)+1\\right)\\)\nentropy normal distribution unit variance.use following code demonstrate population log-likelihood\n\\(E\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\) \\(\\mu_{0}=2\\) (solid line)\n3 sample realizations \\(n=4\\) (dashed lines).**knitr** part","code":""},{"path":"least-squares-finite-sample-theory.html","id":"likelihood-estimation-for-regression","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.2 Likelihood Estimation for Regression","text":"Notation: \\(y_{}\\) scalar, \n\\(x_{}=\\left(x_{i1},\\ldots,x_{iK}\\right)'\\) \\(K\\times1\\) vector. \\(Y\\)\n\\(n\\times1\\) vector, \\(X\\) \\(n\\times K\\) matrix.chapter employ classical statistical framework \nrestrictive distributional assumption\n\\[y_{}|x_{}\\sim N\\left(x_{}'\\beta,\\gamma\\right),\\label{eq:normal_yx}\\]\n\\(\\gamma=\\sigma^{2}\\) ease differentiation. assumption\nequivalent \n\\(e_{}|x_{}=\\left(y_{}-x_{}'\\beta\\right)|x_{}\\sim N\\left(0,\\gamma\\right)\\).\ndistribution \\(e_{}\\) invariant \\(x_{}\\), error\nterm \\(e_{}\\sim N\\left(0,\\gamma\\right)\\) statistically independent\n\\(x_{}\\). strong assumption.likelihood observing pair \\(\\left(y_{},x_{}\\right)\\) \n\\[\\begin{aligned}\nf_{yx}\\left(y_{},x_{}\\right) & =f_{y|x}\\left(y_{}|x_{}\\right)f_{x}\\left(x\\right)\\\\\n& =\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{}-x_{}'\\beta\\right)^{2}\\right)\\times f_{x}\\left(x\\right),\\end{aligned}\\]\n\\(f_{yx}\\) joint pdf, \\(f_{y|x}\\) conditional pdf \n\\(f_{x}\\) marginal pdf \\(x\\), second equality holds \n(\\[eq:normal\\_yx\\]). likelihood random sample\n\\(\\left(y_{},x_{}\\right)_{=1}^{n}\\) \\[\\begin{aligned}\n\\prod_{=1}^{n}f_{yx}\\left(y_{},x_{}\\right) & =\\prod_{=1}^{n}f_{y|x}\\left(y_{}|x_{}\\right)f_{x}\\left(x\\right)\\\\\n& =\\prod_{=1}^{n}f_{y|x}\\left(y_{}|x_{}\\right)\\times\\prod_{=1}^{n}f_{x}\\left(x\\right)\\\\\n& =\\prod_{=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{}-x_{}'\\beta\\right)^{2}\\right)\\times\\prod_{=1}^{n}f_{x}\\left(x\\right).\\end{aligned}\\]\nparameters interest \\(\\left(\\beta,\\gamma\\right)\\) irrelevant \nsecond term \\(\\prod_{=1}^{n}f_{x}\\left(x\\right)\\) appear\nconditional likelihood\n\\[\\prod_{=1}^{n}f_{y|x}\\left(y_{}|x_{}\\right)=\\prod_{=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{}-x_{}'\\beta\\right)^{2}\\right).\\]\nfocus conditional likelihood. facilitate derivation, \nwork (averaged) conditional log-likelihood function\n\\[\\ell_{n}\\left(\\beta,\\gamma\\right)=-\\frac{1}{2}\\log2\\pi-\\frac{1}{2}\\log\\gamma-\\frac{1}{2n\\gamma}\\sum_{=1}^{n}\\left(y_{}-x_{}'\\beta\\right)^{2},\\]\n\\(\\log\\left(\\cdot\\right)\\) monotonic transformation \nchange maximizer. maximum likelihood estimator\n\\(\\widehat{\\beta}_{MLE}\\) can found using FOC: \\[\\begin{aligned}\n\\frac{\\partial}{\\partial\\beta}\\ell_{n}\\left(\\beta,\\gamma\\right) & =\\frac{1}{n\\gamma}\\sum_{=1}^{n}x_{}\\left(y_{}-x_{}'\\beta\\right)=0\\\\\n\\frac{\\partial}{\\partial\\gamma}\\ell_{n}\\left(\\beta,\\gamma\\right) & =-\\frac{1}{2\\gamma}+\\frac{1}{2n\\gamma^{2}}\\sum_{=1}^{n}\\left(y_{}-x_{}'\\beta\\right)^{2}=0.\\end{aligned}\\]\nRearranging equations matrix form: \\[\\begin{aligned}\nX'X\\beta & =X'Y\\\\\n\\gamma & =\\frac{1}{n}\\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right).\\end{aligned}\\]\nsolve \\[\\begin{aligned}\n\\widehat{\\beta}_{MLE} & =(X'X)^{-1}X'Y\\\\\n\\widehat{\\gamma}_{\\mathrm{MLE}} & =\\frac{1}{n}\\left(Y-X\\widehat{\\beta}_{MLE}\\right)'\\left(Y-X\\widehat{\\beta}_{MLE}\\right)=\\widehat{e}'\\widehat{e}/n\\end{aligned}\\]\n\\(X'X\\) invertible. MLE slope coefficient\n\\(\\widehat{\\beta}_{MLE}\\) coincides OLS estimator, \n\\(\\widehat{e}\\) exactly OLS residual.","code":""},{"path":"least-squares-finite-sample-theory.html","id":"finite-sample-distribution","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.3 Finite Sample Distribution","text":"can show finite-sample exact distribution \\(\\widehat{\\beta}\\)\nassuming error term follows Gaussian distribution. Finite sample\ndistribution means distribution holds \\(n\\); \ncontrast asymptotic distribution, large sample\napproximation finite sample distribution. first review \nproperties generic jointly normal random vector.\\[fact31\\] Let\n\\(z\\sim N\\left(\\mu,\\Omega\\right)\\) \\(l\\times1\\) random vector \npositive definite variance-covariance matrix \\(\\Omega\\). Let \\(\\) \n\\(m\\times l\\) non-random matrix \\(m\\leq l\\). \n\\(Az\\sim N\\left(\\mu,\\Omega '\\right)\\).\\[fact32\\]\\(z\\sim N\\left(0,1\\right)\\),\n\\(w\\sim\\chi^{2}\\left(d\\right)\\) \\(z\\) \\(w\\) independent. \n\\(\\frac{z}{\\sqrt{w/d}}\\sim t\\left(d\\right)\\).OLS estimator\n\\[\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'Y=\\left(X'X\\right)^{-1}X'\\left(X'\\beta+e\\right)=\\beta+\\left(X'X\\right)^{-1}X'e,\\]\nconditional distribution can written \\[\\begin{aligned}\n\\widehat{\\beta}|X & =\\beta+\\left(X'X\\right)^{-1}X'e|X\\\\\n& \\sim\\beta+\\left(X'X\\right)^{-1}X'\\cdot N\\left(0_{n},\\gamma I_{n}\\right)\\\\\n& \\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}X'X\\left(X'X\\right)^{-1}\\right)\\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}\\right)\\end{aligned}\\]\nFact \\[fact31\\].\n\\(k\\)-th element vector coefficient\n\\[\\widehat{\\beta}_{k}|X=\\eta_{k}'\\widehat{\\beta}|X\\sim N\\left(\\beta_{k},\\gamma\\eta_{k}'\\left(X'X\\right)^{-1}\\eta_{k}\\right)\\sim N\\left(\\beta_{k},\\gamma\\left[\\left(X'X\\right)^{-1}\\right]_{kk}\\right),\\]\n\\(\\eta_{k}=\\left(1\\left\\{ l=k\\right\\} \\right)_{l=1,\\ldots,K}\\) \nselector \\(k\\)-th element.reality, \\(\\sigma^{2}\\) unknown parameter, \n\\[s^{2}=\\widehat{e}'\\widehat{e}/\\left(n-K\\right)=e'M_{X}e/\\left(n-K\\right)\\]\nunbiased estimator \\(\\gamma\\). (\\[\\begin{aligned}\nE\\left[s^{2}|X\\right] & =\\frac{1}{n-K}E\\left[e'M_{X}e|X\\right]=\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[e'M_{X}e|X\\right]\\right)\\\\\n& =\\frac{1}{n-K}\\mathrm{trace}\\left(E\\left[M_{X}ee'|X\\right]\\right)=\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}E\\left[ee'|X\\right]\\right)\\\\\n& =\\frac{1}{n-K}\\mathrm{trace}\\left(M_{X}\\gamma I_{n}\\right)=\\frac{\\gamma}{n-K}\\mathrm{trace}\\left(M_{X}\\right)=\\gamma\\end{aligned}\\]\nuse property trace\n\\(\\mathrm{trace}\\left(AB\\right)=\\mathrm{trace}\\left(BA\\right)\\).)null hypothesis \\(H_{0}:\\beta_{k}=\\beta_{k}^{*}\\), \n\\(\\beta_{k}^{*}\\) hypothesized value want test. can\nconstruct \\(t\\)-statistic\n\\[T_{k}=\\frac{\\widehat{\\beta}_{k}-\\beta_{k}^{*}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}},\\]\ninfeasible sense can directly computed\ndata unknown object statistic. \nhypothesis true, \\(\\beta_{k}=\\beta_{k}^{*}\\) thus\n\\[\\begin{aligned}\nT_{k} & =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}\\nonumber \\\\\n& =\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}\\cdot\\frac{\\sqrt{\\sigma^{2}}}{\\sqrt{s^{2}}}\\nonumber \\\\\n& =\\frac{\\left(\\widehat{\\beta}_{k}-\\beta_{0,k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}{\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}},\\label{eq:t-stat}\\end{aligned}\\]\nintroduce population quantity \\(\\sigma^{2}\\) second\nequality help derive distribution numerator \ndenominator last expression. numerator\n\\[\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}\\sim N\\left(0,1\\right),\\]\ndenominator\n\\(\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}\\) follows\n\\(\\sqrt{\\frac{1}{n-K}\\chi^{2}\\left(n-K\\right)}\\). Moreover, \n\\[\\begin{aligned}\n\\begin{bmatrix}\\widehat{\\beta}-\\beta\\\\\n\\widehat{e}\n\\end{bmatrix} & =\\begin{bmatrix}\\left(X'X\\right)^{-1}X'e\\\\\nM_{X}e\n\\end{bmatrix}=\\begin{bmatrix}\\left(X'X\\right)^{-1}X'\\\\\nM_{X}\n\\end{bmatrix}e\\\\\n& \\sim\\begin{bmatrix}\\left(X'X\\right)^{-1}X'\\\\\nM_{X}\n\\end{bmatrix}\\cdot N\\left(0,\\gamma I_{n}\\right)\\sim N\\left(0,\\gamma\\begin{bmatrix}\\left(X'X\\right)^{-1} & 0\\\\\n0 & M_{X}\n\\end{bmatrix}\\right)\\end{aligned}\\] jointly normal zero\n-diagonal blocks, \\(\\left(\\widehat{\\beta}-\\beta\\right)\\) \n\\(\\widehat{e}\\) statistically independent. (claim true,\nalthough covariance matrix \\(\\widehat{e}\\) singular.) Given\n\\(X\\) viewed non-random, numerator denominator\n(\\[eq:t-stat\\]) statistically independent well \nfunction since former function \n\\(\\left(\\widehat{\\beta}-\\beta\\right)\\) latter function \n\\(\\widehat{e}\\). (Alternatively, statistically independent can \nverified Basu’s theorem, See Appendix\n\\[subsec:Basu\\'s-Theorem\\].) result, conclude\n\\(T_{k}\\sim t\\left(n-K\\right)\\) Fact\n\\[fact32\\]. \nfinite sample distribution allows us conduct statistical inference.","code":""},{"path":"least-squares-finite-sample-theory.html","id":"mean-and-variancemean-and-variance","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.4 Mean and Variance\\[mean-and-variance\\]","text":"Now relax normality assumption statistical independence.\nInstead, represent regression model \\(Y=X\\beta+e\\) \n\\[\\begin{aligned}\nE[e|X] & =0_{n}\\\\\n\\mathrm{var}\\left[e|X\\right] & =E\\left[ee'|X\\right]=\\sigma^{2}I_{n}.\\end{aligned}\\]\nfirst condition mean independence assumption, \nsecond condition homoskedasticity assumption. assumptions\nfirst second moments \\(e_{}\\) conditional \n\\(x_{}\\). Unlike normality assumption, restrict \ndistribution \\(e_{}\\).Unbiasedness: \\[\\begin{aligned}\nE\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(X'X\\right)^{-1}XY|X\\right]=E\\left[\\left(X'X\\right)^{-1}X\\left(X'\\beta+e\\right)|X\\right]\\\\\n& =\\beta+\\left(X'X\\right)^{-1}XE\\left[e|X\\right]=\\beta.\\end{aligned}\\]\nlaw iterated expectations, unconditional expectation\n\\(E\\left[\\widehat{\\beta}\\right]=E\\left[E\\left[\\widehat{\\beta}|X\\right]\\right]=\\beta.\\)\nUnbiasedness rely homoskedasticity.Unbiasedness: \\[\\begin{aligned}\nE\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(X'X\\right)^{-1}XY|X\\right]=E\\left[\\left(X'X\\right)^{-1}X\\left(X'\\beta+e\\right)|X\\right]\\\\\n& =\\beta+\\left(X'X\\right)^{-1}XE\\left[e|X\\right]=\\beta.\\end{aligned}\\]\nlaw iterated expectations, unconditional expectation\n\\(E\\left[\\widehat{\\beta}\\right]=E\\left[E\\left[\\widehat{\\beta}|X\\right]\\right]=\\beta.\\)\nUnbiasedness rely homoskedasticity.Variance:\n\\[\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)'|X\\right]\\\\\n& =E\\left[\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\widehat{\\beta}-\\beta\\right)'|X\\right]\\\\\n& =E\\left[\\left(X'X\\right)^{-1}X'ee'X\\left(X'X\\right)^{-1}|X\\right]\\\\\n& =\\left(X'X\\right)^{-1}X'E\\left[ee'|X\\right]X\\left(X'X\\right)^{-1}\n\\end{aligned}\\] second equality holds asVariance:\n\\[\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =E\\left[\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)'|X\\right]\\\\\n& =E\\left[\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\widehat{\\beta}-\\beta\\right)'|X\\right]\\\\\n& =E\\left[\\left(X'X\\right)^{-1}X'ee'X\\left(X'X\\right)^{-1}|X\\right]\\\\\n& =\\left(X'X\\right)^{-1}X'E\\left[ee'|X\\right]X\\left(X'X\\right)^{-1}\n\\end{aligned}\\] second equality holds asUnder assumption homoskedasticity, can simplified \n\\[\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =\\left(X'X\\right)^{-1}X'\\left(\\sigma^{2}I_{n}\\right)X\\left(X'X\\right)^{-1}\\\\\n& =\\sigma^{2}\\left(X'X\\right)^{-1}X'I_{n}X\\left(X'X\\right)^{-1}\\\\\n& =\\sigma^{2}\\left(X'X\\right)^{-1}.\n\\end{aligned}\\]assumption homoskedasticity, can simplified \n\\[\\begin{aligned}\\mathrm{var}\\left[\\widehat{\\beta}|X\\right] & =\\left(X'X\\right)^{-1}X'\\left(\\sigma^{2}I_{n}\\right)X\\left(X'X\\right)^{-1}\\\\\n& =\\sigma^{2}\\left(X'X\\right)^{-1}X'I_{n}X\\left(X'X\\right)^{-1}\\\\\n& =\\sigma^{2}\\left(X'X\\right)^{-1}.\n\\end{aligned}\\](Heteroskedasticity) \\(e_{}=x_{}u_{}\\), \\(x_{}\\) scalar\nrandom variable, \\(u_{}\\) statistically independent \\(x_{}\\),\n\\(E\\left[u_{}\\right]=0\\) \\(E\\left[u_{}^{2}\\right]=\\sigma_{u}^{2}\\).\n\n\\(E\\left[e_{}|x_{}\\right]=E\\left[x_{}u_{}|x_{}\\right]=x_{}E\\left[u_{}|x_{}\\right]=0\\)\n\n\\(E\\left[e_{}^{2}|x_{}\\right]=E\\left[x_{}^{2}u_{}^{2}|x_{}\\right]=x_{}^{2}E\\left[u_{}^{2}|x_{}\\right]=\\sigma_{u}^{2}x_{}^{2}\\)\nfunction \\(x_{}\\). say \\(e_{}^{2}\\) heteroskedastic error.**knitr**important notice independently identically distributed\nsample (iid) \\(\\left(y_{},x_{}\\right)\\) imply homoskedasticity.\nHomoskedasticity heteroskedasticity relationship \n\\(\\left(x_{},e_{}=y_{}-\\beta x\\right)\\) within observation, whereas\niid relationship \\(\\left(y_{},x_{}\\right)\\) \n\\(\\left(y_{j},x_{j}\\right)\\) \\(\\neq j\\) across observations.","code":""},{"path":"least-squares-finite-sample-theory.html","id":"gauss-markov-theorem","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.5 Gauss-Markov Theorem","text":"Gauss-Markov theorem concerned optimality OLS. \njustifies OLS efficient estimator among linear unbiased ones.\nEfficient means enjoys smallest variance family\nestimators.shown OLS unbiased \n\\(E\\left[\\widehat{\\beta}\\right]=\\beta\\). numerous linearly\nunbiased estimators. example, \\(\\left(Z'X\\right)^{-1}Z'y\\) \n\\(z_{}=x_{}^{2}\\) unbiased \n\\(E\\left[\\left(Z'X\\right)^{-1}Z'y\\right]=E\\left[\\left(Z'X\\right)^{-1}Z'\\left(X\\beta+e\\right)\\right]=\\beta\\).\nsay OLS better unbiased estimators \nunbiased — equally good aspect. move\nsecond order property variance: estimator better \nvariance smaller.two generic random vectors \\(X\\) \\(Y\\) size, say\n\\(X\\)’s variance smaller equal \\(Y\\)’s variance \n\\(\\left(\\Omega_{Y}-\\Omega_{X}\\right)\\) positive semi-definite matrix.\ncomparison defined way non-zero constant\nvector \\(c\\), variance linear combination \\(X\\)\n\\[\\mathrm{var}\\left(c'X\\right)=c'\\Omega_{X}c\\leq c'\\Omega_{Y}c=\\mathrm{var}\\left(c'Y\\right)\\]\nbigger linear combination \\(Y\\).Let \\(\\tilde{\\beta}='y\\) generic linear estimator, \\(\\) \n\\(n\\times K\\) functions \\(X\\). \n\\[E\\left['y|X\\right]=E\\left['\\left(X\\beta+e\\right)|X\\right]='X\\beta.\\]\nlinearity unbiasedness \\(\\tilde{\\beta}\\) implies\n\\('X=I_{n}\\). Moreover, variance\n\\[\\mbox{var}\\left('y|X\\right)=E\\left[\\left('y-\\beta\\right)\\left('y-\\beta\\right)'|X\\right]=E\\left['ee'|X\\right]=\\sigma^{2}'.\\]\nLet \\(C=-X\\left(X'X\\right)^{-1}.\\)\n\\[\\begin{aligned}'-\\left(X'X\\right)^{-1} & =\\left(C+X\\left(X'X\\right)^{-1}\\right)'\\left(C+X\\left(X'X\\right)^{-1}\\right)-\\left(X'X\\right)^{-1}\\\\\n& =C'C+\\left(X'X\\right)^{-1}X'C+C'X\\left(X'X\\right)^{-1}\\\\\n& =C'C,\n\\end{aligned}\\] last equality follows \n\\[\\left(X'X\\right)^{-1}X'C=\\left(X'X\\right)^{-1}X'\\left(-X\\left(X'X\\right)^{-1}\\right)=\\left(X'X\\right)^{-1}-\\left(X'X\\right)^{-1}=0.\\]\nTherefore \\('-\\left(X'X\\right)^{-1}\\) positive semi-definite\nmatrix. variance \\(\\tilde{\\beta}\\) smaller OLS\nestimator \\(\\widehat{\\beta}\\). derivation shows OLS achieves \nsmallest variance among linear unbiased estimators.Homoskedasticity restrictive assumption. homoskedasticity,\n\\(\\mathrm{var}\\left[\\widehat{\\beta}\\right]=\\sigma^{2}\\left(X'X\\right)^{-1}\\).\nPopular estimator \\(\\sigma^{2}\\) sample mean residuals\n\\(\\widehat{\\sigma}^{2}=\\frac{1}{n}\\widehat{e}'\\widehat{e}\\) \nunbiased one \\(s^{2}=\\frac{1}{n-K}\\widehat{e}'\\widehat{e}\\). \nheteroskedasticity, Gauss-Markov theorem apply.","code":""},{"path":"least-squares-finite-sample-theory.html","id":"summary-3","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.6 Summary","text":"exact distribution normality assumption error term\nclassical statistical results. Gauss Markov theorem holds\ntwo crucial assumptions: linear CEF homoskedasticity.Historical notes: MLE promulgated popularized Ronald\nFisher (1890–1962). major contributor frequentist\napproach dominates mathematical statistics today, sharply\ncriticized Bayesian approach. Fisher collected iris flower\ndataset 150 observations biological study 1936, can\ndisplayed R typing iris. Fisher invented many concepts \nclassical mathematical statistics, sufficient statistic,\nancillary statistic, completeness, exponential family, etc.reading: Phillips (1983) offered comprehensive\ntreatment exact small sample theory econometrics. ,\ntheoretical studies econometrics swiftly shifted large sample\ntheory, introduce next chapter.","code":""},{"path":"least-squares-finite-sample-theory.html","id":"appendix","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.7 Appendix","text":"","code":""},{"path":"least-squares-finite-sample-theory.html","id":"joint-normal-distribution","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.7.1 Joint Normal Distribution","text":"arguable normal distribution frequently\nencountered distribution statistical inference, \nasymptotic distribution many popular estimators. Moreover, boasts\nunique features facilitates calculation objects \ninterest. note summaries .\\(n\\times1\\) random vector \\(Y\\) follows joint normal distribution\n\\(N\\left(\\mu,\\Sigma\\right)\\), \\(\\mu\\) \\(n\\times1\\) vector \n\\(\\Sigma\\) \\(n\\times n\\) symmetric positive definite matrix. \nprobability density function \n\\[f_{y}\\left(y\\right)=\\left(2\\pi\\right)^{-n/2}\\left(\\mathrm{det}\\left(\\Sigma\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2}\\left(y-\\mu\\right)'\\Sigma^{-1}\\left(y-\\mu\\right)\\right)\\]\n\\(\\mathrm{det}\\left(\\cdot\\right)\\) determinant matrix.\nmoment generating function \n\\[M_{y}\\left(t\\right)=\\exp\\left(t'\\mu+\\frac{1}{2}t'\\Sigma t\\right).\\]discuss relationship two components random\nvector. fix notation, \\[Y=\\left(\\begin{array}{c}\nY_{1}\\\\\nY_{2}\n\\end{array}\\right)\\sim N\\left(\\left(\\begin{array}{c}\n\\mu_{1}\\\\\n\\mu_{2}\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\Sigma_{11} & \\Sigma_{12}\\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{array}\\right)\\right)\\] \\(Y_{1}\\) \\(m\\times1\\) vector, \n\\(Y_{2}\\) \\(\\left(n-m\\right)\\times1\\) vector. \\(\\mu_{1}\\) \\(\\mu_{2}\\)\ncorresponding mean vectors, \\(\\Sigma_{ij}\\), \\(j=1,2\\) \ncorresponding variance covariance matrices. now , always\nmaintain assumption \\(Y=\\left(Y_{1}',Y_{2}'\\right)'\\) jointly\nnormal.Fact \\[fact31\\]\nimmediately implies convenient feature normal distribution.\nGenerally speaking, given joint pdf two random variables\nintend find marginal distribution one random variables, \nneed integrate variable joint pdf. However, \nvariables jointly normal, information random\nvariable irrelevant marginal distribution random\nvariable interest. need know partial information \npart interest, say mean \\(\\mu_{1}\\) variance\n\\(\\Sigma_{11}\\) decide marginal distribution \\(Y_{1}\\).\\[fact:marginal\\]marginal\ndistribution \\(Y_{1}\\sim N\\left(\\mu_{1},\\Sigma_{11}\\right)\\).result convenient interested component \nestimator, entire vector estimator. example,\nOLS estimator linear regression model\n\\(y_{}=x_{}'\\beta+e_{}\\), classical assumption () random\nsample; (ii) independence \\(z_{}\\) \\(e_{}\\); (iii)\n\\(e_{}\\sim N\\left(0,\\gamma\\right)\\) \n\\[\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y,\\] finite sample\nexact distribution \\(\\widehat{\\beta}\\) \n\\[\\left(\\widehat{\\beta}-\\beta\\right)|X\\sim N\\left(0,\\gamma\\left(X'X\\right)^{-1}\\right)\\]\ninterested inference \\(j\\)-th component \n\\(\\beta_{0}^{\\left(j\\right)}\\), Fact\n\\[fact:marginal\\],\n\\[\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\left(X'X\\right)_{kk}^{-1}\\sim N\\left(0,\\gamma\\right)\\]\n\\(\\left[\\left(X'X\\right)^{-1}\\right]_{kk}\\) \\(k\\)-th diagonal\nelement \\(\\left(X'X\\right)^{-1}\\). marginal distribution \nindependent components. saves us integrating \ncomponents, troublesome dimension \nvector high.Generally, zero covariance two random variables indicates \nuncorrelated, whereas full statistical independence much\nstronger requirement. However, \\(Y_{1}\\) \\(Y_{2}\\) jointly\nnormal, zero covariance equivalent full independence.\\(\\Sigma_{12}=0\\), \\(Y_{1}\\) \\(Y_{2}\\) independent.\\(\\Sigma\\) invertible, \n\\(Y'\\Sigma^{-1}Y\\sim\\chi^{2}\\left(\\mathrm{rank}\\left(\\Sigma\\right)\\right)\\).last result, useful linear regression, \n\\(Y_{1}\\) \\(Y_{2}\\) jointly normal, conditional distribution \n\\(Y_{1}\\) \\(Y_{2}\\) still jointly normal, mean variance\nspecified following fact.\\(Y_{1}|Y_{2}\\sim N\\left(\\mu_{1}+\\Sigma_{12}\\Sigma_{22}^{-1}\\left(Y_{2}-\\mu_{2}\\right),\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\right)\\).","code":""},{"path":"least-squares-finite-sample-theory.html","id":"basus-theorem-subsecbasus-theoremsubsecbasus-theorem-labelsubsecbasus-theorem","chapter":"5 Least Squares: Finite Sample Theory","heading":"5.7.2 Basu’s Theorem* [\\[subsec:Basu\\'s-Theorem\\]]{#subsec:Basu’s-Theorem label=“subsec:Basu’s-Theorem”}","text":"\\(Y=\\left(y_{1},\\ldots,y_{n}\\right)\\) consists \\(n\\) iid observations. \nsay \\(T\\left(Y\\right)\\) sufficient statistic parameter\n\\(\\theta\\) conditional probability\n\\(f\\left(Y|T\\left(Y\\right)\\right)\\) depend \\(\\theta\\). say\n\\(S\\left(Y\\right)\\) ancillary statistic \\(\\theta\\) \ndistribution depend \\(\\theta\\).Basu’s theorem says complete sufficient statistic \nstatistically independent ancillary statistic.Sufficient statistic closely related exponential family \nclassical mathematical statistics. parametric distribution indexed \n\\(\\theta\\) member exponential family PDF can \nwritten \n\\[f\\left(Y|\\theta\\right)=h\\left(Y\\right)g\\left(\\theta\\right)\\exp\\left(\\eta\\left(\\theta\\right)'T\\left(Y\\right)\\right),\\]\n\\(g\\left(\\theta\\right)\\) \\(\\eta\\left(\\theta\\right)\\) functions\ndepend, \\(\\theta\\) \\(h\\left(Y\\right)\\) \\(T\\left(Y\\right)\\) \nfunctions depend \\(Y\\).(Univariate Gaussian location model.) normal distribution\n\\(y_{}\\sim N\\left(\\mu,\\gamma\\right)\\) known \\(\\gamma\\) unknown\n\\(\\mu\\), sample mean \\(\\bar{y}\\) sufficient statistic \nsample standard deviation \\(s^{2}\\) ancillary statistic.first verify sample mean \\(\\bar{y}=n^{-1}\\sum_{=1}^{n}y_{}\\)\nsufficient statistic \\(\\mu\\). Notice joint density \n\\(Y\\) \\[\\begin{aligned}\nf\\left(Y\\right) & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{=1}^{n}\\left(y_{}-\\mu\\right)^{2}\\right)\\\\\n& =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{=1}^{n}\\left(\\left(y_{}-\\bar{y}\\right)+\\left(\\bar{y}-\\mu\\right)\\right)^{2}\\right)\\\\\n& =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{=1}^{n}\\left(\\left(y_{}-\\bar{y}\\right)^{2}+2\\left(y_{}-\\bar{y}\\right)\\left(\\bar{y}-\\mu\\right)+\\left(\\bar{y}-\\mu\\right)^{2}\\right)\\right)\\\\\n& =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{=1}^{n}\\left(y_{}-\\bar{y}\\right)^{2}\\right)\\exp\\left(-\\frac{n}{2\\gamma}\\left(\\bar{y}-\\mu\\right)^{2}\\right).\\end{aligned}\\]\n\\(\\bar{y}\\sim N\\left(\\mu,\\gamma/n\\right),\\) marginal density\n\n\\[f\\left(\\bar{y}\\right)=\\left(2\\pi\\gamma/n\\right)^{-1/2}\\exp\\left(-\\frac{n}{2\\gamma}\\left(\\bar{y}-\\mu\\right)^{2}\\right).\\]\n\\(\\bar{y}\\) statistic \\(Y\\), \n\\(f\\left(Y,\\bar{y}\\right)=f\\left(Y\\right)\\). conditional density \n\\[f\\left(Y|\\bar{y}\\right)=\\frac{f\\left(Y,\\bar{y}\\right)}{f\\left(\\bar{y}\\right)}=\\frac{f\\left(Y\\right)}{f\\left(\\bar{y}\\right)}=\\sqrt{n}\\left(2\\pi\\gamma\\right)^{-\\frac{n-1}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{=1}^{n}\\left(y_{}-\\bar{y}\\right)^{2}\\right)\\]\nindependent \\(\\mu\\), thus \\(\\bar{y}\\) sufficient statistic\n\\(\\mu\\). meantime, sample standard deviation\n\\(s^{2}=\\frac{1}{n-1}\\sum_{=1}^{n}\\left(y_{}-\\bar{y}\\right)\\) \nancillary statistic \\(\\mu\\) , distribution \\(s^{2}\\)\ndepend \\(\\mu.\\)normal distribution known \\(\\sigma^{2}\\) unknown \\(\\mu\\)\nbelongs exponential family view decomposition\n\\[\\begin{aligned}\nf(Y) & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{=1}^{n}\\left(y_{}-\\mu\\right)^{2}\\right)\\\\\n& =\\underbrace{\\exp\\left(-\\sum_{=1}^{n}\\frac{y_{}^{2}}{2\\gamma}\\right)}_{h\\left(Y\\right)}\\cdot\\underbrace{\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{n}{2\\gamma}\\mu^{2}\\right)}_{g\\left(\\theta\\right)}\\cdot\\underbrace{\\exp\\left(\\frac{\\mu}{2\\gamma}n\\bar{y}\\right)}_{\\exp\\left(\\eta\\left(\\theta\\right)'T\\left(Y\\right)\\right)}.\\end{aligned}\\]\nexponential family class distributions special\nfunctional form convenient deriving sufficient statistics\nwell desirable properties classical mathematical\nstatistics.(Conditional Gaussian location model.) \n\\(y_{}\\sim N\\left(x_{}\\beta,\\gamma\\right)\\) known \\(\\gamma\\) \nunknown \\(\\beta\\), verify sample mean \\(\\widehat{\\beta}\\) \nsufficient statistic \\(\\beta\\). Notice joint density \\(Y\\)\ngiven \\(X\\) \\[\\begin{aligned}\nf\\left(Y|X\\right) & =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\sum_{=1}^{n}\\left(y_{}-\\mu\\right)^{2}\\right)\\\\\n& =\\left(2\\pi\\gamma\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(Y-X\\widehat{\\beta}\\right)'\\left(Y-X\\widehat{\\beta}\\right)\\right)\\exp\\left(-\\frac{1}{2\\gamma}\\left(\\widehat{\\beta}-\\beta\\right)'X'X\\left(\\widehat{\\beta}-\\beta\\right)\\right).\\end{aligned}\\]\n\n\\(\\widehat{\\beta}\\sim N\\left(\\beta,\\gamma\\left(X'X\\right)^{-1}\\right),\\)\nmarginal density \n\\[f\\left(\\widehat{\\beta}|X\\right)=\\left(2\\pi\\gamma\\right)^{-\\frac{K}{2}}\\left(\\mathrm{det}\\left(\\left(X'X\\right)^{-1}\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2\\gamma}\\left(\\widehat{\\beta}-\\beta\\right)'X'X\\left(\\widehat{\\beta}-\\beta\\right)\\right).\\]\nconditional density \\[\\begin{aligned}\nf\\left(Y|\\widehat{\\beta},X\\right) & =\\frac{f\\left(Y|X\\right)}{f\\left(\\widehat{\\beta}|X\\right)}\\\\\n& =\\left(2\\pi\\gamma\\right)^{-\\frac{n-K}{2}}\\left(\\mathrm{det}\\left(\\left(X'X\\right)^{-1}\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2\\gamma}\\left(Y-X\\widehat{\\beta}\\right)'\\left(Y-X\\widehat{\\beta}\\right)\\right)\\end{aligned}\\]\nindependent \\(\\beta\\), thus \\(\\widehat{\\beta}\\) sufficient\nstatistic \\(\\beta\\).meantime, sample standard deviation\n\\(s^{2}=\\frac{1}{n-1}\\sum_{=1}^{n}\\left(y_{}-x_{}\\widehat{\\beta}\\right)\\)\nancillary statistic \\(\\beta\\) , distribution \n\\(s^{2}\\) depend \\(\\beta.\\)Zhentao Shi. Oct 10.","code":""},{"path":"basic-asymptotic-theory.html","id":"basic-asymptotic-theory","chapter":"6 Basic Asymptotic Theory","heading":"6 Basic Asymptotic Theory","text":"universe, though enormous, consists fewer \\(10^{82}\\) atoms,\nfinite number. However, mathematical ideas bounded \nsecular realities. Asymptotic theory behaviors statistics\nsample size arbitrarily large infinity. set \napproximation techniques simplify complicated finite-sample analysis.\nAsymptotic theory cornerstone modern econometrics. sheds\nlights estimation inference procedures much general\nconditions covered exact finite sample theory.Nevertheless, always hand finite sample, mostly \ndifficult increase sample size reality. Asymptotic theory\nrarely answers “large large”, must cautious \ntreacherous landscape asymptopia. era big data, albeit\nsheer size data balloons dramatically, build \nsophisticated models better capture heterogeneity data. Large\nsample relative notion complexity model \nunderlying ()dependence structure data.classical parametric approach, based hard--verify\nparametric assumptions, asymptotic approach, predicated\nimaginary infinite sequences, deviate reality. \napproach constructive can judged case case. \nprevalence asymptotic theory mathematical amenability \ngenerality. law evolution elevates asymptotic theory \nthrone mathematical statistics time.","code":""},{"path":"basic-asymptotic-theory.html","id":"modes-of-convergence","chapter":"6 Basic Asymptotic Theory","heading":"6.1 Modes of Convergence","text":"first review convergence non-random sequence, \nlearned high school. Let \\(z_{1},z_{2},\\ldots\\) infinite\nsequence non-random variables.Convergence non-random sequence means \n\\(\\varepsilon>0\\), exists \\(N\\left(\\varepsilon\\right)\\) \n\\(n>N\\left(\\varepsilon\\right)\\), \n\\(\\left|z_{n}-z\\right|<\\varepsilon\\). say \\(z\\) limit \\(z_{n}\\),\nwrite \\(z_{n}\\z\\) \\(\\lim_{n\\\\infty}z_{n}=z\\).Instead deterministic sequence, interested \nconvergence sequence random variables. Since random variable\n“random” thanks induced probability measure measurable\nfunction, must clear convergence means. Several modes \nconvergence widely used.say sequence random variables \\(\\left(z_{n}\\right)\\) converges \nprobability \\(z\\), \\(z\\) can either random variable \nnon-random constant, \\(\\varepsilon>0\\), probability\n\\(P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|<\\varepsilon\\right\\} \\to1\\)\n(equivalently\n\\(P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|\\geq\\varepsilon\\right\\} \\to0\\))\n\\(n\\\\infty\\). can write \\(z_{n}\\stackrel{p}{\\}z\\) \n\\(\\mathrm{plim}_{n\\\\infty}z_{n}=z\\).sequence random variables \\(\\left(z_{n}\\right)\\) converges \nsquared-mean \\(z\\), \\(z\\) can either random variable \nnon-random constant, \\(E\\left[\\left(z_{n}-z\\right)^{2}\\right]\\to0.\\) \ndenoted \\(z_{n}\\stackrel{m.s.}{\\}z\\).definitions either\n\\(P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|>\\varepsilon\\right\\}\\)\n\\(E\\left[\\left(z_{n}-z\\right)^{2}\\right]\\) non-random quantity,\nconverges 0 non-random sequence.Squared-mean convergence stronger convergence probability.\n, \\(z_{n}\\stackrel{m.s.}{\\}z\\) implies \\(z_{n}\\stackrel{p}{\\}z\\)\nconverse untrue. example.\\[eg:\\_p\\_in\\_ms\\]\\((z_{n})\\) \nsequence binary random variables: \\(z_{n}=\\sqrt{n}\\) probability\n\\(1/n\\), \\(z_{n}=0\\) probability \\(1-1/n\\). \n\\(z_{n}\\stackrel{p}{\\}0\\) \\(z_{n}\\stackrel{m.s.}{\\nrightarrow}0\\). \nverify claims, notice \\(\\varepsilon>0\\), \n\\(P\\left(\\omega:\\left|z_{n}\\left(\\omega\\right)-0\\right|<\\varepsilon\\right)=P\\left(\\omega:z_{n}\\left(\\omega\\right)=0\\right)=1-1/n\\rightarrow1\\)\nthereby \\(z_{n}\\stackrel{p}{\\}0\\). hand,\n\\(E\\left[\\left(z_{n}-0\\right)^{2}\\right]=n\\cdot1/n+0\\cdot(1-1/n)=1\\nrightarrow0,\\)\n\\(z_{n}\\stackrel{m.s.}{\\nrightarrow}0\\).Example \\[eg:\\_p\\_in\\_ms\\] highlights difference two\nmodes convergence. Convergence probability count \nhappens subset sample space small probability.\nSquared-mean convergence deals average entire\nprobability space. random variable can take wild value, \nsmall probability though, may blow away squared-mean convergence.\ncontrary, irregularity undermine convergence \nprobability.convergence probability squared-mean convergence \nconvergence random variables target random variable constant.\n, distribution \\(z_{n}-z\\) concentrated around 0 \n\\(n\\\\infty\\). Instead, convergence distribution \nconvergence CDF, random variable. Let\n\\(F_{z_{n}}\\left(\\cdot\\right)\\) CDF \\(z_{n}\\) \n\\(F_{z}\\left(\\cdot\\right)\\) CDF \\(z\\).say sequence random variables \\(\\left(z_{n}\\right)\\) converges \ndistribution random variable \\(z\\) \n\\(F_{z_{n}}\\left(\\right)\\F_{z}\\left(\\right)\\) \\(n\\\\infty\\) \npoint \\(\\\\mathbb{R}\\) \\(F_{z}\\left(\\cdot\\right)\\) \ncontinuous. write \\(z_{n}\\stackrel{d}{\\}z\\).Convergence distribution weakest mode. \n\\(z_{n}\\stackrel{p}{\\}z\\), \\(z_{n}\\stackrel{d}{\\}z\\). converse\ntrue general, unless \\(z\\) non-random constant (constant\n\\(z\\) can viewed degenerate random variables, corresponding\n“CDF” \\(F_{z}\\left(\\cdot\\right)=1\\left\\{ \\cdot\\geq z\\right\\}\\).Let \\(x\\sim N\\left(0,1\\right)\\). \\(z_{n}=x+1/n\\), \n\\(z_{n}\\stackrel{p}{\\}x\\) course \\(z_{n}\\stackrel{d}{\\}x\\).\nHowever, \\(z_{n}=-x+1/n\\), \\(z_{n}=y+1/n\\) \n\\(y\\sim N\\left(0,1\\right)\\) independent \\(x\\), \n\\(z_{n}\\stackrel{d}{\\}x\\) \\(z_{n}\\stackrel{p}{\\nrightarrow}x\\).\\((z_{n})\\) sequence binary random variables: \\(z_{n}=n\\) \nprobability \\(1/\\sqrt{n}\\), \\(z_{n}=0\\) probability \\(1-1/\\sqrt{n}\\).\n\\(z_{n}\\stackrel{d}{\\}z=0.\\) \n\\[F_{z_{n}}\\left(\\right)=\\begin{cases}\n0 & <0\\\\\n1-1/\\sqrt{n} & 0\\leq \\leq n\\\\\n1 & \\geq n\n\\end{cases}.\\] \\(F_{z}\\left(\\right)=\\begin{cases} 0, & <0\\\\ 1 & \\geq0 \\end{cases}\\). easy verify \\(F_{z_{n}}\\left(\\right)\\)\nconverges \\(F_{z}\\left(\\right)\\) pointwisely point \n\\(\\left(-\\infty,0\\right)\\cup\\left(0,+\\infty\\right)\\), \n\\(F_{z}\\left(\\right)\\) continuous.far talked convergence scalar variables. three\nmodes converges can easily generalized random vectors. \nparticular, Cramer-Wold device collapses random vector \nrandom vector via arbitrary linear combination. say sequence \n\\(K\\)-dimensional random vectors \\(\\left(z_{n}\\right)\\) converge \ndistribution \\(z\\) \\(\\lambda'z_{n}\\stackrel{d}{\\}\\lambda'z\\) \n\\(\\lambda\\\\mathbb{R}^{K}\\) \\(\\left\\Vert \\lambda\\right\\Vert _{2}=1.\\)","code":""},{"path":"basic-asymptotic-theory.html","id":"law-of-large-numbers","chapter":"6 Basic Asymptotic Theory","heading":"6.2 Law of Large Numbers","text":"(Weak) law large numbers (LLN) collection statements \nconvergence probability sample average population\ncounterpart. basic form LLN :\n\\[\\frac{1}{n}\\sum_{=1}^{n}(z_{}-E[z_{}])\\stackrel{p}{\\}0\\] \n\\(n\\\\infty\\). Various versions LLN work different assumptions\nfeatures /dependence underlying random\nvariables.","code":""},{"path":"basic-asymptotic-theory.html","id":"cherbyshev-lln","chapter":"6 Basic Asymptotic Theory","heading":"6.2.1 Cherbyshev LLN","text":"illustrate LLN simple example Chebyshev LLN, can \nproved elementary calculation. utilizes Chebyshev\ninequality.Chebyshev inequality: random variable \\(x\\) finite second\nmoment \\(E\\left[x^{2}\\right]<\\infty\\), \n\\(P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} \\leq E\\left[x^{2}\\right]/\\varepsilon^{2}\\)\nconstant \\(\\varepsilon>0\\).Show \\(r_{2}\\geq r_{1}\\geq1\\), \n\\(E\\left[\\left|x\\right|^{r_{2}}\\right]<\\infty\\) implies\n\\(E\\left[\\left|x\\right|^{r_{1}}\\right]<\\infty.\\) (Hint: use Holder’s\ninequality.)Chebyshev inequality special case Markov inequality.Markov inequality: random variable \\(x\\) finite \\(r\\)-th\nabsolute moment \\(E\\left[\\left|x\\right|^{r}\\right]<\\infty\\) \n\\(r\\ge1\\), \n\\(P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} \\leq E\\left[\\left|x\\right|^{r}\\right]/\\varepsilon^{r}\\)\nconstant \\(\\varepsilon>0\\).easy verify Markov inequality.\n\\[\\begin{aligned}E\\left[\\left|x\\right|^{r}\\right] & =\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}+\\int_{\\left|x\\right|\\leq\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\\n& \\geq\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\\n& \\geq\\varepsilon^{r}\\int_{\\left|x\\right|>\\varepsilon}dF_{X}=\\varepsilon^{r}P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} .\n\\end{aligned}\\] Rearrange inequality obtain Markov\ninequality.Let partial sum \\(S_{n}=\\sum_{=1}^{n}x_{}\\), \n\\(\\mu_{}=E\\left[x_{}\\right]\\) \n\\(\\sigma_{}^{2}=\\mathrm{var}\\left[x_{}\\right]\\). apply Chebyshev\ninequality sample mean\n\\(z_{n}=\\overline{x}-\\bar{\\mu}=n^{-1}\\left(S_{n}-E\\left[S_{n}\\right]\\right)\\).\n\\[\\begin{aligned}\nP\\left\\{ \\left|z_{n}\\right|\\geq\\varepsilon\\right\\}  & =P\\left\\{ n^{-1}\\left|S_{n}-E\\left[S_{n}\\right]\\right|\\geq\\varepsilon\\right\\} \\nonumber \\\\\n& \\leq E\\left[\\left(n^{-1}\\sum_{=1}^{n}\\left(x_{}-\\mu_{}\\right)\\right)^{2}\\right]/\\varepsilon^{2}\\nonumber \\\\\n& =\\left(n\\varepsilon\\right)^{-2}\\left\\{ E\\left[\\sum_{=1}^{n}\\left(x_{}-\\mu_{}\\right)^{2}\\right]+\\sum_{=1}^{n}\\sum_{j\\neq }E\\left[\\left(x_{}-\\mu_{}\\right)\\left(x_{j}-\\mu_{j}\\right)\\right]\\right\\} \\nonumber \\\\\n& =\\left(n\\varepsilon\\right)^{-2}\\left\\{ \\sum_{=1}^{n}\\mathrm{var}\\left(x_{}\\right)+\\sum_{=1}^{n}\\sum_{j\\neq }\\mathrm{cov}\\left(x_{},x_{j}\\right)\\right\\} .\\label{eq:cheby_mean}\\end{aligned}\\]\nConvergence probability holds right-hand side shrinks 0 \n\\(n\\\\infty\\). example, \\(x_{1},\\ldots,x_{n}\\) iid \n\\(\\mathrm{var}\\left(x_{1}\\right)=\\sigma^{2}\\), RHS \n(\\[eq:cheby\\_mean\\]) \n\\(\\left(n\\varepsilon\\right)^{-2}\\left(n\\sigma^{2}\\right)=o\\left(n^{-1}\\right)\\to0\\).\nresult gives Chebyshev LLN:Chebyshev LLN: \\(\\left(z_{1},\\ldots,z_{n}\\right)\\) sample \niid observations, \\(E\\left[z_{1}\\right]=\\mu\\) , \n\\(\\sigma^{2}=\\mathrm{var}\\left[z_{1}\\right]<\\infty\\) exists, \n\\(\\frac{1}{n}\\sum_{=1}^{n}z_{}\\stackrel{p}{\\}\\mu.\\)convergence probability can indeed maintained much \ngeneral conditions iid case. random variables \nsample identically distributed, \nindependent either.Consider inid (independent non-identically distributed) sample\n\\(\\left(x_{1},\\ldots,x_{n}\\right)\\) \\(E\\left[x_{}\\right]=0\\) \n\\(\\mathrm{var}\\left[x_{}\\right]=\\sqrt{n}c\\) constant \\(c>0\\). Use\nChebyshev inequality show \n\\(n^{-1}\\sum_{=1}^{n}x_{}\\stackrel{p}{\\}0\\).Consider time series moving average model\n\\(x_{}=\\varepsilon_{}+\\theta\\varepsilon_{-1}\\) \\(=1,\\ldots,n\\),\n\\(\\left|\\theta\\right|<1\\), \\(E\\left[\\varepsilon_{}\\right]=0\\),\n\\(\\mathrm{var}\\left[\\varepsilon_{}\\right]=\\sigma^{2}\\), \n\\(\\left(\\varepsilon_{}\\right)_{=0}^{n}\\) iid. Use Chebyshev\ninequality show \\(n^{-1}\\sum_{=1}^{n}x_{}\\stackrel{p}{\\}0\\).Another useful LLN Kolmogorov LLN. Since derivation\nrequires advanced knowledge probability theory, state \nresult without proof.Kolmogorov LLN: \\(\\left(z_{1},\\ldots,z_{n}\\right)\\) sample \niid observations \\(E\\left[z_{1}\\right]=\\mu\\) exists, \n\\(\\frac{1}{n}\\sum_{=1}^{n}z_{}\\stackrel{p}{\\}\\mu\\).Compared Chebyshev LLN, Kolmogorov LLN requires \nexistence population mean, higher moments. \nhand, iid essential Kolmogorov LLN.Consider three distributions: standard normal \\(N\\left(0,1\\right)\\),\n\\(t\\left(2\\right)\\) (zero mean, infinite variance), Cauchy\ndistribution (moments exist). plot paths sample average\n\\(n=2^{1},2^{2},\\ldots,2^{20}\\). see sample averages\n\\(N\\left(0,1\\right)\\) \\(t\\left(2\\right)\\) converge, \nCauchy distribution .knitrout","code":""},{"path":"basic-asymptotic-theory.html","id":"central-limit-theorem","chapter":"6 Basic Asymptotic Theory","heading":"6.3 Central Limit Theorem","text":"central limit theorem (CLT) collection probability results\nconvergence distribution stable distribution. \nlimiting distribution usually Gaussian distribution. basic\nform CLT :conditions spelled , sample average \nzero-mean random variables \\(\\left(z_{1},\\ldots,z_{n}\\right)\\)\nmultiplied \\(\\sqrt{n}\\) satisfies\n\\[\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}z_{}\\stackrel{d}{\\}N\\left(0,\\sigma^{2}\\right)\\]\n\\(n\\\\infty\\).Various versions CLT work different assumptions \nrandom variables. Lindeberg-Levy CLT simplest CLT.sample \\(\\left(x_{1},\\ldots,x_{n}\\right)\\) iid,\n\\(E\\left[x_{1}\\right]=0\\) \n\\(\\mathrm{var}\\left[x_{1}\\right]=\\sigma^{2}<\\infty\\), \n\\(\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}x_{}\\stackrel{d}{\\}N\\left(0,\\sigma^{2}\\right)\\).Lindeberg-Levy CLT can proved moment generating function.\nrandom variable \\(x\\), function\n\\(M_{x}\\left(t\\right)=E\\left[\\exp\\left(xt\\right)\\right]\\) called \nmoment generating function (MGF) exists. MGF fully describes\ndistribution, just like PDF CDF. example, MGF \n\\(N\\left(\\mu,\\sigma^{2}\\right)\\) \n\\(\\exp\\left(\\mu t+\\frac{1}{2}\\sigma^{2}t^{2}\\right)\\).\\(E\\left[\\left|x\\right|^{k}\\right]<\\infty\\) positive integer \\(k\\),\n\n\\[M_{X}\\left(t\\right)=1+tE\\left[X\\right]+\\frac{t^{2}}{2}E\\left[X^{2}\\right]+\\ldots\\frac{t}{k!}E\\left[X^{k}\\right]+O\\left(t^{k+1}\\right).\\]\nassumption Lindeberg-Levy CLT,\n\\[M_{\\frac{X_{}}{\\sqrt{n}}}\\left(t\\right)=1+\\frac{t^{2}}{2n}\\sigma^{2}+O\\left(\\frac{t^{3}}{n^{3/2}}\\right)\\]\n\\(\\), independence \n\\[\\begin{aligned}M_{\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}x_{}}\\left(t\\right) & =\\prod_{=1}^{n}M_{\\frac{X_{}}{\\sqrt{n}}}\\left(t\\right)=\\left(1+\\frac{t^{2}}{2n}\\sigma^{2}+O\\left(\\frac{t^{3}}{n^{3/2}}\\right)\\right)^{n}\\\\\n& \\\\exp\\left(\\frac{\\sigma^{2}}{2}t^{2}\\right),\n\\end{aligned}\\] limit exactly characteristic function\n\\(N\\left(0,\\sigma^{2}\\right)\\).proof MGF simple elementary. drawback \ndistributions well-defined MGF. general proof can \ncarried replacing MGF characteristic function\n\\(\\varphi_{x}\\left(t\\right)=E\\left[\\exp\\left(\\mathrm{}xt\\right)\\right]\\),\n“\\(\\mathrm{}\\)” imaginary number. characteristic\nfunction Fourier transform probability measure \nalways exists. proof require background knowledge Fourier\ntransform inverse transform, pursuit .Lindeberg-Feller CLT: \\(\\left(x_{}\\right)_{=1}^{n}\\) inid. \nLindeberg condition satisfied (fixed \\(\\varepsilon>0\\),\n\\(\\frac{1}{s_{n}^{2}}\\sum_{=1}^{n}E\\left[x_{}^{2}\\cdot\\boldsymbol{1}\\left\\{ \\left|x_{}\\right|\\geq\\varepsilon s_{n}\\right\\} \\right]\\to0\\)\n\\(s_{n}=\\sqrt{\\sum_{=1}^{n}\\sigma_{}^{2}}\\)), \n\\[\\frac{\\sum_{=1}^{n}x_{}}{s_{n}}\\stackrel{d}{\\}N\\left(0,1\\right).\\]Lindeberg-Feller CLT: \\(\\left(x_{}\\right)_{=1}^{n}\\) inid. \nLindeberg condition satisfied (fixed \\(\\varepsilon>0\\),\n\\(\\frac{1}{s_{n}^{2}}\\sum_{=1}^{n}E\\left[x_{}^{2}\\cdot\\boldsymbol{1}\\left\\{ \\left|x_{}\\right|\\geq\\varepsilon s_{n}\\right\\} \\right]\\to0\\)\n\\(s_{n}=\\sqrt{\\sum_{=1}^{n}\\sigma_{}^{2}}\\)), \n\\[\\frac{\\sum_{=1}^{n}x_{}}{s_{n}}\\stackrel{d}{\\}N\\left(0,1\\right).\\]Lyapunov CLT: \\(\\left(x_{}\\right)_{=1}^{n}\\) inid. \n\\(\\max_{\\leq n}E\\left[\\left|x_{}\\right|^{3}\\right]<C<\\infty,\\) \n\n\\[\\frac{\\sum_{=1}^{n}x_{}}{s_{n}}\\stackrel{d}{\\}N\\left(0,1\\right).\\]Lyapunov CLT: \\(\\left(x_{}\\right)_{=1}^{n}\\) inid. \n\\(\\max_{\\leq n}E\\left[\\left|x_{}\\right|^{3}\\right]<C<\\infty,\\) \n\n\\[\\frac{\\sum_{=1}^{n}x_{}}{s_{n}}\\stackrel{d}{\\}N\\left(0,1\\right).\\]simulated example.\\[knitrout\\]","code":""},{"path":"basic-asymptotic-theory.html","id":"tools-for-transformations","chapter":"6 Basic Asymptotic Theory","heading":"6.4 Tools for Transformations","text":"original forms, LLN deals sample mean, CLT handles\nscaled (\\(\\sqrt{n}\\)) /standardized (standard deviation)\nsample mean. However, econometric estimators interest \nfunctions sample means. example, OLS estimator\n\\[\\widehat{\\beta}=\\left(\\frac{1}{n}\\sum_{}x_{}x_{}'\\right)^{-1}\\frac{1}{n}\\sum_{}x_{}y_{}\\]\ninvolves matrix inverse matrix-vector multiplication. need\ntools handle transformations.Continuous mapping theorem 1: \\(x_{n}\\stackrel{p}{\\}\\) \n\\(f\\left(\\cdot\\right)\\) continuous \\(\\), \n\\(f\\left(x_{n}\\right)\\stackrel{p}{\\}f\\left(\\right)\\).Continuous mapping theorem 1: \\(x_{n}\\stackrel{p}{\\}\\) \n\\(f\\left(\\cdot\\right)\\) continuous \\(\\), \n\\(f\\left(x_{n}\\right)\\stackrel{p}{\\}f\\left(\\right)\\).Continuous mapping theorem 2: \\(x_{n}\\stackrel{d}{\\}x\\) \n\\(f\\left(\\cdot\\right)\\) continuous almost surely support \n\\(x\\), \\(f\\left(x_{n}\\right)\\stackrel{d}{\\}f\\left(x\\right)\\).Continuous mapping theorem 2: \\(x_{n}\\stackrel{d}{\\}x\\) \n\\(f\\left(\\cdot\\right)\\) continuous almost surely support \n\\(x\\), \\(f\\left(x_{n}\\right)\\stackrel{d}{\\}f\\left(x\\right)\\).Slutsky’s theorem: \\(x_{n}\\stackrel{d}{\\}x\\) \n\\(y_{n}\\stackrel{p}{\\}\\), \n\\(x_{n}+y_{n}\\stackrel{d}{\\}x+\\)\n\\(x_{n}y_{n}\\stackrel{d}{\\}ax\\)\n\\(x_{n}/y_{n}\\stackrel{d}{\\}x/\\) \\(\\neq0\\).\nSlutsky’s theorem: \\(x_{n}\\stackrel{d}{\\}x\\) \n\\(y_{n}\\stackrel{p}{\\}\\), \\(x_{n}+y_{n}\\stackrel{d}{\\}x+\\)\\(x_{n}+y_{n}\\stackrel{d}{\\}x+\\)\\(x_{n}y_{n}\\stackrel{d}{\\}ax\\)\\(x_{n}y_{n}\\stackrel{d}{\\}ax\\)\\(x_{n}/y_{n}\\stackrel{d}{\\}x/\\) \\(\\neq0\\).\\(x_{n}/y_{n}\\stackrel{d}{\\}x/\\) \\(\\neq0\\).Slutsky’s theorem consists special cases continuous mapping\ntheorem 2. addition, multiplication division \nencountered frequently practice, list separate theorem.Delta method: \n\\(\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\}N\\left(0,\\Omega\\right)\\),\n\\(f\\left(\\cdot\\right)\\) continuously differentiable \n\\(\\theta_{0}\\) (meaning\n\\(\\frac{\\partial}{\\partial\\theta}f\\left(\\cdot\\right)\\) continuous\n\\(\\theta_{0}\\)), \n\\[\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\}N\\left(0,\\frac{\\partial f}{\\partial\\theta'}\\left(\\theta_{0}\\right)\\Omega\\left(\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)\\right)'\\right).\\]Take Taylor expansion \\(f\\left(\\widehat{\\theta}\\right)\\) around\n\\(f\\left(\\theta_{0}\\right)\\):\n\\[f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)=\\frac{\\partial f\\left(\\dot{\\theta}\\right)}{\\partial\\theta'}\\left(\\widehat{\\theta}-\\theta_{0}\\right),\\]\n\\(\\dot{\\theta}\\) lies line segment \\(\\widehat{\\theta}\\)\n\\(\\theta_{0}\\). Multiply \\(\\sqrt{n}\\) sides,\n\\[\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)=\\frac{\\partial f\\left(\\dot{\\theta}\\right)}{\\partial\\theta'}\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right).\\]\n\\(\\widehat{\\theta}\\stackrel{p}{\\}\\theta_{0}\\) implies\n\\(\\dot{\\theta}\\stackrel{p}{\\}\\theta_{0}\\) \n\\(\\frac{\\partial}{\\partial\\theta'}f\\left(\\cdot\\right)\\) continuous \n\\(\\theta_{0}\\), \n\\(\\frac{\\partial}{\\partial\\theta'}f\\left(\\dot{\\theta}\\right)\\stackrel{p}{\\}\\frac{\\partial f\\left(\\theta_{0}\\right)}{\\partial\\theta'}\\)\ncontinuous mapping theorem 1. view \n\\(\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\}N\\left(0,\\Omega\\right)\\),\nSlutsky’s Theorem implies\n\\[\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\}\\frac{\\partial f\\left(\\theta_{0}\\right)}{\\partial\\theta'}N\\left(0,\\Omega\\right)\\]\nconclusion follows.","code":""},{"path":"basic-asymptotic-theory.html","id":"summary-4","chapter":"6 Basic Asymptotic Theory","heading":"6.5 Summary","text":"Asymptotic theory topic vast breadth depth. \nchapter scratch surface . discuss \nnext chapter apply asymptotic tools learned \nOLS estimator.Historical notes: 1980s, econometricians \ngood training mathematical rigor master asymptotic theory. \nprominent young (time) econometricians came field \nchanged situation, among Halbert White (UCSD), Peter\nC.B. Phillips (Yale) Peter Robinson (LSE), name .reading: Halbert White (1950-2012) wrote accessible book\n(White 2000 first edition 1984) introduce asymptotics \neconometricians. book remains popular among researchers \ngraduate students economics. Davidson (1994) longer \nself-contained monograph.","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"asymptotic-properties-of-least-squares","chapter":"7 Asymptotic Properties of Least Squares","heading":"7 Asymptotic Properties of Least Squares","text":"learned basic asymptotic theory previous chapter. \napply results study asymptotic properties OLS estimator\n\\(\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'Y\\), key interest \ncourse. show () \\(\\widehat{\\beta}\\) consistent estimator\nlinear projection coefficient \\(\\beta\\); (ii) \\(\\widehat{\\beta}\\) \nasymptotically normal; (iii) asymptotic normality allows asymptotic\ninference \\(\\beta\\); (iv) condition variance components\ntest statistic can consistently estimated testing\nprocedure make feasible.","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"consistency","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.1 Consistency","text":"Consistency basic requirement estimators large\nsample. Intuitively, says sample size arbitrarily\nlarge, desirable estimator arbitrarily close (sense\nconvergence probability) population quantity interest.\nOtherwise, estimator still deviates object interest\ninfinite sample size, hard persuade researchers \nuse estimator unless compelling justification provided.generic estimator \\(\\widehat{\\theta}\\), say \\(\\widehat{\\theta}\\) \nconsistent \\(\\theta\\) \\(\\widehat{\\theta}\\stackrel{p}{\\}\\theta\\),\n\\(\\theta\\) non-random object.OLS, say \\(\\widehat{\\beta}\\) consistent \n\\(\\widehat{\\beta}\\stackrel{p}{\\}\\beta\\) \\(n\\\\infty\\), \\(\\beta\\)\nlinear projection coefficient population model\n\\(y_{}=x_{}'\\beta+e_{}\\) \\(E\\left[x_{}e_{}\\right]=0\\). verify\nconsistency, write\n\\[\\widehat{\\beta}-\\beta=\\left(X'X\\right)^{-1}X'e=\\left(\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\right)^{-1}\\frac{1}{n}\\sum_{=1}^{n}x_{}e_{}.\\label{eq:ols_d}\\]simplicity, chapter discuss iid setting . \nfirst term, LLN,\n\\[\\widehat{Q}:=\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\stackrel{p}{\\}Q:=E\\left[x_{}x_{}'\\right].\\]\n\\(\\widehat{Q}\\) sample mean \\(x_{}x_{}'\\) \\(Q\\) \npopulation mean \\(x_{}x_{}'\\). second term, LLN,\n\\[\\frac{1}{n}\\sum_{=1}^{n}x_{}e_{}\\stackrel{p}{\\}0.\\] \ncontinuous mapping theorem immediately implies\n\\[\\widehat{\\beta}-\\beta\\stackrel{p}{\\}Q^{-1}\\times0=0.\\] OLS\nestimator \\(\\widehat{\\beta}\\) consistent estimator \\(\\beta\\).matter whether \\(\\left(y_{},x_{}\\right)_{=1}^{n}\\) iid, \ninid, dependent sample, consistency holds long convergence\nprobability holds two expressions \\(Q\\) \ninvertible matrix.","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"asymptotic-distribution","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.2 Asymptotic Distribution","text":"finite sample, \\(\\widehat{\\beta}\\) random variable. shown\ndistribution \\(\\widehat{\\beta}\\) normality . Without\nrestrictive normality assumption, can characterize \nrandomness OLS estimator?know previous section \n\\(\\hat{\\beta}-\\beta\\stackrel{p}{\\}0\\) degenerates constant. \nstudy distribution, must scale proper multiplier \nlimit neither degenerates explodes. suitable\nscaling factor \\(\\sqrt{n}\\), CLT.\n\\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)=\\left(\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\right)^{-1}\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}x_{}e_{}.\\]\nSince \\(E\\left[x_{}e_{}\\right]=0\\), apply CLT obtain\n\\[\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}x_{}e_{}\\stackrel{d}{\\}N\\left(0,\\Sigma\\right)\\]\n\\(\\Sigma=E\\left[x_{}x_{}'e_{}^{2}\\right]\\). continuous\nmapping theorem,\n\\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}Q^{-1}\\times N\\left(0,\\Sigma\\right)\\sim N\\left(0,\\Omega\\right)\\label{eq:asym_norm}\\]\n\\(\\Omega=Q^{-1}\\Sigma Q^{-1}\\) called asymptotic variance.\nresult asymptotic normality OLS estimator.asymptotic variance \\(\\Omega=Q^{-1}\\Sigma Q^{-1}\\) called \nsandwich form. can simplified conditional homoskedasticity\n\\(E\\left[e_{}^{2}|x_{}\\right]=\\sigma^{2}\\) \\(\\), gives\n\\[\\Sigma=E\\left[x_{}x_{}'e_{}^{2}\\right]=E\\left[x_{}x_{}'E\\left[e_{}^{2}|X\\right]\\right]=\\sigma^{2}E\\left[x_{}x_{}'\\right]=\\sigma^{2}Q.\\]\ncase, \\(\\Omega=Q^{-1}\\Sigma Q^{-1}=\\sigma^{2}Q^{-1}\\), thus\n\\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}N\\left(0,\\sigma^{2}Q^{-1}\\right).\\label{eq:asym_norm_homo}\\]interested \\(k\\)-th parameter \\(\\beta_{k}\\), joint\ndistribution \n(\\[eq:asym\\_norm\\]) implies \\[\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right) & =\\sqrt{n}\\eta_{k}'\\left(\\widehat{\\beta}-\\beta\\right)\\nonumber \\\\\n& \\stackrel{d}{\\}N\\left(0,\\sigma^{2}\\eta_{k}'Q^{-1}\\eta_{k}\\right)\\sim N\\left(0,\\sigma^{2}[Q^{-1}]_{kk}\\right),\\label{eq:asym_norm_homok}\\end{aligned}\\]\n\\(\\eta_{k}=\\left(0,\\ldots,0,1,0\\ldots,0\\right)'\\) selector \n\\(k\\)-th element.\\(\\Omega^{-1/2}\\) multiplied sides \n\\[eq:asym\\_norm\\], \n\\[\\Omega^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}N\\left(0,I_{K}\\right).\\label{eq:asym_norm-pivot}\\]\nsay asymptotic distribution \n\\[eq:asym\\_norm-pivot\\], \\(N\\left(0,I_{K}\\right)\\), pivotal\ninvolve unknown parameter. contrast, \nasymptotic distribution \n\\[eq:asym\\_norm\\] pivotal \\(\\Omega\\) unknown \n\\(N\\left(0,\\Omega\\right)\\). interested \\(k\\)-th parameter\n\\(\\beta_{k}\\), can write\n\\[eq:asym\\_norm-pivot\\] pivotal form \n\\[\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)}{\\sqrt{\\sigma^{2}[Q^{-1}]_{kk}}}\\stackrel{d}{\\}N\\left(0,1\\right).\\label{eq:asym_norm_homok_pivot}\\]","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"asymptotic-inference","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.3 Asymptotic Inference","text":"now derived asymptotic distribution \n\\(\\widehat{\\beta}\\). However,\n\\[eq:asym\\_norm\\] \n\\[eq:asym\\_norm-pivot\\] useful statistical inference\n\\(\\Omega\\) known. reality \\(\\Omega\\) mostly unknown, \ntherefore need estimate make statistical inference\nfeasible. Suppose \\(\\tilde{\\Omega}\\) consistent estimator \n\\(\\Omega\\) \\(\\tilde{\\Omega}\\stackrel{p}{\\}\\Omega\\). \nreplace \\(\\Omega\\) \n\\[eq:asym\\_norm-pivot\\] \\(\\tilde{\\Omega}\\), \n\\[\\begin{aligned}\n\\tilde{\\Omega}^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right) & =\\tilde{\\Omega}^{-1/2}\\Omega^{1/2}\\times\\Omega^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right).\\end{aligned}\\]\n\\(\\Omega\\) positive definite, first factor\n\\(\\tilde{\\Omega}^{-1/2}\\Omega^{1/2}\\stackrel{p}{\\}I_{K}\\) \ncontinuous mapping theorem. second factor asymptotic normal \n\\[eq:asym\\_norm-pivot\\]. Thus Slutsky’s theorem implies\n\\[\\tilde{\\Omega}^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}N\\left(0,I_{K}\\right)\\label{eq:asym_norm_feasible}\\]\n\n\\[eq:asym\\_norm\\_feasible\\] feasible statistic \nasymptotic inference.next question consistently estimate\n\\(\\Omega=Q^{-1}\\Sigma Q^{-1}\\), equivalent come \n\\(\\tilde{\\Omega}\\). \\(\\widehat{Q}\\stackrel{p}{\\}Q\\). \nconsistent estimator \\(\\tilde{\\Sigma}\\) \\(\\Sigma\\), can\nplug consistent estimators form\n\\(\\tilde{\\Omega}=\\widehat{Q}^{-1}\\tilde{\\Sigma}\\widehat{Q}^{-1}\\). \ntricky question consistently estimate\n\\(\\Sigma=E\\left[x_{}x_{}'e_{}^{2}\\right]\\). use sample\nmean \\(x_{}x_{}'e_{}^{2}\\) estimate \\(\\Sigma\\) \\(e_{}\\) \nunobservable. homoskedasticity\n\\(\\Omega=Q^{-1}\\Sigma Q^{-1}=\\sigma^{2}Q^{-1}\\), similarly \nuse sample mean \\(e_{}^{2}\\) estimate \\(\\sigma^{2}\\).Heteroskedasticity ubiquitous econometrics. regression example\nnaturally generates conditional heteroskedasticity linear\nprobability model \\(y_{}=x_{}'\\beta+e_{}\\), \n\\(y_{}\\\\left\\{ 0,1\\right\\}\\) binary dependent variable. Assume CEF\n\\(E\\left[y_{}|x_{}\\right]=x_{}'\\beta\\), can use OLS \nconsistently estimate \\(\\beta\\). conditional variance\n\\[\\mathrm{var}\\left[e_{}|x_{}\\right]=\\mathrm{var}\\left[y_{}|x_{}\\right]=E\\left[y_{}|x_{}\\right]\\left(1-E\\left[y_{}|x_{}\\right]\\right)=x_{}'\\beta\\left(1-x_{}'\\beta\\right)\\]\nexplicitly depends \\(x_{}\\). words, conditional variance\nvaries \\(x_{}\\).Naturally, one may attempt use OLS residual\n\\(\\widehat{e}_{}=\\widehat{y}_{}-x_{}'\\widehat{\\beta}\\) replace \nregression error \\(e_{}\\), plug-estimators\n\\(\\widehat{\\Omega}=\\widehat{\\sigma}^{2}\\widehat{Q}^{-1}\\) \nhomoskedasticity, \n\\(\\widehat{\\sigma}^{2}=\\widehat{e}'\\widehat{e}/\\left(n-K\\right)\\) \n\\(\\widehat{\\sigma}^{2}=\\widehat{e}'\\widehat{e}/n\\), \n\\(\\widehat{\\Omega}=\\widehat{Q}^{-1}\\widehat{\\Sigma}\\widehat{Q}^{-1}\\) \nheteroskedasticity, \n\\(\\widehat{\\Sigma}=n^{-1}\\sum_{}x_{}x_{}'\\widehat{e}_{}^{2}\\).choose\n\\(\\widehat{\\sigma}^{2}=\\widehat{e}'\\widehat{e}/\\left(n-K\\right)\\) \nreplace \\(\\sigma^{2}\\) \n\\[eq:asym\\_norm\\_homok\\_pivot\\], resulting statistic\n\\(T_{k}=\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)}{\\sqrt{\\widehat{\\sigma}^{2}[\\widehat{Q}^{-1}]_{kk}}}\\)\nexactly \\(t\\)-statistic finite sample analysis. Recall \nclassical normal-error assumption, \\(t\\)-statistics follows\nexact finite sample \\(t\\)-distribution degrees freedom \\(n-K\\). \nasymptotic analysis, allow \\(e_{}\\) distribution \n\\(E\\left[e_{}^{2}|x_{}\\right]<\\infty\\) (impose assumption \nsimplicity. can relaxed inid cases.) asymptotic\nnormality allows us conduct asymptotic statistical inference. \n\\(t\\)-statistic, must draw critical values normal\ndistribution, \n\\[T_{k}=\\frac{\\sqrt{\\sigma^{2}[Q^{-1}]_{kk}}}{\\sqrt{\\widehat{\\sigma}^{2}[\\widehat{Q}^{-1}]_{kk}}}\\cdot\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)}{\\sqrt{\\sigma^{2}[Q^{-1}]_{kk}}}\\stackrel{d}{\\}1\\times N\\left(0,1\\right)\\sim N\\left(0,1\\right)\\]\nSlutsky’s theorem \n\\(\\widehat{\\sigma}^{2}\\stackrel{p}{\\}\\sigma^{2}\\).next section give sufficient conditions \n\\(\\widehat{\\sigma}^{2}\\stackrel{p}{\\}\\sigma^{2}\\) \n\\(\\widehat{\\Sigma}\\stackrel{p}{\\}\\Sigma\\).","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"consistency-of-feasible-variance-estimator","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.4 Consistency of Feasible Variance Estimator","text":"first show conditions elements \n\\(\\Sigma=E\\left[x_{}x_{}'e_{}^{2}\\right]\\) finite. ,\n\\(\\left\\Vert \\Sigma\\right\\Vert _{\\infty}<\\infty\\), \n\\(\\left\\Vert \\cdot\\right\\Vert _{\\infty}\\) value largest\nelement absolute value matrix vector. Let \\(z_{}=x_{}e_{}\\),\n\\(\\Sigma=E\\left[z_{}z_{}'\\right]\\).generic random variable \\(u_{}\\) finite variance, define \n\\(L_{2}\\)-norm \\(\\sqrt{E\\left[u_{}^{2}\\right]}\\). Given another\ngeneric random variable \\(v_{}\\) finite variance, define inner\nproduct \\(u_{}\\) \\(v_{}\\) \\(E\\left[u_{}v_{}\\right]\\).\\(\\left|E\\left[u_{}v_{}\\right]\\right|\\leq\\sqrt{E\\left[u_{}^{2}\\right]E\\left[v_{}^{2}\\right]}\\).Cauchy-Schwarz inequality (cross moments larger\nvariance)\n\\[\\left\\Vert \\Sigma\\right\\Vert _{\\infty}=\\max_{k\\[K]}E\\left[z_{ik}^{2}\\right],\\]\n\\(\\left[K\\right]:=\\left\\{ 1,2,\\ldots,K\\right\\}\\). \\(k\\),\n\\[E\\left[z_{ik}^{2}\\right]=E\\left[x_{ik}^{2}e_{}^{2}\\right]\\leq\\left(E\\left[x_{ik}^{4}\\right]E\\left[e_{}^{4}\\right]\\right)^{1/2}\\]\nlast inequality follows Cauchy-Schwarz\ninequality. implies sufficient conditions finite\nvariance \n\\[\\max_{k}E\\left[x_{ik}^{4}\\right]<\\infty\\ \\ \\mbox{}\\ \\ E\\left[e_{}^{4}\\right]<\\infty.\\label{eq:4th_moment}\\]\nmaintain conditions following derivation.","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"homoskedasticity","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.4.1 Homoskedasticity","text":"estimation variance, error homoskedastic,\n\\[\\begin{aligned}\\frac{1}{n}\\sum_{=1}^{n}\\widehat{e}_{}^{2} & =\\frac{1}{n}\\sum_{=1}^{n}\\left(e_{}+x_{}'\\left(\\widehat{\\beta}-\\beta\\right)\\right)^{2}\\\\\n& =\\frac{1}{n}\\sum_{=1}^{n}e_{}^{2}+\\left(\\frac{2}{n}\\sum_{=1}^{n}e_{}x_{}\\right)'\\left(\\widehat{\\beta}-\\beta\\right)+\\frac{1}{n}\\sum_{=1}^{n}\\left(\\widehat{\\beta}-\\beta\\right)'x_{}x_{}'\\left(\\widehat{\\beta}-\\beta\\right).\n\\end{aligned}\n\\label{eq:v-homo1}\\]generic \\(m\\)-vector \\(u\\), define \\(L_{2}\\)-norm \n\\(\\left\\Vert u\\right\\Vert _{2}=\\sqrt{u'u}\\). Given another generic\n\\(m\\)-vector \\(v\\), define inner product \\(u\\) \\(v\\) \n\\(\\left\\langle u,v\\right\\rangle =u'v\\).\\(\\left|\\left\\langle u,v\\right\\rangle \\right|\\leq\\left\\Vert u\\right\\Vert _{2}\\left\\Vert v\\right\\Vert _{2}\\),\nequivalently\n\\(\\left|u'v\\right|\\leq\\sqrt{\\left(u'u\\right)\\left(v'v\\right)}\\).Notice\n\\(\\frac{1}{n}\\sum_{=1}^{n}e_{}x_{}\\stackrel{p}{\\}E\\left[e_{}x_{}\\right]=0\\),\nsecond term \\[eq:v-homo1\\] \\[\\begin{aligned}\n\\left|\\left(\\frac{2}{n}\\sum_{=1}^{n}e_{}x_{}\\right)'\\left(\\widehat{\\beta}-\\beta\\right)\\right| & \\leq2\\left\\Vert \\frac{1}{n}\\sum_{=1}^{n}x_{}e_{}\\right\\Vert _{2}\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\nonumber \\\\\n& =o_{p}\\left(1\\right)o_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\label{eq:homo1}\\end{aligned}\\]\nCauchy-Schwarz inequality.generic \\(m\\times m\\) symmetric positive semi-definite matrix \\(\\)\ngeneric \\(m\\) vector \\(u\\), \n\\[\\left\\Vert u\\right\\Vert _{2}^{2}\\lambda_{\\min}\\left(\\right)\\leq u'Au\\leq\\left\\Vert u\\right\\Vert _{2}^{2}\\lambda_{\\max}\\left(\\right).\\]third term \\[eq:v-homo1\\] bounded \\[\\begin{aligned}\n\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\frac{1}{n}\\sum_{=1}^{n}e_{}^{2}x_{}x'_{}\\right)\\left(\\widehat{\\beta}-\\beta\\right) & \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\lambda_{\\max}\\left(\\frac{1}{n}\\sum_{=1}^{n}x_{}x'_{}\\right)\\nonumber \\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\mathrm{trace}\\left(\\frac{1}{n}\\sum_{=1}^{n}x_{}x'_{}\\right)\\nonumber \\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}K\\max_{k}\\left\\{ \\frac{1}{n}\\sum_{=1}^{n}x_{ik}^{2}\\right\\} \\nonumber \\\\\n& =o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right),\\label{eq:homo2}\\end{aligned}\\]\nstochastic order follows \n\\[\\frac{1}{n}\\sum_{=1}^{n}x_{ik}^{2}\\stackrel{p}{\\}E\\left[x_{ik}^{2}\\right]<\\infty\\]\nview condition\n\\[eq:4th\\_moment\\].(\\[eq:homo1\\])\n(\\[eq:homo2\\]) implies \n\\[\\frac{1}{n}\\sum_{=1}^{n}\\widehat{e}_{}^{2}=\\frac{1}{n}\\sum_{=1}^{n}e_{}^{2}+o_{p}\\left(1\\right)+o_{p}\\left(1\\right)=\\frac{1}{n}\\sum_{=1}^{n}e_{}^{2}+o_{p}\\left(1\\right)\\stackrel{p}{\\}\\sigma_{e}^{2}.\\]\n(See Appendix operations small op big Op.)","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"heteroskedasticity","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.4.2 Heteroskedasticity","text":"basic strategy proof similar general case \nheteroskedasticity, though step complicated.\n\\[\\begin{aligned}\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\widehat{e}_{}^{2} & =\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\left(e_{}+x_{}'\\left(\\widehat{\\beta}-\\beta\\right)\\right)^{2}\\\\\n& =\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'e_{}^{2}+\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\cdot e_{}x_{}'\\left(\\widehat{\\beta}-\\beta\\right)+\\frac{1}{n}\\sum_{=1}^{n}x_{}x_{}'\\left(\\left(\\widehat{\\beta}-\\beta\\right)'x_{}\\right)^{2}.\n\\end{aligned}\n\\label{eq:v-hetero}\\]generic \\(m\\)-vector \\(u\\), \\(L_{p}\\)-norm (\\(p\\geq1\\)) defined\n\n\\(\\left\\Vert u\\right\\Vert _{p}=\\left(\\left|u_{1}\\right|^{p}+\\cdots+\\left|u_{m}\\right|^{p}\\right)^{1/p}\\).two generic \\(m\\)-vectors \\(u\\) \\(v\\),\n\\[\\left|u'v\\right|\\leq\\left\\Vert u\\right\\Vert _{p}\\left\\Vert q\\right\\Vert _{q}\\]\n\\(p,q\\[1,\\infty)\\) \\(1/p+1/q=1\\).Cauchy-Schwarz inequality special case Holder’s inequality \n\\(p=q=2\\).second term \n\\[eq:v-hetero\\] bounded \n\\[\\begin{aligned} & \\max_{k,k'}\\left|\\frac{1}{n}\\sum_{=1}^{n}x_{ik}x_{ik'}\\cdot e_{}x_{}'\\left(\\widehat{\\beta}-\\beta\\right)\\right|\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\max_{k,k'}\\left\\Vert \\frac{1}{n}\\sum_{=1}^{n}x_{}e_{}x_{ik}x_{ik'}\\right\\Vert _{2}\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\sqrt{K}\\max_{k,k',k''}\\left|\\frac{1}{n}\\sum_{=1}^{n}e_{}x_{ik}x_{ik'}x_{ik''}\\right|\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\sqrt{K}\\left(\\frac{1}{n}\\sum_{=1}^{n}e_{}^{4}\\right)^{1/4}\\max_{k,k',k''}\\left(\\frac{1}{n}\\sum_{=1}^{n}\\left(x_{ik}x_{ik'}x_{ik''}\\right)^{4/3}\\right)^{3/4}\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\sqrt{K}\\left(\\frac{1}{n}\\sum_{=1}^{n}e_{}^{4}\\right)^{1/4}\\max_{k}\\left(\\frac{1}{n}\\sum_{=1}^{n}x_{ik}^{4}\\right)^{3/4}\\\\\n& =o_{p}\\left(1\\right)O_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right)\n\\end{aligned}\\] third inequality hold Holder’s\ninequality \\(p=4\\) \\(q=4/3\\), stochastic order \nguaranteed suitable conditions\n\\[\\frac{1}{n}\\sum_{=1}^{n}e_{}^{4}\\stackrel{p}{\\}E\\left[e_{}^{4}\\right]<\\infty\\ \\ \\text{}\\ \\ \\frac{1}{n}\\sum_{=1}^{n}x_{ik}^{4}\\stackrel{p}{\\}E\\left[x_{ik}^{4}\\right]<\\infty.\\]third term \\[eq:v-hetero\\] bounded \n\\[\\begin{aligned} & \\max_{k_{1},k_{2}}\\left|\\frac{1}{n}\\sum_{=1}^{n}x_{ik_{1}}x_{ik_{2}}\\left(\\left(\\widehat{\\beta}-\\beta\\right)'x_{}\\right)^{2}\\right|\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k_{1},k_{2}}\\left|\\frac{1}{n}\\sum_{=1}^{n}x_{ik_{1}}x_{ik_{2}}\\left(x_{}x_{}'\\right)\\right|\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k_{1},k_{2},k_{3},k_{4}}\\left|\\frac{1}{n}\\sum_{=1}^{n}x_{ik_{1}}x_{ik_{2}}x_{ik_{3}}x_{ik_{4}}\\right|\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k_{1},k_{2}}\\left|\\frac{1}{n}\\sum_{=1}^{n}x_{ik_{1}}^{2}x_{ik_{2}}^{2}\\right|\\\\\n& \\leq\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k}\\left|\\frac{1}{n}\\sum_{=1}^{n}x_{ik}^{4}\\right|\\\\\n& =o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right).\n\\end{aligned}\\] third fourth inequalities follow \napplying Cauchy Schwarz inequality.","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"summary-5","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.5 Summary","text":"One important techniques asymptotic theory \nmanipulating inequalities. derivations variances look\ncomplicated first glance, often encountered proofs \ntheoretical results. many years torment, accustomed\nroutine calculations.Historical notes: White (1980) drew attention \neconomic contexts violate classical statistical assumptions \nlinear regressions. seeded econometricians’ care, obsession, \nvariance estimation statistical inference. following decades \nwitnessed plethora proposals variance estimation deal \nvarious deviation classical assumptions.reading: chapter vectors finite\ndimensional. results can extended allow infinite \\(K\\) \n\\(K\\\\infty\\) much slower speed \\(n\\). asymptotic\ndevelopment require multiple indices, goes beyond \nsimplest case \\(n\\\\infty\\) learned . Big data \naccompanied big model, model indexed \nsample size can grow sophisticated \\(n\\) get bigger. \nproofs latest paper Shi, Su, Xie (2020), find loads \ninequality operations similar flavor chapter.","code":""},{"path":"asymptotic-properties-of-least-squares.html","id":"appendix-1","chapter":"7 Asymptotic Properties of Least Squares","heading":"7.6 Appendix","text":"introduce “big Op small op” notation. stochastic\ncounterparts “big O small o” notation deterministic cases.Small op: \\(x_{n}=o_{p}\\left(r_{n}\\right)\\) \n\\(x_{n}/r_{n}\\stackrel{p}{\\}0\\).Small op: \\(x_{n}=o_{p}\\left(r_{n}\\right)\\) \n\\(x_{n}/r_{n}\\stackrel{p}{\\}0\\).Big Op: \\(x_{n}=O_{p}\\left(r_{n}\\right)\\) \\(\\varepsilon>0\\),\nexists \\(c>0\\) \n\\(P\\left(\\left|x_{n}\\right|/r_{n}>c\\right)<\\varepsilon\\).Big Op: \\(x_{n}=O_{p}\\left(r_{n}\\right)\\) \\(\\varepsilon>0\\),\nexists \\(c>0\\) \n\\(P\\left(\\left|x_{n}\\right|/r_{n}>c\\right)<\\varepsilon\\).operations:\\(o_{p}\\left(1\\right)+o_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\);\\(o_{p}\\left(1\\right)+o_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\);\\(o_{p}\\left(1\\right)+O_{p}\\left(1\\right)=O_{p}\\left(1\\right)\\);\\(o_{p}\\left(1\\right)+O_{p}\\left(1\\right)=O_{p}\\left(1\\right)\\);\\(o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\).\\(o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right)\\).big Op small op notation allows us keep using equalities \ncalculation expressing stochastic order random objects.Zhentao Shi. Oct 21, 2020.","code":""},{"path":"asymptotic-properties-of-mle.html","id":"asymptotic-properties-of-mle","chapter":"8 Asymptotic Properties of MLE","heading":"8 Asymptotic Properties of MLE","text":"","code":""},{"path":"asymptotic-properties-of-mle.html","id":"examples-of-mle","chapter":"8 Asymptotic Properties of MLE","heading":"8.1 Examples of MLE","text":"Normal, Logistic, Probit, Poisson","code":""},{"path":"asymptotic-properties-of-mle.html","id":"consistency-1","chapter":"8 Asymptotic Properties of MLE","heading":"8.2 Consistency","text":"specify parametric distribution (pdf) \\(f\\left(x;\\theta\\right)\\) \nparameter space \\(\\Theta\\). Define\n\\(Q\\left(\\theta\\right)=E\\left[\\log f\\left(x;\\theta\\right)\\right]\\), \n\\(\\theta_{0}=\\arg\\max_{\\theta\\\\Theta}Q\\left(\\theta\\right)\\) maximizes\nexpected log-likelihood. Given sample \\(n\\) observations, \ncompute average sample log-likelihood\n\\(\\ell_{n}\\left(\\theta\\right)=\\frac{1}{n}\\sum_{=1}^{n}\\log f\\left(x;\\theta\\right)\\).\nMLE estimator \n\\(\\widehat{\\theta}=\\arg\\max_{\\theta\\\\Theta}\\ell_{n}\\left(\\theta\\right)\\).say correctly specified data\n\\(\\left(x_{1},\\ldots,x_{n}\\right)\\) generated pdf\n\\(f\\left(x;\\theta\\right)\\) \\(\\theta\\\\Theta\\). Otherwise \ndata generated member class distributions\n\\(\\mathcal{M}^{*}:=\\left\\{ \\theta\\\\Theta:f\\left(x;\\theta\\right)\\right\\}\\),\nsay misspecified. model misspecified, strictly\nspeaking log-likelihood function \\(\\ell_{n}\\left(\\theta\\right)\\)\ncalled quasi log-likelihood MLE estimator\n\\(\\widehat{\\theta}\\) called quasi MLE.discuss condition\n\\(\\widehat{\\theta}\\stackrel{p}{\\}\\theta_{0}\\), , maximizer \nsample log-likelihood converges probability maximizer \nexpected log-likelihood population. Notice unlike OLS, \nMLE estimators admit closed-form. defined \nmaximizer solved numerical optimization.first requirement consistency MLE \\(\\theta_{0}\\)\nuniquely defined. Suppose \\(\\theta_{0}\\\\mathrm{int}\\left(\\Theta\\right)\\)\nlies interior \\(\\Theta\\). Let\n\\(N\\left(\\theta_{0},\\varepsilon\\right)=\\left\\{ \\theta\\\\Theta:\\left|\\theta-\\theta_{0}\\right|<\\varepsilon\\right\\}\\)\nneighborhood around \\(\\theta_{0}\\) radius \\(\\varepsilon\\) \n\\(\\varepsilon>0\\).value \\(\\theta_{0}\\) identified \\(\\varepsilon>0\\), \nexists \\(\\delta=\\delta\\left(\\varepsilon\\right)>0\\) \n\\(Q\\left(\\theta_{0}\\right)>\\sup_{\\theta\\\\Theta\\backslash N\\left(\\theta_{0},\\varepsilon\\right)}Q\\left(\\theta\\right)+\\delta\\).know suitable condition, LLN implies\n\\(\\ell_{n}\\left(\\theta\\right)\\stackrel{p}{\\}Q\\left(\\theta\\right)\\) \n\\(\\theta\\\\Theta\\). pointwise result, meaning \\(\\theta\\) \ntaken fixed \\(n\\\\infty\\). However, \\(\\widehat{\\theta}\\) random \nfinite-sample, makes \\(\\ell_{n}(\\widehat{\\theta})\\) complicated\nfunction data particular \\(\\widehat{\\theta}\\) \nclosed-form solution. therefore need strengthen pointwise LLN.say uniform law large numbers (ULLN) \n\\(Q\\left(\\theta\\right)\\) holds \\(\\Theta\\) \n\\[P\\left\\{ \\sup_{\\theta\\\\Theta}\\left|\\ell_{n}\\left(\\theta\\right)-Q\\left(\\theta\\right)\\right|\\geq\\varepsilon\\right\\} \\to0\\label{eq:ULLN}\\]\n\\(\\varepsilon>0\\) \\(n\\\\infty\\).ULLN can established pointwise LLN plus regularity\nconditions, example \\(\\Theta\\) compact set, \n\\(\\log f\\left(x;\\cdot\\right)\\) continuous \\(\\theta\\) almost everywhere\nsupport \\(x\\).\\(\\theta_{0}\\) identified ULLN\n\\[eq:ULLN\\]\nhold, \\(\\widehat{\\theta}\\stackrel{p}{\\}\\theta_{0}\\).According definition consistency, can check\n\\[\\begin{aligned}\n& P\\left\\{ \\left|\\widehat{\\theta}-\\theta_{0}\\right|>\\varepsilon\\right\\} \\leq P\\left\\{ Q\\left(\\theta_{0}\\right)-Q(\\widehat{\\theta})>\\delta\\right\\} \\\\\n& =P\\left\\{ Q\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)+\\ell_{n}\\left(\\theta_{0}\\right)-\\ell_{n}(\\widehat{\\theta})+\\ell_{n}\\left(\\widehat{\\theta}\\right)-Q(\\widehat{\\theta})>\\delta\\right\\} \\\\\n& \\leq P\\left\\{ \\left|Q\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)\\right|+\\ell_{n}\\left(\\theta_{0}\\right)-\\ell_{n}(\\widehat{\\theta})+\\left|\\ell_{n}\\left(\\widehat{\\theta}\\right)-Q(\\widehat{\\theta})\\right|>\\delta\\right\\} \\\\\n& \\leq P\\left\\{ \\left|Q\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)\\right|+\\left|\\ell_{n}(\\widehat{\\theta})-Q(\\widehat{\\theta})\\right|\\geq\\delta\\right\\} \\\\\n& \\leq P\\left\\{ 2\\sup_{\\theta\\\\Theta}\\left|\\ell_{n}\\left(\\theta\\right)-Q\\left(\\theta\\right)\\right|\\geq\\delta\\right\\} =P\\left\\{ \\sup_{\\theta\\\\Theta}\\left|\\ell_{n}\\left(\\theta\\right)-Q\\left(\\theta\\right)\\right|\\geq\\frac{\\delta}{2}\\right\\} \\to0.\\end{aligned}\\]\nfirst line holds identification, third line \ntriangle inequality, fourth line definition MLE \n\\(\\ell_{n}(\\widehat{\\theta})\\geq\\ell_{n}\\left(\\theta_{0}\\right)\\), \nlast line ULLN.Identification necessary condition consistent estimation.\nAlthough \\(\\widehat{\\theta}\\) closed-form solution general, \nestablish consistency via ULLN point \\(\\theta\\\\Theta\\) \nconsideration.","code":""},{"path":"asymptotic-properties-of-mle.html","id":"asymptotic-normality","chapter":"8 Asymptotic Properties of MLE","heading":"8.3 Asymptotic Normality","text":"next step derive asymptotic distribution MLE\nestimator. Let\n\\(s\\left(x;\\theta\\right)=\\partial\\log f\\left(x;\\theta\\right)/\\partial\\theta\\)\n\n\\(h\\left(x;\\theta\\right)=\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\log f\\left(x;\\theta\\right)\\)\\[thm:mis-MLE\\] suitable\nregularity conditions, MLE estimator\n\\[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\}N\\left(0,\\left(E\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1}\\mathrm{var}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\left(E\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1}\\right).\\]“suitable regularity conditions” spelled later. Indeed,\nconditions can observed proof.\\(\\widehat{\\theta}\\) maximizer entails\n\\(\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\widehat{\\theta}\\right)=0\\).\nTake Taylor expansion \n\\(\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\widehat{\\theta}\\right)\\)\naround \\(\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)\\):\n\\[0-\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)=\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\widehat{\\theta}\\right)-\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)=\\frac{\\partial}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\dot{\\theta}\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\]\n\\(\\dot{\\theta}\\) point line segment connecting\n\\(\\widehat{\\theta}\\) \\(\\theta_{0}.\\) Rearrange equation \nmultiply side \\(\\sqrt{n}:\\)\n\\[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=-\\left(\\frac{\\partial}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)^{-1}\\sqrt{n}\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right).\\label{eq:taylor1}\\]\\(Q\\left(\\theta\\right)\\) differentiable \\(\\theta_{0}\\), \n\\(\\frac{\\partial}{\\partial\\theta}Q\\left(\\theta_{0}\\right)=0\\) first\ncondition optimality \\(\\theta_{0}\\) \\(Q\\left(\\theta\\right)\\).\nNotice \n\\(E\\left[s\\left(x;\\theta_{0}\\right)\\right]=\\frac{\\partial}{\\partial\\theta}Q\\left(\\theta_{0}\\right)=0\\)\ndifferentiation integration interchangeable. CLT, \nsecond factor \\[eq:taylor1\\] follows\n\\[\\sqrt{n}\\frac{\\partial}{\\partial\\theta}\\ell_{n}\\left(\\theta_{0}\\right)\\stackrel{d}{\\}N\\left(0,\\mathrm{var}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\right).\\]\nSuppose second factor \n\\[eq:taylor1\\] follows\n\\(\\frac{\\partial}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\dot{\\theta}\\right)\\stackrel{p}{\\}E\\left[h\\left(x;\\theta_{0}\\right)\\right]\\)\n(sufficient assume\n\\(E\\left[\\frac{\\partial^{3}}{\\partial\\theta_{}\\partial\\theta_{j}\\partial\\theta_{l}}\\log f\\left(x;\\theta_{0}\\right)\\right]\\)\ncontinuous \\(\\theta\\) \\(,j,l\\leq K\\). Thus \nconclusion Slutsky’s theorem.model misspecified, asymptotic variance takes \ncomplicated sandwich form. parametric model correctly\nspecified, asymptotic variance can simplified,\nthanks following important result information matrix equality.","code":""},{"path":"asymptotic-properties-of-mle.html","id":"information-matrix-equality","chapter":"8 Asymptotic Properties of MLE","heading":"8.4 Information Matrix Equality","text":"model correctly specified, \\(\\theta_{0}\\) true\nparameter value. variance\n\\(\\mathcal{}\\left(\\theta_{0}\\right):=\\mathrm{var}_{f\\left(x;\\theta_{0}\\right)}\\left[\\frac{\\partial}{\\partial\\theta}\\log f\\left(x;\\theta_{0}\\right)\\right]\\)\ncalled (Fisher) information matrix, \n\\(\\mathcal{H}\\left(\\theta_{0}\\right):=E_{f\\left(x;\\theta_{0}\\right)}\\left[h\\left(x;\\theta_{0}\\right)\\right]\\)\ncalled expected Hessian matrix. emphasize true\nunderlying distribution \\(f\\left(x;\\theta_{0}\\right)\\) writing \nsubscript mathematical expectations.\\[fact:Info\\]suitable regularity\nconditions, \n\\(\\mathcal{}\\left(\\theta_{0}\\right)=-\\mathcal{H}\\left(\\theta_{0}\\right)\\)\\(f\\left(x;\\theta_{0}\\right)\\) pdf,\n\\(\\int f\\left(x;\\theta_{0}\\right)dx=1\\). Take partial derivative \nrespect \\(\\theta\\), \\[\\begin{aligned}\n0 & =\\int\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)dx=\\int\\frac{\\partial f\\left(x;\\theta_{0}\\right)/\\partial\\theta}{f\\left(x;\\theta_{0}\\right)}f\\left(x;\\theta_{0}\\right)dx\\nonumber \\\\\n& =\\int\\left[s\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx=E_{f\\left(x;\\theta_{0}\\right)}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\label{eq:info_eqn_1}\\end{aligned}\\]\nthird equality holds chain rule\n\\[s\\left(x;\\theta_{0}\\right)=\\frac{\\partial f\\left(x;\\theta_{0}\\right)/\\partial\\theta}{f\\left(x;\\theta_{0}\\right)}.\\label{eq:ell_d}\\]\nTake second partial derivative \n(\\[eq:info\\_eqn\\_1\\]) respective \\(\\theta\\), according \nchain rule: \\[\\begin{aligned}\n0 & =\\int\\left[h\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx+\\int\\left[s\\left(x;\\theta_{0}\\right)\\right]\\frac{\\partial}{\\partial\\theta'}f\\left(x;\\theta_{0}\\right)dx\\\\\n& =\\int\\left[h\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx+\\int s\\left(x;\\theta_{0}\\right)\\frac{\\partial f\\left(x;\\theta_{0}\\right)/\\partial\\theta}{f\\left(x;\\theta\\right)}f\\left(x;\\theta_{0}\\right)dx\\\\\n& =\\int\\left[h\\left(x;\\theta_{0}\\right)\\right]f\\left(x;\\theta_{0}\\right)dx+\\int\\left[s\\left(x;\\theta_{0}\\right)s\\left(x;\\theta_{0}\\right)'\\right]f\\left(x;\\theta_{0}\\right)dx\\\\\n& =E_{f\\left(x;\\theta_{0}\\right)}\\left[h\\left(x;\\theta_{0}\\right)\\right]+E_{f\\left(x;\\theta_{0}\\right)}\\left[s\\left(x;\\theta_{0}\\right)s\\left(x;\\theta_{0}\\right)'\\right]\\\\\n& =\\mathcal{H}\\left(\\theta_{0}\\right)+\\mathcal{}\\left(\\theta_{0}\\right).\\end{aligned}\\]\nsecond equality follows \n(\\[eq:ell\\_d\\]).\nlast equality \n\\[eq:info\\_eqn\\_1\\] zero mean ensures variance \n\\(\\frac{\\partial}{\\partial\\theta}\\log f\\left(x;\\theta_{0}\\right)\\) \nequal expectation -product.Notice correct specification essential information\nmatrix equality. true data generating distribution \n\\(g\\notin\\mathcal{M}^{*}\\), \n\\[eq:info\\_eqn\\_1\\] breaks \n\\[0=\\int\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)=\\int\\left[g^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)\\right]g=E_{g}\\left[g^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)\\right]\\]\n\n\\(g^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)\\neq\\left(f\\left(x;\\theta_{0}\\right)\\right)^{-1}\\frac{\\partial}{\\partial\\theta}f\\left(x;\\theta_{0}\\right)=\\frac{\\partial}{\\partial\\theta}\\log f\\left(\\theta_{0}\\right)\\).\nasymptotic variance Theorem\n\\[thm:mis-MLE\\],\n\\[\\left(E_{g}\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1}\\mathrm{var}_{g}\\left[s\\left(x;\\theta_{0}\\right)\\right]\\left(E_{g}\\left[h\\left(x;\\theta_{0}\\right)\\right]\\right)^{-1},\\]\nwritten explicitly \\(E_{g}\\left[\\cdot\\right]\\), still valid.parametric model \\(\\mathcal{M}^{*}\\) correctly specified, \ncan replace\n\\(E_{g}\\left[\\frac{\\partial^{2}\\ell_{n}}{\\partial\\theta\\partial\\theta'}\\left(\\theta_{0}\\right)\\right]\\)\n\\(\\mathcal{H}\\left(\\theta_{0}\\right)\\) replace\n\\(\\mathrm{var}_{g}\\left[\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\theta_{0}\\right)\\right]\\)\n\\(\\mathcal{}\\left(\\theta_{0}\\right)\\), simplify asymptotic\nvariance \n\\[\\left(\\mathcal{H}\\left(\\theta_{0}\\right)\\right)^{-1}\\mathcal{}\\left(\\theta_{0}\\right)\\left(\\mathcal{H}\\left(\\theta_{0}\\right)\\right)^{-1}=\\left(-\\mathcal{}\\left(\\theta_{0}\\right)\\right)^{-1}\\mathcal{}\\left(\\theta_{0}\\right)\\left(-\\mathcal{}\\left(\\theta_{0}\\right)\\right)^{-1}=\\left(\\mathcal{}\\left(\\theta_{0}\\right)\\right)^{-1}\\]\ninformation matrix equality Fact\n\\[fact:Info\\].model correctly specified, conditions Theorem\n\\[eq:info\\_eqn\\_1\\] Fact\n\\[fact:Info\\]\nMLE estimator\n\\[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\}N\\left(0,\\left[\\mathcal{}\\left(\\theta_{0}\\right)\\right]^{-1}\\right).\\]classical asymptotic normality result MLE.","code":""},{"path":"asymptotic-properties-of-mle.html","id":"cramer-rao-lower-bound","chapter":"8 Asymptotic Properties of MLE","heading":"8.5 Cramer-Rao Lower Bound","text":"","code":""},{"path":"asymptotic-properties-of-mle.html","id":"summary-6","chapter":"8 Asymptotic Properties of MLE","heading":"8.6 Summary","text":"reading: White (1996), Newey McFadden (1994).Zhentao Shi. Oct 29, 2020.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"9 Hypothesis Testing","heading":"9 Hypothesis Testing","text":"Notation: \\(\\mathbf{X}\\) denotes random variable random vector.\n\\(\\mathbf{x}\\) realization.hypothesis statement parameter space \\(\\Theta\\).\nHypothesis testing checks whether data support null hypothesis\n\\(\\Theta_{0}\\), subset \\(\\Theta\\) interest. Ideally \nnull hypothesis suggested scientific theory. \nalternative hypothesis \\(\\Theta_{1}=\\Theta\\backslash\\Theta_{0}\\) \ncomplement \\(\\Theta_{0}\\). Based observed evidence, hypothesis\ntesting decides accept reject null hypothesis. null\nhypothesis rejected data, implies statistical\nperspective data incompatible proposed scientific\ntheory.chapter, first introduce idea practice \nhypothesis testing related confidence interval. mainly\nfocus frequentist interpretation hypothesis, briefly\ndiscuss Bayesian approach statistical decision. application\ntesting procedures linear regression model, elaborate\ntest linear nonlinear hypothesis slope coefficients\nbased unrestricted restricted OLS estimators.","code":""},{"path":"hypothesis-testing.html","id":"testing","chapter":"9 Hypothesis Testing","heading":"9.1 Testing","text":"","code":""},{"path":"hypothesis-testing.html","id":"decision-rule-and-errors","chapter":"9 Hypothesis Testing","heading":"9.1.1 Decision Rule and Errors","text":"\\(\\Theta_{0}\\) singleton, call simple hypothesis;\notherwise call composite hypothesis. example, \nparameter space \\(\\Theta=\\mathbb{R}\\), \\(\\Theta_{0}=\\left\\{ 0\\right\\}\\)\n(equivalently \\(\\theta_{0}=0\\)) simple hypothesis, whereas\n\\(\\Theta_{0}=(-\\infty,0]\\) (equivalently \\(\\theta_{0}\\leq0\\)) \ncomposite hypothesis.test function mapping\n\\[\\phi:\\mathcal{X}^{n}\\mapsto\\left\\{ 0,1\\right\\} ,\\] \\(\\mathcal{X}\\)\nsample space. null hypothesis accepted \n\\(\\phi\\left(\\mathbf{x}\\right)=0\\), rejected \n\\(\\phi\\left(\\mathbf{x}\\right)=1\\). call set\n\\(A_{\\phi}=\\left\\{ \\mathbf{x}\\\\mathcal{X}^{n}:\\phi_{\\theta}\\left(\\mathbf{x}\\right)=0\\right\\}\\)\nacceptance region, complement\n\\(R_{\\phi}=\\left\\{ \\mathbf{x}\\\\mathcal{X}^{n}:\\phi\\left(\\mathbf{x}\\right)=1\\right\\}\\)\nrejection region.power function test \\(\\phi\\) \n\\[\\beta\\left(\\theta\\right)=P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=1\\right\\} =E_{\\theta}\\left[\\phi\\left(\\mathbf{X}\\right)\\right].\\]\npower function measures probability test function\nrejects null data generated true parameter\n\\(\\theta\\), reflected \\(P_{\\theta}\\) \\(E_{\\theta}\\).power test \\(\\theta\\\\Theta_{1}\\) value \n\\(\\beta\\left(\\theta\\right)\\). size test \n\\(\\sup_{\\theta\\\\Theta_{0}}\\beta\\left(\\theta\\right).\\) Notice \ndefinition power depends \\(\\theta\\) alternative hypothesis\n\\(\\Theta_{1}\\), whereas size independent \\(\\theta\\) due \nsupremum set null \\(\\Theta_{0}\\). level test \nvalue \\(\\alpha\\\\left(0,1\\right)\\) \n\\(\\alpha\\geq\\sup_{\\theta\\\\Theta_{0}}\\beta\\left(\\theta\\right)\\), \noften used difficult attain exact supremum. test \nsize \\(\\alpha\\) also level \\(\\alpha\\) bigger; test level\n\\(\\alpha\\) must size smaller equal \\(\\alpha\\).concept level useful sufficient\ninformation derive exact size test. \n\\(\\left(X_{1i},X_{2i}\\right)_{=1}^{n}\\) randomly drawn \nunknown joint distribution, know marginal distribution \n\\(X_{ji}\\sim N\\left(\\theta_{j},1\\right)\\), \\(j=1,2\\). order test\njoint hypothesis \\(\\theta_{1}=\\theta_{2}=0\\), can construct test\nfunction\n\\[\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)=1\\left\\{ \\left\\{ \\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\cup\\left\\{ \\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\right\\} ,\\]\n\\(z_{1-\\alpha/4}\\) \\(\\left(1-\\alpha/4\\right)\\)-th quantile \nstandard normal distribution. level test \n\\[\\begin{aligned}P\\left(\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)\\right) & \\leq P\\left(\\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right)+P\\left(\\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right)\\\\\n& =\\alpha/2+\\alpha/2=\\alpha.\n\\end{aligned}\\] inequality follows Bonferroni\ninequality\n\\[P\\left(\\cup B\\right)\\leq P\\left(\\right)+P\\left(B\\right).\\] (\nseemingly trivial Bonferroni inequality useful many proofs \nprobability results.) Therefore, level \n\\(\\phi\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)\\) \\(\\alpha\\), \nexact size unknown without knowledge joint distribution.\n(Even know correlation \\(X_{1i}\\) \\(X_{2i}\\), putting two\nmarginally normal distributions together make jointly normal\nvector general.): \\[tab:Decisions--States\\] Actions, States ConsequencesThe probability committing Type error \n\\(\\beta\\left(\\theta\\right)\\) \\(\\theta\\\\Theta_{0}\\).probability committing Type error \n\\(\\beta\\left(\\theta\\right)\\) \\(\\theta\\\\Theta_{0}\\).probability committing Type II error \n\\(1-\\beta\\left(\\theta\\right)\\) \\(\\theta\\\\Theta_{1}\\).probability committing Type II error \n\\(1-\\beta\\left(\\theta\\right)\\) \\(\\theta\\\\Theta_{1}\\).philosophy hypothesis testing debated centuries. \npresent prevailing framework statistics textbooks \nfrequentist perspective. frequentist views parameter fixed\nconstant. keep conservative attitude Type error: \noverwhelming evidence demonstrated shall researcher reject \nnull. principle protecting null hypothesis, desirable\ntest small level. Conventionally take \\(\\alpha=0.01,\\)\n0.05 0.1. say test unbiased \n\\(\\beta\\left(\\theta\\right)>\\sup_{\\theta\\\\Theta_{0}}\\beta\\left(\\theta\\right)\\)\n\\(\\theta\\\\Theta_{1}\\). can many tests correct size.trivial test function\n\\(\\phi(\\mathbf{x})=1\\left\\{ 0\\leq U\\leq\\alpha\\right\\}\\) \n\\(\\theta\\\\Theta\\), \\(U\\) random variable uniform\ndistribution \\(\\left[0,1\\right]\\), correct size \\(\\alpha\\) \nnon-trivial power alternative. extreme, trivial\ntest function \\(\\phi\\left(\\mathbf{x}\\right)=1\\) \\(\\mathbf{x}\\)\nenjoys biggest power suffers incorrect size.Usually, design test proposing test statistic\n\\(T_{n}:\\mathcal{X}^{n}\\mapsto\\mathbb{R}^{+}\\) critical value\n\\(c_{1-\\alpha}\\). Given \\(T_{n}\\) \\(c_{1-\\alpha}\\), write test\nfunction \n\\[\\phi\\left(\\mathbf{X}\\right)=1\\left\\{ T_{n}\\left(\\mathbf{X}\\right)>c_{1-\\alpha}\\right\\} .\\]\nensure \\(\\phi\\left(\\mathbf{x}\\right)\\) correct size, need\nfigure distribution \\(T_{n}\\) null hypothesis\n(called null distribution), choose critical value\n\\(c_{1-\\alpha}\\) according null distribution desirable size\nlevel \\(\\alpha\\).Another commonly used indicator hypothesis testing \\(p\\)-value:\n\\[\\sup_{\\theta\\\\Theta_{0}}P_{\\theta}\\left\\{ T_{n}\\left(\\mathbf{x}\\right)\\leq T_{n}\\left(\\mathbf{X}\\right)\\right\\} .\\]\nexpression, \\(T_{n}\\left(\\mathbf{x}\\right)\\) realized\nvalue test statistic \\(T_{n}\\), \n\\(T_{n}\\left(\\mathbf{X}\\right)\\) random variable generated \n\\(\\mathbf{X}\\) null \\(\\theta\\\\Theta_{0}\\). interpretation \n\\(p\\)-value tricky. \\(p\\)-value probability observe\n\\(T_{n}(\\mathbf{X})\\) greater realized \\(T_{n}(\\mathbf{x})\\)\nnull hypothesis true.\\(p\\)-value probability null hypothesis true.\nfrequentist perspective, null hypothesis either true \nfalse, certainty. randomness test comes \nsampling, hypothesis. \\(p\\)-value measures whether \ndataset compatible null hypothesis. \\(p\\)-value closely\nrelated corresponding test. \\(p\\)-value smaller \nspecified test size \\(\\alpha\\), test rejects null.far talking hypothesis testing finite sample.\ndiscussion terminologies can carried asymptotic\nworld \\(n\\\\infty\\). denote power function \n\\(\\beta_{n}\\left(\\theta\\right)\\), make dependence \nsample size \\(n\\) explicit, test asymptotic size \\(\\alpha\\) \n\\(\\limsup_{n\\\\infty}\\beta_{n}\\left(\\theta\\right)\\leq\\alpha\\) \n\\(\\theta\\\\Theta_{0}\\). test consistent \n\\(\\beta_{n}\\left(\\theta\\right)\\to1\\) every \\(\\theta\\\\Theta_{1}\\).","code":"                   accept $H_{0}$     reject $H_{0}$\n  $H_{0}$ true    correct decision     Type I error\n  $H_{0}$ false    Type II error     correct decision"},{"path":"hypothesis-testing.html","id":"optimality","chapter":"9 Hypothesis Testing","heading":"9.1.2 Optimality","text":"Just may multiple valid estimators task estimation,\nmay multiple tests task hypothesis testing. \nclass tests level \\(\\alpha\\) null\n\\(\\Psi_{\\alpha}=\\left\\{ \\phi:\\sup_{\\theta\\\\Theta_{0}}\\beta_{\\phi}\\left(\\theta\\right)\\leq\\alpha\\right\\}\\)\nput subscript \\(\\phi\\) \\(\\beta_{\\phi}\\left(\\theta\\right)\\) \ndistinguish power different tests, natural prefer \ntest \\(\\phi^{*}\\) exhibits higher power tests \nconsideration point alternative hypothesis \n\\[\\beta_{\\phi^{*}}\\left(\\theta\\right)\\geq\\beta_{\\phi}\\left(\\theta\\right)\\]\nevery \\(\\theta\\\\Theta_{1}\\) every \\(\\phi\\\\Psi_{\\alpha}\\). \ntest \\(\\phi^{*}\\\\Psi_{\\alpha}\\) exists, call uniformly\npowerful test.Suppose random sample size 6 generated \n\\[\\left(X_{1},\\ldots,X_{6}\\right)\\sim\\text{iid.}N\\left(\\theta,1\\right),\\]\n\\(\\theta\\) unknown. want infer population mean \nnormal distribution. null hypothesis \\(H_{0}\\): \\(\\theta\\leq0\\) \nalternative \\(H_{1}\\): \\(\\theta>0\\). tests \n\\[\\Psi=\\left\\{ 1\\left\\{ \\bar{X}\\geq c/\\sqrt{6}\\right\\} :c\\geq1.64\\right\\}\\]\ncorrect level. Since \\(\\bar{X}=N\\left(\\theta,1/6\\right)\\), \npower function \\(\\Psi\\) \\[\\begin{aligned}\n\\beta_{\\phi}\\left(\\theta\\right) & =P\\left(\\bar{X}\\geq\\frac{c}{\\sqrt{6}}\\right)=P\\left(\\frac{\\bar{X}-\\theta}{1/\\sqrt{6}}\\geq\\frac{\\frac{c}{\\sqrt{6}}-\\theta}{1/\\sqrt{6}}\\right)\\\\\n& =P\\left(N\\geq c-\\sqrt{6}\\theta\\right)=1-\\Phi\\left(c-\\sqrt{6}\\theta\\right)\\end{aligned}\\]\n\\(N=\\frac{\\bar{X}-\\theta}{1/\\sqrt{6}}\\) follows standard normal,\n\\(\\Phi\\) cdf standard normal. clear \n\\(\\beta_{\\phi}\\left(\\theta\\right)\\) monotonically decreasing \\(c\\).\nThus test function\n\\[\\phi_{\\theta=0}\\left(\\mathbf{X}\\right)=1\\left\\{ \\bar{X}\\geq1.64/\\sqrt{6}\\right\\}\\]\npowerful test \\(\\Psi\\), \\(c=1.64\\) lower bound \n\\(\\Psi_{\\alpha}\\) allows order keep level \\(\\alpha\\).","code":""},{"path":"hypothesis-testing.html","id":"likelihood-ratio-test-and-wilks-theorem","chapter":"9 Hypothesis Testing","heading":"9.1.3 Likelihood-Ratio Test and Wilks’ theorem","text":"estimators available closed-forms, likelihood-ratio\ntest (LRT) serves general testing statistic \nlikelihood principle. Let\n\\(\\ell_{n}\\left(\\theta\\right)=n^{-1}\\sum_{}\\log f\\left(x_{};\\theta\\right)\\)\naverage sample log-likelihood, \n\\(\\widehat{\\theta}=\\arg\\max_{\\theta\\\\Theta}\\ell_{n}\\left(\\theta\\right)\\)\nmaximum likelihood estimator (MLE). Take Taylor expansion \n\\(\\ell_{n}\\left(\\theta_{0}\\right)\\) around\n\\(\\ell_{n}\\left(\\widehat{\\theta}\\right)\\): \\[\\begin{aligned}\n\\ell_{n}\\left(\\theta_{0}\\right)-\\ell_{n}\\left(\\widehat{\\theta}\\right) & =\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\widehat{\\theta}\\right)'\\left(\\theta_{0}-\\widehat{\\theta}\\right)+\\frac{1}{2}\\left(\\theta_{0}-\\widehat{\\theta}\\right)'\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\theta_{0}-\\widehat{\\theta}\\right)+O\\left(\\left\\Vert \\widehat{\\theta}-\\theta_{0}\\right\\Vert _{2}^{3}\\right)\\\\\n& =\\frac{1}{2}\\left(\\widehat{\\theta}-\\theta_{0}\\right)'\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)+O\\left(\\left\\Vert \\widehat{\\theta}-\\theta_{0}\\right\\Vert _{2}^{3}\\right)\\\\\n& =\\frac{1}{2}\\left(\\widehat{\\theta}-\\theta_{0}\\right)'\\left(\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right)\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right)+o_{p}\\left(1\\right)\\end{aligned}\\]\n\n\\(\\frac{\\partial\\ell_{n}}{\\partial\\theta}\\left(\\widehat{\\theta}\\right)=0\\)\ndue first order condition optimality. Define\n\\(L_{n}\\left(\\theta\\right):=\\sum_{}\\log f\\left(x_{};\\theta\\right)\\), \nlikelihood-ratio statistic \n\\[\\mathcal{LR}:=2\\left(L_{n}\\left(\\widehat{\\theta}\\right)-L_{n}\\left(\\theta_{0}\\right)\\right)=2n\\left(\\ell_{n}\\left(\\widehat{\\theta}\\right)-\\ell_{n}\\left(\\theta_{0}\\right)\\right).\\]\nObviously \\(\\mathcal{LR}\\geq0\\) \\(\\widehat{\\theta}\\) maximizes\n\\(\\ell_{n}\\left(\\theta\\right)\\). Multiply \\(-2n\\) two sides \nTaylor expansion:\n\\[\\mathcal{LR}=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)'\\left(-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)+o_{p}\\left(1\\right)\\]\nNotice model correctly specified \n\\[\\begin{aligned}\n-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\theta_{0}\\right) & \\stackrel{p}{\\}-\\mathcal{H}\\left(\\theta_{0}\\right)=\\mathcal{}\\left(\\theta_{0}\\right)\\\\\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) & \\stackrel{d}{\\}N\\left(0,\\mathcal{}^{-1}\\left(\\theta_{0}\\right)\\right)\\end{aligned}\\]\nSlutsky’s theorem:\n\\[\\left(-\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta'}\\ell_{n}\\left(\\dot{\\theta}\\right)\\right)^{1/2}\\left[\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\right]\\stackrel{d}{\\}\\mathcal{}^{1/2}\\left(\\theta_{0}\\right)\\times N\\left(0,\\mathcal{}^{-1}\\left(\\theta_{0}\\right)\\right)\\sim N\\left(0,I_{k}\\right).\\]\n\\(\\mathcal{LR}\\stackrel{d}{\\}\\chi_{K}^{2}\\) continuous\nmapping theorem.Wilks’ theorem, Wilks’ phenomenon referred fact \n\\(\\mathcal{LR}\\stackrel{d}{\\}\\chi^{2}\\left(K\\right)\\) \nparametric model correctly specified.","code":""},{"path":"hypothesis-testing.html","id":"score-test","chapter":"9 Hypothesis Testing","heading":"9.1.4 Score Test","text":"","code":""},{"path":"hypothesis-testing.html","id":"confidence-intervalconfidence-interval","chapter":"9 Hypothesis Testing","heading":"9.2 Confidence Interval\\[confidence-interval\\]","text":"interval estimate function\n\\(C:\\mathcal{X}^{n}\\mapsto\\left\\{ \\Theta_{1}:\\Theta_{1}\\subseteq\\Theta\\right\\}\\)\nmaps point sample space subset parameter\nspace. coverage probability interval estimator\n\\(C\\left(\\mathbf{X}\\right)\\) defined \n\\(P_{\\theta}\\left(\\theta\\C\\left(\\mathbf{X}\\right)\\right)\\). \n\\(\\theta\\) one dimension, usually call interval estimator\nconfidence interval. \\(\\theta\\) multiple dimensions, call\nconfidence region course includes one-dimensional\n\\(\\theta\\) special case. coverage probability frequency\ninterval estimator captures true parameter generates\nsample. frequentist perspective, parameter fixed\nconfidence region random. probability \n\\(\\theta\\) inside given confidence interval.Suppose random sample size 6 generated \n\\(\\left(X_{1},\\ldots,X_{6}\\right)\\sim\\text{iid }N\\left(\\theta,1\\right).\\)\nFind coverage probability random interval \n\\(\\left[\\bar{X}-1.96/\\sqrt{6},\\ \\bar{X}+1.96/\\sqrt{6}\\right].\\)Hypothesis testing confidence region closely related. Sometimes\ndifficult directly construct confidence region, easy \ntest hypothesis. One way construct confidence region \ninverting test. Suppose \\(\\phi_{\\theta}\\) test size \\(\\alpha\\).\n\\(C\\left(\\mathbf{X}\\right)\\) constructed \n\\[C\\left(\\mathbf{X}\\right)=\\left\\{ \\theta\\\\Theta:\\phi\\left(\\mathbf{X}\\right)=0\\right\\} .\\]\ncoverage probability true data generating parameter \\(\\theta\\)\n\n\\[P_{\\theta}\\left\\{ \\theta\\C\\left(\\mathbf{X}\\right)\\right\\} =P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=0\\right\\} =1-P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=1\\right\\} =1-\\beta\\left(\\theta\\right)\\geq1-\\alpha\\]\nlast inequality follows \n\\(\\beta\\left(\\theta\\right)\\leq\\alpha\\) \\(\\theta\\\\Theta_{0}\\). \n\\(\\Theta_{0}\\) singleton, equality holds.knitr","code":""},{"path":"hypothesis-testing.html","id":"bayesian-credible-set","chapter":"9 Hypothesis Testing","heading":"9.3 Bayesian Credible Set","text":"Bayesian framework offers coherent natural language \nstatistical decision. However, major criticism Bayesian\nstatistics arbitrariness choice prior.Bayesian approach views data \\(\\mathbf{X}_{n}\\) \nparameter \\(\\theta\\) random variables. observes data,\nholds prior distribution \\(\\pi\\) \\(\\theta\\). observing\ndata, updates prior distribution posterior\ndistribution \\(p(\\theta|\\mathbf{X}_{n})\\). Bayes Theorem connects\nprior posterior \n\\[p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)\\]\n\\(f(\\mathbf{X}_{n}|\\theta)\\) likelihood function.classical example illustrate Bayesian approach \nstatistical inference. Suppose \\(\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})\\) \niid sample drawn normal distribution unknown \\(\\theta\\) \nknown \\(\\sigma\\). researcher’s prior distribution\n\\(\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})\\), posterior distribution\n, routine calculation, also normal distribution\n\\[p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),\\]\n\n\\(\\tilde{\\theta}=\\frac{\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\theta_{0}+\\frac{n\\sigma_{0}^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\bar{x}\\)\n\n\\(\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}\\).\nThus Bayesian credible set \n\\[\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).\\]\nposterior distribution depends \\(\\theta_{0}\\) \\(\\sigma_{0}^{2}\\)\nprior. sample size sufficiently large posterior\ncan approximated \\(N(\\bar{x},\\sigma^{2}/n)\\), prior\ninformation overwhelmed information accumulated data.contrast, frequentist estimate\n\\(\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)\\). confidence\ninterval \n\\[\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).\\]\nBayesian credible set frequentist confidence interval \ndifferent finite \\(n\\), coincide \\(n\\\\infty\\).","code":""},{"path":"hypothesis-testing.html","id":"applications-in-ols","chapter":"9 Hypothesis Testing","heading":"9.4 Applications in OLS","text":"introduce three tests hypothesis linear regression\ncoefficients, namely Wald test, Lagrangian multiplier (LM) test,\nlikelihood ratio test. Wald test based \nunrestricted OLS estimator \\(\\widehat{\\beta}\\). LM test based \nrestricted estimator \\(\\tilde{\\beta}\\). LRT, discussed,\nbased difference log-likelihood function evaluated \nunrestricted OLS estimator restricted estimator.Let \\(R\\) \\(q\\times K\\) constant matrix \\(q\\leq K\\) \n\\(\\mbox{rank}\\left(R\\right)=q\\). linear restrictions \\(\\beta\\) can\nwritten form \\(R\\beta=r\\), \\(r\\) \\(q\\times1\\) constant\nvector.want simultaneously test \\(\\beta_{1}=1\\) \\(\\beta_{3}+\\beta_{4}=2\\)\nexample. null hypothesis can expressed \ngeneral form \\(R\\beta=r\\), restriction matrix \\(R\\) \n\\[R=\\begin{pmatrix}1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 1 & 0\n\\end{pmatrix}\\] \\(r=\\left(1,2\\right)'\\).","code":""},{"path":"hypothesis-testing.html","id":"wald-test","chapter":"9 Hypothesis Testing","heading":"9.4.1 Wald Test","text":"Suppose OLS estimator \\(\\widehat{\\beta}\\) asymptotic normal, .e.\n\\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}N\\left(0,\\Omega\\right)\\]\n\\(\\Omega\\) \\(K\\times K\\) positive definite covariance matrix .\nSince\n\\(R\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}N\\left(0,R\\Omega R'\\right)\\),\nquadratic form\n\\[n\\left(\\widehat{\\beta}-\\beta\\right)'R'\\left(R\\Omega R'\\right)^{-1}R\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}\\chi_{q}^{2}.\\]\nNow intend test linear null hypothesis \\(R\\beta=r\\). \nnull, Wald statistic\n\\[\\mathcal{W}=n\\left(R\\widehat{\\beta}-r\\right)'\\left(R\\widehat{\\Omega}R'\\right)^{-1}\\left(R\\widehat{\\beta}-r\\right)\\stackrel{d}{\\}\\chi_{q}^{2}\\]\n\\(\\widehat{\\Omega}\\) consistent estimator \\(\\Omega\\).(Single test) linear regression\n\\[\\begin{aligned}y & =x_{}'\\beta+e_{}=\\sum_{k=1}^{5}\\beta_{k}x_{ik}+e_{}.\\nonumber\\\\\nE\\left[e_{}x_{}\\right] & =\\mathbf{0}_{5},\\label{eq:example}\n\\end{aligned}\\] \\(y\\) wage \n\\[x=\\left(\\mbox{edu},\\mbox{age},\\mbox{experience},\\mbox{experience}^{2},1\\right)'.\\]\ntest whether education affects wage, specify null\nhypothesis \\(\\beta_{1}=0\\). Let \\(R=\\left(1,0,0,0,0\\right)\\) \\(r=0\\).\n\\[\\sqrt{n}\\widehat{\\beta}_{1}=\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)=\\sqrt{n}R\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\}N\\left(0,R\\Omega R'\\right)\\sim N\\left(0,\\Omega_{11}\\right),\\label{eq:R11}\\]\n\\(\\Omega{}_{11}\\) \\(\\left(1,1\\right)\\) (scalar) element \n\\(\\Omega\\). \n\\[H_{0}:R\\beta=\\left(1,0,0,0,0\\right)\\left(\\beta_{1},\\ldots,\\beta_{5}\\right)'=\\beta_{1}=0,\\]\n\n\\(\\sqrt{n}R\\left(\\widehat{\\beta}-\\beta\\right)=\\sqrt{n}\\widehat{\\beta}_{1}\\stackrel{d}{\\}N\\left(0,\\Omega_{11}\\right).\\)\nTherefore,\n\\[\\sqrt{n}\\frac{\\widehat{\\beta}_{1}}{\\widehat{\\Omega}_{11}^{1/2}}=\\sqrt{\\frac{\\Omega_{11}}{\\widehat{\\Omega}_{11}}}\\sqrt{n}\\frac{\\widehat{\\beta}_{1}}{\\sqrt{\\Omega_{11}}}\\]\n\\(\\widehat{\\Omega}\\stackrel{p}{\\}\\Omega\\), \n\\(\\left(\\Omega_{11}/\\widehat{\\Omega}_{11}\\right)^{1/2}\\stackrel{p}{\\}1\\)\ncontinuous mapping theorem. \n\\(\\sqrt{n}\\widehat{\\beta}_{1}/\\Omega_{11}^{1/2}\\stackrel{d}{\\}N\\left(0,1\\right)\\),\nconclude\n\\(\\sqrt{n}\\widehat{\\beta}_{1}/\\widehat{\\Omega}_{11}^{1/2}\\stackrel{d}{\\}N\\left(0,1\\right).\\)example test single coefficient, test\nstatistic essentially square t-statistic, null\ndistribution square standard normal.order test nonlinear regression, use delta method.(good example can rewritten linear\nhypothesis.) example linear regression, optimal\nexperience level can found setting zero first order\ncondition respective experience,\n\\(\\beta_{3}+2\\beta_{4}\\mbox{experience}^{*}=0\\). test hypothesis\noptimal experience level 20 years; words,\n\\[\\mbox{experience}^{*}=-\\frac{\\beta_{3}}{2\\beta_{4}}=20.\\] \nnonlinear hypothesis. \\(q\\leq K\\) \\(q\\) number \nrestrictions, \n\\[n\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)'\\left(\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)\\Omega\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)'\\right)^{-1}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\}\\chi_{q}^{2},\\]\nexample, \\(\\theta=\\beta\\),\n\\(f\\left(\\beta\\right)=-\\beta_{3}/\\left(2\\beta_{4}\\right)\\). gradient\n\\[\\frac{\\partial f}{\\partial\\beta'}\\left(\\beta\\right)=\\left(0,0,-\\frac{1}{2\\beta_{4}},\\frac{\\beta_{3}}{2\\beta_{4}^{2}},0\\right)\\]\nSince \\(\\widehat{\\beta}\\stackrel{p}{\\}\\beta_{0}\\), continuous\nmapping theorem, \\(\\beta_{0,4}\\neq0\\), \n\\(\\frac{\\partial}{\\partial\\beta}f\\left(\\widehat{\\beta}\\right)\\stackrel{p}{\\}\\frac{\\partial}{\\partial\\beta}f\\left(\\beta_{0}\\right)\\).\nTherefore, (nonlinear) Wald test \n\\[\\mathcal{W}=n\\left(f\\left(\\widehat{\\beta}\\right)-20\\right)'\\left(\\frac{\\partial f}{\\partial\\beta'}\\left(\\widehat{\\beta}\\right)\\widehat{\\Omega}\\frac{\\partial f}{\\partial\\beta'}\\left(\\widehat{\\beta}\\right)\\right)^{-1}\\left(f\\left(\\widehat{\\beta}\\right)-20\\right)\\stackrel{d}{\\}\\chi_{1}^{2}.\\]\nvalid test correct asymptotic size.However, can equivalently state null hypothesis \n\\(\\beta_{3}+40\\beta_{4}=0\\) can construct Wald statistic\naccordingly. Asymptotically equivalent though, general linear\nhypothesis preferred nonlinear one, due approximation\nerror delta method null importantly \ninvalidity Taylor expansion alternative. also\nhighlights problem Wald test variant \nre-parametrization.","code":""},{"path":"hypothesis-testing.html","id":"lagrangian-multiplier-test","chapter":"9 Hypothesis Testing","heading":"9.4.2 Lagrangian Multiplier Test","text":"key difference Wald test LM test former\nbased unrestricted OLS estimator latter based \nrestricted OLS estimator. Estimate constrained OLS estimator\n\\[\\min_{\\beta}\\left(y-X\\beta\\right)'\\left(y-X\\beta\\right)\\mbox{ s.t. }R\\beta=r.\\]\nknow restricted minimization problem can converted \nunrestricted problem\n\\[L\\left(\\beta,\\lambda\\right)=\\frac{1}{2n}\\left(y-X\\beta\\right)'\\left(y-X\\beta\\right)+\\lambda'\\left(R\\beta-r\\right),\\label{eq:Lagran}\\]\n\\(L\\left(\\beta,\\lambda\\right)\\) called Lagrangian, \n\\(\\lambda\\) Lagrangian multiplier.LM test also called score test, derivation \nbased score function restricted OLS estimator. Set \nfirst-order condition \n\\[eq:Lagran\\] zero: \\[\\begin{aligned}\n\\frac{\\partial}{\\partial\\beta}L & =-\\frac{1}{n}X'\\left(y-X\\tilde{\\beta}\\right)+\\tilde{\\lambda}R=-\\frac{1}{n}X'e+\\frac{1}{n}X'X\\left(\\tilde{\\beta}-\\beta_{0}\\right)+R'\\tilde{\\lambda}=0.\\\\\n\\frac{\\partial}{\\partial\\lambda}L & =R\\tilde{\\beta}-r=R\\left(\\tilde{\\beta}-\\beta_{0}\\right)=0\\end{aligned}\\]\n\\(\\tilde{\\beta}\\) \\(\\tilde{\\lambda}\\) denote roots \nequation, \\(\\beta_{0}\\) hypothesized true value. two\nequations can written linear system\n\\[\\begin{pmatrix}\\widehat{Q} & R'\\\\\nR & 0\n\\end{pmatrix}\\begin{pmatrix}\\tilde{\\beta}-\\beta_{0}\\\\\n\\tilde{\\lambda}\n\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{n}X'e\\\\\n0\n\\end{pmatrix},\\] \\(\\hat{Q}=X'X/n\\).\\[\\begin{pmatrix}\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & \\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}\\\\\n\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & -(R'Q^{-1}R)^{-1}\n\\end{pmatrix}\\begin{pmatrix}\\widehat{Q} & R'\\\\\nR & 0\n\\end{pmatrix}=I_{K+q}.\\]Given fact, can explicitly express \\[\\begin{aligned}\n\\begin{pmatrix}\\tilde{\\beta}-\\beta_{0}\\\\\n\\tilde{\\lambda}\n\\end{pmatrix}\\begin{aligned}=\\end{aligned}\n& \\begin{pmatrix}\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & \\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}\\\\\n\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1} & -(R'Q^{-1}R)^{-1}\n\\end{pmatrix}\\begin{pmatrix}\\frac{1}{n}X'e\\\\\n0\n\\end{pmatrix}\\\\\n= & \\begin{pmatrix}\\widehat{Q}^{-1}\\frac{1}{n}X'e-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{n}X'e\\\\\n\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{n}X'e\n\\end{pmatrix}\\end{aligned}\\] \\(\\tilde{\\lambda}\\) component \n\\[\\begin{aligned}\n\\sqrt{n}\\tilde{\\lambda} & =\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\\\\n& \\stackrel{d}{\\}N\\left(0,\\left(RQ^{-1}R'\\right)^{-1}RQ^{-1}\\Omega Q^{-1}R'\\left(RQ^{-1}R'\\right)^{-1}\\right)\\end{aligned}\\]\n\\(\\widehat{Q}\\stackrel{p}{\\}Q\\). Denote\n\\(\\Sigma=\\left(RQ^{-1}R'\\right)^{-1}RQ^{-1}\\Omega Q^{-1}R'\\left(RQ^{-1}R'\\right)^{-1}\\),\n\n\\[n\\tilde{\\lambda}'\\Sigma^{-1}\\tilde{\\lambda}\\stackrel{d}{\\}\\chi_{q}^{2}.\\]\nLet\n\\[\\widehat{\\Sigma}=\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\widehat{\\Omega}\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}.\\]\n\\(\\widehat{\\Omega}\\stackrel{p}{\\}\\Omega\\), \\[\\begin{aligned}\n\\mathcal{LM} & =n\\tilde{\\lambda}'\\widehat{\\Sigma}^{-1}\\tilde{\\lambda}=n\\tilde{\\lambda}'\\Sigma^{-1}\\tilde{\\lambda}+n\\tilde{\\lambda}'\\left(\\widehat{\\Sigma}^{-1}-\\Sigma^{-1}\\right)\\tilde{\\lambda}\\\\\n& =n\\tilde{\\lambda}'\\Sigma^{-1}\\tilde{\\lambda}+o_{p}\\left(1\\right)\\stackrel{d}{\\}\\chi_{q}^{2}.\\end{aligned}\\]\ngeneral expression LM test.special case homoskedasticity,\n\\(\\Sigma=\\sigma^{2}\\left(RQ^{-1}R'\\right)^{-1}RQ^{-1}QQ^{-1}R'\\left(RQ^{-1}R'\\right)^{-1}=\\sigma^{2}\\left(RQ^{-1}R'\\right)^{-1}.\\)\nReplace \\(\\Sigma\\) estimated \\(\\hat{\\Sigma}\\), \n\\[\\begin{aligned}\\frac{n\\tilde{\\lambda}'R\\hat{Q}^{-1}R'\\tilde{\\lambda}}{\\hat{\\sigma}^{2}} & =\\frac{1}{n\\hat{\\sigma}^{2}}\\left(y-X\\tilde{\\beta}\\right)'X\\hat{Q}^{-1}R'(R\\hat{Q}^{-1}R')^{-1}R\\hat{Q}^{-1}X'\\left(y-X\\tilde{\\beta}\\right)\\stackrel{d}{\\}\\chi_{q}^{2}.\\end{aligned}\\]test hypothesis optimal experience level 20 years;\n\\(\\mbox{experience}^{*}=-\\frac{\\beta_{3}}{2\\beta_{4}}=20.\\) can replace\n\\(\\beta_{3}\\) \\(-40\\beta_{4}\\) need estimate 3 slope\ncoefficients OLS construct LM test. Moreover, LM test\ninvariant re-parametrization.","code":""},{"path":"hypothesis-testing.html","id":"likelihood-ratio-test-for-regression","chapter":"9 Hypothesis Testing","heading":"9.4.3 Likelihood-Ratio Test for Regression","text":"previous section discussed LRT. put \ncontext regression Gaussian error. Let \\(\\gamma=\\sigma_{e}^{2}\\).\nclassical assumptions normal regression model,\n\\[L_{n}\\left(\\beta,\\gamma\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\gamma-\\frac{1}{2\\gamma}\\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right).\\]\nunrestricted estimator, know\n\\[\\widehat{\\gamma}=\\gamma\\left(\\widehat{\\beta}\\right)=n^{-1}\\left(Y-X\\widehat{\\beta}\\right)'\\left(Y-X\\widehat{\\beta}\\right)\\]\nsample log-likelihood function evaluated MLE \n\\[\\widehat{L}_{n}=L_{n}\\left(\\widehat{\\beta},\\widehat{\\gamma}\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\widehat{\\gamma}-\\frac{n}{2}\\]\nrestricted estimator\n\\(\\tilde{L}_{n}=L_{n}\\left(\\tilde{\\beta},\\tilde{\\gamma}\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{n}{2}\\log\\tilde{\\gamma}-\\frac{n}{2}\\).\nlikelihood ratio \\[\\begin{aligned}\n\\mathcal{LR} & =2\\left(\\widehat{L}_{n}-\\tilde{L}_{n}\\right)=n\\log\\left(\\tilde{\\gamma}/\\widehat{\\gamma}\\right).\\end{aligned}\\]\nnormal regression correctly specified, can immediately\nconclude \\(\\mathcal{LR}\\stackrel{d}{\\}\\chi_{q}^{2}.\\)Now drop Gaussian error assumption keep conditional\nhomoskedasticity. case, classical results applicable\n\\(L_{n}\\left(\\beta,\\gamma\\right)\\) (genuine)\nlog-likelihood function; instead quasi log-likelihood\nfunction. Notice \\[\\begin{aligned}\n\\mathcal{LR} & =n\\log\\left(1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}\\right)=n\\left(\\log1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}+O\\left(\\frac{\\left|\\tilde{\\gamma}-\\widehat{\\gamma}\\right|^{2}}{\\widehat{\\gamma}^{2}}\\right)\\right)\\nonumber \\\\\n& =n\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}+o_{p}\\left(1\\right)\\label{eq:LRT1}\\end{aligned}\\]\nTaylor expansion \n\\(\\log\\left(1+\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\widehat{\\gamma}}\\right)\\)\naround \\(\\log1=0\\). focus \\[\\begin{aligned}\nn\\left(\\tilde{\\gamma}-\\widehat{\\gamma}\\right) & =n\\left(\\gamma\\left(\\tilde{\\beta}\\right)-\\gamma\\left(\\widehat{\\beta}\\right)\\right)\\nonumber \\\\\n& =n\\left(\\frac{\\partial\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+\\frac{1}{2}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)'\\frac{\\partial^{2}\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta\\partial\\beta'}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+O\\left(\\left\\Vert \\tilde{\\beta}-\\widehat{\\beta}\\right\\Vert _{2}^{3}\\right)\\right)\\nonumber \\\\\n& =\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)'\\widehat{Q}\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)+o_{p}\\left(1\\right)\\label{eq:LRT2}\\end{aligned}\\]\nlast line follows \n\\(\\frac{\\partial\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta}=-\\frac{2}{n}X'\\left(Y-X\\widehat{\\beta}\\right)=-\\frac{2}{n}X'\\widehat{e}=0\\)\n\n\\(\\frac{1}{2}\\cdot\\frac{\\partial^{2}\\gamma\\left(\\widehat{\\beta}\\right)}{\\partial\\beta\\partial\\beta'}=\\frac{1}{2}\\cdot\\frac{2}{n}X'X=\\widehat{Q}\\).derivation LM test, \n\\[\\begin{aligned}\\sqrt{n}\\left(\\tilde{\\beta}-\\beta_{0}\\right) & =\\left(\\widehat{Q}^{-1}-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\right)\\frac{1}{\\sqrt{n}}X'e\\\\\n& =\\frac{1}{\\sqrt{n}}\\left(X'X\\right)^{-1}X'e-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\\\\n& =\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{0}\\right)-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e.\n\\end{aligned}\\] Rearrange equation obtain\n\\[\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)=-\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\]\nthus quadratic form \\[\\begin{aligned}\n&  & \\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)'\\widehat{Q}\\sqrt{n}\\left(\\tilde{\\beta}-\\widehat{\\beta}\\right)\\nonumber \\\\\n& = & \\frac{1}{\\sqrt{n}}e'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\widehat{Q}\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\nonumber \\\\\n& = & \\frac{1}{\\sqrt{n}}e'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e\\nonumber \\\\\n& = & \\frac{1}{\\sqrt{n}}e'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'e.\\label{eq:LRT3}\\end{aligned}\\]\nCollecting (\\[eq:LRT1\\]), (\\[eq:LRT2\\]) (\\[eq:LRT3\\]), \\[\\begin{aligned}\n\\mathcal{LR} & =n\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\cdot\\frac{\\tilde{\\gamma}-\\widehat{\\gamma}}{\\sigma_{e}^{2}}+o_{p}\\left(1\\right)\\\\\n& =\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\frac{1}{\\sqrt{n}}\\frac{e}{\\sigma_{e}}'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}}+o_{p}\\left(1\\right)\\end{aligned}\\]\nNotice homoskedasticity, CLT gives \\[\\begin{aligned}\nR\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}} & =R\\widehat{Q}^{-1/2}\\widehat{Q}^{-1/2}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}}\\\\\n& \\stackrel{d}{\\}RQ^{-1/2}\\times N\\left(0,I_{K}\\right)\\sim N\\left(0,RQ^{-1}R'\\right),\\end{aligned}\\]\nthus\n\\[\\frac{1}{\\sqrt{n}}\\frac{e}{\\sigma_{e}}'X\\widehat{Q}^{-1}R'\\left(R\\widehat{Q}^{-1}R'\\right)^{-1}R\\widehat{Q}^{-1}\\frac{1}{\\sqrt{n}}X'\\frac{e}{\\sigma_{e}}\\stackrel{d}{\\}\\chi_{q}^{2}.\\]\nMoreover, \\(\\frac{\\sigma_{e}^{2}}{\\widehat{\\gamma}}\\stackrel{p}{\\}1\\).\nSlutsky’s theorem, conclude\n\\[\\mathcal{LR}\\stackrel{d}{\\}\\chi_{q}^{2}.\\] homoskedasticity.","code":""},{"path":"hypothesis-testing.html","id":"summary-7","chapter":"9 Hypothesis Testing","heading":"9.5 Summary","text":"Applied econometrics field obsessed hypothesis testing, \nhope establish least statistical association ideally\ncausality. Hypothesis testing fundamentally important topic \nstatistics. states decisions Table\n\\[tab:Decisions--States\\] remind us intrinsic\nconnections game theory economics. , game player, plays \nsequential game “nature”.Step0:\nparameter space \\(\\Theta\\) partitioned null hypothesis\n\\(\\Theta_{0}\\) alternative hypothesis \\(\\Theta_{1}\\) according\nscientific theory.\nparameter space \\(\\Theta\\) partitioned null hypothesis\n\\(\\Theta_{0}\\) alternative hypothesis \\(\\Theta_{1}\\) according\nscientific theory.Step1:\nobserve data, design test function \\(\\phi\\) according\n\\(\\Theta_{0}\\) \\(\\Theta_{1}\\). game theory terminology, \ncontingency plan \\(\\phi\\) strategy.\nobserve data, design test function \\(\\phi\\) according\n\\(\\Theta_{0}\\) \\(\\Theta_{1}\\). game theory terminology, \ncontingency plan \\(\\phi\\) strategy.Step2:\nobserve fixed data \\(\\mathbf{x}\\), act according \ninstruction \\(\\phi\\left(\\mathbf{x}\\right)\\) — either accept\n\\(\\Theta_{0}\\) reject \\(\\Theta_{0}\\).\nobserve fixed data \\(\\mathbf{x}\\), act according \ninstruction \\(\\phi\\left(\\mathbf{x}\\right)\\) — either accept\n\\(\\Theta_{0}\\) reject \\(\\Theta_{0}\\).Step3:\nNature reveals true parameter \\(\\theta^{*}\\) behind \\(\\mathbf{x}\\).\ncan evaluate gain/loss decision\n\\(\\phi\\left(\\mathbf{x}\\right)\\).\nNature reveals true parameter \\(\\theta^{*}\\) behind \\(\\mathbf{x}\\).\ncan evaluate gain/loss decision\n\\(\\phi\\left(\\mathbf{x}\\right)\\).loss function (negative payoff) specified \n\\[\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)=\\phi\\left(\\mathbf{x}\\right)\\cdot1\\left\\{ \\theta\\\\Theta_{0}\\right\\} +\\left(1-\\phi\\left(\\mathbf{x}\\right)\\right)\\cdot1\\left\\{ \\theta\\\\Theta_{1}\\right\\} ,\\]\nrandomness data incur risk (expected loss)\n\\[\\mathscr{R}\\left(\\theta,\\phi\\right)=E\\left[\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)\\right]=\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\\\Theta_{1}\\right\\} .\\]\nrational person. understand structure game \nwant good job Step 1 designing strategy. want \nminimize risk.frequentist, one one \n\\(1\\left\\{ \\theta\\\\Theta_{0}\\right\\}\\) \n\\(1\\left\\{ \\theta\\\\Theta_{1}\\right\\}\\) can happen. unbiased test\nmakes sure\n\\(\\sup_{\\theta\\\\Theta_{0}}\\beta_{\\phi}\\left(\\theta\\right)\\leq\\alpha\\).\nmany tests unbiased, ideally like pick best one.\nexists, class \\(\\Psi_{\\alpha}\\) unbiased tests size\n\\(\\alpha\\) uniformly power test \\(\\phi^{*}\\) satisfies\n\\(\\mathscr{R}\\left(\\theta,\\phi^{*}\\right)\\geq\\sup_{\\phi\\\\Psi_{\\alpha}}\\mathscr{R}\\left(\\theta,\\phi\\right)\\)\nevery \\(\\theta\\\\Theta_{1}\\). simple versus simple tests, LRT \nuniformly powerful test according Neyman-Pearson Lemma.Bayesian, mind imposing probability (weight) \nparameter space, prior belief \\(\\pi\\left(\\theta\\right)\\). \nBayesian risk becomes \\[\\begin{aligned}\n\\mathscr{BR}\\left(\\pi,\\phi\\right) & =E_{\\pi\\left(\\theta\\right)}\\left[\\mathscr{R}\\left(\\theta,\\phi\\right)\\right]=\\int\\left[\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\\\Theta_{1}\\right\\} \\right]\\pi\\left(\\theta\\right)d\\theta\\\\\n& =\\int_{\\left\\{ \\theta\\\\Theta_{0}\\right\\} }\\beta_{\\phi}\\left(\\theta\\right)\\pi\\left(\\theta\\right)d\\theta+\\int_{\\left\\{ \\theta\\\\Theta_{1}\\right\\} }(1-\\beta_{\\phi}\\left(\\theta\\right))\\pi\\left(\\theta\\right)d\\theta.\\end{aligned}\\]\naverage (respect \\(\\pi\\left(\\theta\\right)\\)) risk \nnull alternative.Historical notes: Hypothesis testing started take modern\nshape beginning 20th century. Karl Pearson (1957–1936)\nlaid foundation hypothesis testing introduced \\(\\chi^{2}\\)\ntest, \\(p\\)-value, among many concepts keep using today.\nNeyman-Pearson Lemma named Jerzy Neyman (1894–1981) Egon\nPearson (1895–1980), Karl’s son.reading: Young Smith (2005) concise -depth\nreference statistical inference.","code":""},{"path":"hypothesis-testing.html","id":"appendix-2","chapter":"9 Hypothesis Testing","heading":"9.6 Appendix","text":"","code":""},{"path":"hypothesis-testing.html","id":"neyman-pearson-lemma","chapter":"9 Hypothesis Testing","heading":"9.6.1 Neyman-Pearson Lemma","text":"discussed example uniformly power test \nGaussian location model. likelihood principle, test \nsimple versus simple (null hypothesis singleton \\(\\theta_{0}\\)\nalternative hypothesis another single point \\(\\theta_{1}\\)),\nLRT \\[\\begin{aligned}\n\\phi\\left(\\mathbf{X}\\right) & :=1\\left\\{ \\mathcal{LR}\\geq c_{LR}\\right\\} ,\\end{aligned}\\]\n\\(c_{LR}\\) critical value depending size \ntest, uniformly powerful test. result well-known\nNeyman-Pearson Lemma.Notice\n\\(\\exp\\left(L_{n}\\left(\\theta\\right)\\right)=\\Pi_{}f\\left(x_{};\\theta\\right)=f\\left(\\mathbf{x};\\theta\\right)\\)\n\\(f\\left(\\mathbf{x};\\theta_{0}\\right)\\) joint density \n\\(\\left(x_{1},\\ldots,x_{n}\\right)\\), LRT can equivalently written\nlikelihood ratio form (without log)\n\\[\\phi\\left(\\mathbf{X}\\right)=1\\left\\{ f\\left(\\mathbf{X};\\widehat{\\theta}\\right)/f\\left(\\mathbf{X};\\theta_{0}\\right)\\geq c\\right\\}\\]\n\\(c:=\\exp\\left(c_{LR}/2\\right)\\).see power test simple simple context,\nconsider another test \\(\\psi\\) size single null\nhypothesis\n\\(\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\theta_{0}\\right)=\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)=\\alpha\\),\n\\(f\\left(\\mathbf{x};\\theta_{0}\\right)=\\) joint density \nsample \\(\\mathbf{X}\\). constant \\(c>0\\), power \\(\\phi\\) \nalternative \\(\\theta_{1}\\) \\[\\begin{aligned}\nE_{\\theta_{1}}\\left[\\phi\\left(\\mathbf{X}\\right)\\right] & =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)\\nonumber \\\\\n& =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)-c\\left[\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)-\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\right]\\nonumber \\\\\n& =\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)-c\\int\\phi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\nonumber \\\\\n& =\\int\\phi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right).\\label{eq:NP1}\\end{aligned}\\]\nDefine\n\\(\\xi_{c}:=f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\).\nfact \\(\\phi\\left(\\mathbf{x}\\right)=1\\) \\(\\xi_{c}\\geq0\\) \n\\(\\phi\\left(\\mathbf{x}\\right)=0\\) \\(\\xi_{c}<0\\) implies \\[\\begin{aligned}\n&  & \\int\\phi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)=\\int\\phi\\left(\\mathbf{x}\\right)\\xi_{c}\\\\\n& = & \\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\phi\\left(\\mathbf{x}\\right)\\xi_{c}+\\int_{\\left\\{ \\xi_{c}<0\\right\\} }\\phi\\left(\\mathbf{x}\\right)\\xi_{c}=\\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\xi_{c}=\\int\\xi_{c}\\cdot1\\left\\{ \\xi_{c}\\geq0\\right\\} \\\\\n& \\geq & \\int\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\cdot1\\left\\{ \\xi_{c}\\geq0\\right\\} =\\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\\\\n& \\geq & \\int_{\\left\\{ \\xi_{c}\\geq0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}+\\int_{\\left\\{ \\xi_{c}<0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}=\\int\\psi\\left(x\\right)\\xi_{c}\\\\\n& = & \\int\\psi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)\\end{aligned}\\]\nfirst inequality follows test function\n\\(0\\leq\\psi\\left(\\mathbf{x}\\right)\\leq1\\) realization \n\\(\\mathbf{x}\\), second inequality holds \n\\(\\int_{\\left\\{ \\xi_{c}<0\\right\\} }\\psi\\left(\\mathbf{x}\\right)\\xi_{c}\\leq0\\).\ncontinue \\[eq:NP1\\]: \\[\\begin{aligned}\nE_{\\theta_{1}}\\left[\\phi\\left(\\mathbf{X}\\right)\\right] & \\geq & \\int\\psi\\left(\\mathbf{x}\\right)\\left(f\\left(\\mathbf{x};\\theta_{1}\\right)-cf\\left(\\mathbf{x};\\theta_{0}\\right)\\right)+c\\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{0}\\right)\\\\\n& = & \\int\\psi\\left(\\mathbf{x}\\right)f\\left(\\mathbf{x};\\theta_{1}\\right)=E_{\\theta_{1}}\\left[\\psi\\left(\\mathbf{X}\\right)\\right].\\end{aligned}\\]\nwords, \\(\\phi\\left(\\mathbf{X}\\right)\\) powerful \n\\(\\theta_{1}\\) test \\(\\psi\\) size null.Neyman-Pearson lemma establishes optimality LRT single versus\nsimple hypothesis testing. can generalized show existence\nuniformly power test one sided composite null hypothesis\n\\(H_{0}:\\theta\\leq\\theta_{0}\\) \\(H_{0}:\\theta\\geq\\theta_{0}\\) \nparametric class distributions exhibiting monotone likelihood\nratio.Zhentao Shi. Nov 4, 2020.","code":""},{"path":"panel-data.html","id":"panel-data","chapter":"10 Panel Data","heading":"10 Panel Data","text":"Economists mostly work observational data. data generation\nprocess control researchers. cross\nsectional dataset hand, difficult deal heterogeneity\namong individuals. hand, panel data offers chance \nhandle heterogeneity particular forms.panel dataset tracks individuals across time \\(t=1,\\ldots,T\\).\nassume observations independent across \\(=1,\\ldots,n\\), \nallow form dependence within group across \\(t=1,\\ldots,T\\)\n\\(\\). maintain linear equation\n\\[y_{}=\\beta_{1}+x_{}'\\beta_{2}+u_{},\\ =1,\\ldots,n;t=1,\\ldots,T\\label{eq:basic_eq}\\]\n\\(u_{}=\\alpha_{}+\\epsilon_{}\\) called composite error.\nNote \\(\\alpha_{}\\) time-invariant unobserved heterogeneity,\n\\(\\epsilon_{}\\) varies across individuals time periods.important techniques panel data estimation fixed\neffect regression random effect regression. asymptotic\ndistributions estimators can derived knowledge \nOLS regression. sense, panel data estimation becomes applied\nexamples theory covered course. \nhighlights fundamental role linear regression theory \neconometrics.","code":""},{"path":"panel-data.html","id":"fixed-effect","chapter":"10 Panel Data","heading":"10.1 Fixed Effect","text":"unobservable individual-specific heterogeneity \\(\\alpha_{}\\) \nabsorbed composite error \\(u_{}=\\alpha_{}+\\epsilon_{}\\). \n\\(\\mathrm{cov}\\left(\\alpha_{},x_{}\\right)=0\\), OLS consistent;\notherwise consistency breaks . fixed effect model allows\n\\(\\alpha_{}\\) \\(x_{}\\) arbitrarily correlated. Let us rewrite\n(\\[eq:basic\\_eq\\]) \n\\[y_{}=w_{}'\\boldsymbol{\\beta}+u_{},\\label{eq:basic_eq2}\\] \n\\(\\boldsymbol{\\beta}=\\left(\\beta_{1},\\beta_{2}'\\right)'\\) \n\\(w_{}=\\left(1,x_{}'\\right)'\\) \\(K+1\\) vectors, .e.,\n\\(\\boldsymbol{\\beta}\\) parameter including intercept, \n\\(w_{}\\) explanatory variables including constant.Show \\(\\mathrm{cov}\\left(\\alpha_{},x_{}\\right)\\neq0\\), running OLS\n(\\[eq:basic\\_eq2\\]) deliver inconsistent estimator.naively run OLS fixed effect model inconsistent, \ntrick regain consistency eliminate \\(\\alpha_{}\\). section\ndevelops consistency asymptotic distribution within\nestimator, default fixed-effect (FE) estimator. within\nestimator transforms data subtracting observable\nvariables corresponding group means. Averaging \\(T\\) equations\noriginal regression \\(\\), \n\\[\\overline{y}_{}=\\beta_{1}+\\overline{x}_{}'\\beta_{2}+\\bar{u}_{}=\\beta_{1}+\\overline{x}_{}'\\beta_{2}+\\alpha_{}+\\bar{\\epsilon}_{}.\\label{eq:group_mean}\\]\n\\(\\overline{y}_{}=\\frac{1}{T}\\sum_{t=1}^{T}y_{}\\). Subtracting\naveraged equation original equation gives\n\\[\\tilde{y}_{}=\\tilde{x}_{}'\\beta_{2}+\\tilde{\\epsilon}_{}\\label{eq:FE_demean}\\]\n\\(\\tilde{y}_{}=y_{}-\\overline{y}_{}\\). run OLS \ndemeaned data, obtain within estimator\n\\[\\widehat{\\beta}_{2}^{FE}=\\left(\\tilde{X}'\\tilde{X}\\right)^{-1}\\tilde{X}'\\tilde{y},\\]\n\\(\\tilde{y}=\\left(y_{}\\right)_{,t}\\) stacks \\(nT\\)\nobservations vector, similarly defined \\(\\tilde{X}\\) \n\\(nT\\times K\\) matrix, \\(K\\) dimension \\(\\beta_{2}\\).know OLS consistent \n\\(E\\left[\\tilde{\\epsilon}_{}|\\tilde{x}_{}\\right]=0\\). provide\nsufficient condition, often called strict exogeneity.Assumption FE.1\n\\(E\\left[\\epsilon_{}|\\alpha_{},\\mathbf{x}_{}\\right]=0\\) \n\\(\\mathbf{x}_{}=\\left(x_{i1},\\ldots,x_{}\\right)\\).strictness relative contemporary exogeneity\n\\(E\\left[\\epsilon_{}|\\alpha_{},x_{}\\right]=0\\). FE.1 \nrestrictive assumes error \\(\\epsilon_{}\\) mean\nindependent past, present future explanatory variables.talk consistency panel data, typically \nconsidering \\(n\\\\infty\\) \\(T\\) stays fixed. asymptotic\nframework appropriate panel datasets many individuals \ntime periods.Proposition FE.1 satisfied, \\(\\widehat{\\beta}_{2}^{FE}\\) \nconsistent.variance estimation FE estimator little bit tricky. \nassume homoskedasitcity condition simplify calculation.\nViolation assumption changes form asymptotic\nvariance, jeopardize asymptotic normality.Assumption FE.2\n\\(\\mathrm{var}\\left(\\epsilon_{}|\\alpha_{},\\mathbf{x}_{}\\right)=\\sigma_{\\epsilon}^{2}I_{T}\\)\n\\(\\).FE.1 FE.2,\n\\(\\widehat{\\sigma}_{\\epsilon}^{2}=\\frac{1}{n\\left(T-1\\right)}\\sum_{=1}^{n}\\sum_{t=1}^{T}\\widehat{\\tilde{\\epsilon}}_{}^{2}\\)\nconsistent estimator \\(\\sigma_{\\epsilon}^{2}\\), \n\\(\\widehat{\\tilde{\\epsilon}}=\\tilde{y}_{}-\\tilde{x}_{}'\\widehat{\\beta}_{2}^{FE}\\).\nNote denominator \\(n\\left(T-1\\right)\\), \\(nT\\). \nnecessity adjusting degree freedom can easily seen \nFWL theorem: FE estimator slope coefficient numerical \ncounterpart full regression dummy variable \ncross sectional unit.FE.1 FE.2 satisfied, \n\\[\\left(\\widehat{\\sigma}_{\\epsilon}^{2}\\left(\\tilde{X}'\\tilde{X}\\right)^{-1}\\right)^{-1/2}\\left(\\widehat{\\beta}_{2}^{FE}-\\beta_{2}^{0}\\right)\\stackrel{d}{\\}N\\left(0,I_{K}\\right).\\]Let \\(M_{\\iota}=I_{T}-\\frac{1}{T}\\iota_{T}\\iota_{T}'\\) within-group\ndemeaner, \\(M=I_{n}\\otimes M_{\\iota}\\) (“\\(\\otimes\\)” denotes \nKronecker product). FE estimator can explicitly written \n\\[\\widehat{\\beta}_{2}^{FE}=\\left(\\tilde{X}'\\tilde{X}\\right)^{-1}\\tilde{X}'\\tilde{Y}=\\left(X'MX\\right)^{-1}X'.\\]\n\n\\[\\sqrt{nT}\\left(\\widehat{\\beta}_{2}^{FE}-\\beta_{2}^{0}\\right)=\\left(\\frac{X'MX}{nT}\\right)^{-1}\\frac{X'M\\epsilon}{\\sqrt{nT}}=\\left(\\frac{\\tilde{X}'\\tilde{X}}{nT}\\right)^{-1}\\frac{\\tilde{X}'\\epsilon}{\\sqrt{nT}}\\]\nSince \\[\\begin{aligned}\n\\mathrm{var}\\left(\\frac{\\tilde{X}'\\epsilon}{\\sqrt{nT}}|X\\right) & =\\frac{1}{nT}E\\left(X'M\\epsilon\\epsilon'MX|X\\right)=\\frac{1}{nT}X'\\left(\\epsilon\\epsilon'|X\\right)MX=\\left(\\frac{\\tilde{X}'\\tilde{X}}{nT}\\right)\\sigma^{2},\\end{aligned}\\]\napply law large numbers conclude\n\\[\\left(\\tilde{X}'\\tilde{X}\\right)^{1/2}\\left(\\widehat{\\beta}_{2}^{FE}-\\beta_{2}^{0}\\right)\\stackrel{d}{\\}N\\left(0,\\sigma_{\\epsilon}^{2}I_{K}\\right).\\]\nsimplicity, suppose can direct observe \\(\\tilde{\\epsilon}_{}\\).\n\\[\\begin{aligned}\n\\frac{1}{n\\left(T-1\\right)}E\\left[\\sum_{=1}^{n}\\sum_{t=1}^{T}\\tilde{\\epsilon}_{}^{2}\\right] & =\\frac{1}{n}\\sum_{=1}^{n}\\frac{1}{T-1}E\\left[\\epsilon_{}'M_{\\iota}\\epsilon_{}\\right]\\\\\n& =\\frac{1}{n}\\sum_{=1}^{n}\\frac{1}{T-1}\\mathrm{tr}\\left(E\\left[M_{\\iota}E\\left[\\epsilon_{}\\epsilon_{}'|\\mathbf{x}_{}\\right]\\right]\\right)\\\\\n& =\\frac{\\sigma_{\\epsilon}^{2}}{n}\\sum_{=1}^{n}\\frac{1}{T-1}\\mathrm{tr}\\left(M_{\\iota}\\right)=\\sigma_{\\epsilon}^{2}.\\end{aligned}\\]\nAlthough reality observe \\(\\widehat{\\tilde{\\epsilon}}_{}\\),\ncan show estimation error \n\\(\\widehat{\\tilde{\\epsilon}}_{}\\) \\(\\tilde{\\epsilon}_{}\\) \nnegligible. Thus law large numbers\n\\[\\widehat{\\sigma}_{\\epsilon}^{2}=\\frac{1}{n\\left(T-1\\right)}\\sum_{=1}^{n}\\sum_{t=1}^{T}\\widehat{\\tilde{\\epsilon}}_{}^{2}\\stackrel{d}{\\}\\frac{1}{n\\left(T-1\\right)}E\\left[\\sum_{=1}^{n}\\sum_{t=1}^{T}\\tilde{\\epsilon}_{}^{2}\\right]=\\sigma_{\\epsilon}^{2}\\]\nconsistent estimator variance. stated conclusion\nfollows.implicitly assume regularity conditions allow us invoke law\nlarge numbers central limit theorem. ignore technical\ndetails .important notice within-group demean FE eliminates\ntime-invariant explanatory variables, including intercept.\nTherefore FE obtain coefficient estimates \ntime-invariant variables.","code":""},{"path":"panel-data.html","id":"random-effect","chapter":"10 Panel Data","heading":"10.2 Random Effect","text":"random effect estimator pursues efficiency knife-edge special\ncase \\(\\mathrm{cov}\\left(\\alpha_{},x_{}\\right)=0\\). mentioned ,\nFE consistent \\(\\alpha_{}\\) \\(x_{}\\) uncorrelated.\nHowever, inspection covariance matrix reveals OLS \ninefficient.starting point original model, assumeAssumption RE.1\n\\(E\\left[\\epsilon_{}|\\alpha_{},\\mathbf{x}_{}\\right]=0\\) \n\\(E\\left[\\alpha_{}|\\mathbf{x}_{}\\right]=0\\).RE.1 obviously implies \\(\\mathrm{cov}\\left(\\alpha_{},x_{}\\right)=0\\),\n\n\\[S=\\mathrm{var}\\left(u_{}|\\mathbf{x}_{}\\right)=\\sigma_{\\alpha}^{2}\\mathbf{1}_{T}\\mathbf{1}_{T}'+\\sigma_{\\epsilon}^{2}I_{T},\\ \\mbox{}=1,\\ldots,n.\\]\ncovariance matrix scalar multiplication \nidentity matrix, OLS inefficient.mentioned , FE estimation kills time-invariant regressors.\ncontrast, RE allows time-invariant explanatory variables. \ninfeasible GLS estimator \n\\[\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{infeasible}}^{RE}=\\left(\\sum_{=1}^{n}\\mathbf{w}_{}'S^{-1}\\mathbf{w}_{}\\right)^{-1}\\sum_{=1}^{n}\\mathbf{w}_{}'S^{-1}\\mathbf{y}_{}=\\left(W'\\mathbf{S}^{-1}W\\right)^{-1}W'\\mathbf{S}^{-1}y\\]\n\\(\\mathbf{S}=I_{T}\\otimes S\\). practice, \\(\\sigma_{\\alpha}^{2}\\)\n\\(\\sigma_{\\epsilon}^{2}\\) \\(S\\) unknown, seek consistent\nestimators. , impose simplifying assumption parallel FE.2.Assumption RE.2\n\\(\\mathrm{var}\\left(\\epsilon_{}|\\mathbf{x}_{},\\alpha_{}\\right)=\\sigma_{\\epsilon}^{2}I_{T}\\)\n\n\\(\\mathrm{var}\\left(\\alpha_{}|\\mathbf{x}_{}\\right)=\\sigma_{\\alpha}^{2}.\\)assumption, can consistently estimate variances \nresiduals\n\\(\\widehat{u}_{}=y_{}-x_{}'\\widehat{\\boldsymbol{\\beta}}^{RE}\\). \n\n\\[\\begin{aligned}\\widehat{\\sigma}_{u}^{2} & =\\frac{1}{nT}\\sum_{=1}^{n}\\sum_{t=1}^{T}\\widehat{u}_{}^{2}\\\\\n\\widehat{\\sigma}_{\\alpha}^{2} & =\\frac{1}{n}\\sum_{=1}^{n}\\frac{1}{T\\left(T-1\\right)}\\sum_{t=1}^{T}\\sum_{r=1}^{T}\\sum_{r\\neq t}\\widehat{u}_{}\\widehat{u}_{ir}.\n\\end{aligned}\\] Given estimated variance covariance, can\nconstruct\n\\(\\widehat{\\mathbf{S}}=\\left(\\widehat{\\sigma}_{u}^{2}-\\widehat{\\sigma}_{\\epsilon}^{2}\\right)\\cdot I_{T}+\\widehat{\\sigma}_{\\epsilon}^{2}\\cdot\\boldsymbol{1}_{T}\\boldsymbol{1}_{T}'\\)\nfollows feasible GLS (FGLS)\n\\[\\widehat{\\boldsymbol{\\beta}}^{RE}=\\left(W'\\mathbf{\\widehat{S}}^{-1}W\\right)^{-1}W'\\widehat{\\mathbf{S}}^{-1}y\\]Show RE.1 RE.2 satisfied, \n\\[\\left(\\widehat{\\sigma}_{u}^{2}\\left(W'\\widehat{\\mathbf{S}}^{-1}W\\right)^{-1}\\right)^{-1/2}\\left(\\widehat{\\boldsymbol{\\beta}}^{RE}-\\boldsymbol{\\beta}_{0}\\right)\\stackrel{d}{\\}N\\left(0,I_{K+1}\\right).\\]econometrics practice, FE estimator popular RE\nestimator former consistent general conditions.","code":""},{"path":"panel-data.html","id":"summary-8","chapter":"10 Panel Data","heading":"10.3 Summary","text":"formula FE estimator RE estimators important\nestimation inference automatically handled \neconometric packages. important conceptual difference \nFE RE treatment unobservable individual\nheterogeneity.Panel data first generation economic “big data”, number\nobservations cross section multiplied number time\nperiods. reflected econometrician’s pursuit controlling\nheterogeneity, OLS estimate credible causal\ninterpretation.reading: Hsiao (2014) comprehensive monograph \ntopic panel data. Su, Shi, Phillips (2016) extends fixed effect models\nincorporate group heterogeneity.Zhentao Shi. Nov 8, 2020.","code":""},{"path":"endogeneity.html","id":"endogeneity","chapter":"11 Endogeneity","heading":"11 Endogeneity","text":"microeconomic analysis, exogenous variables factors\ndetermined outside economic system consideration, \nendogenous variables decided within economic system.microeconomic exercise encountered many times goes \nfollows. person utility function \\(u\\left(q_{1},q_{2}\\right)\\)\n\\(q_{1}\\) \\(q_{2}\\) quantities two goods. faces \nbudget \\(p_{1}q_{1}+p_{2}q_{2}\\leq C\\), \\(p_{1}\\) \\(p_{2}\\) \nprices two goods, respectively. optimal quantities\n\\(q_{1}^{*}\\) \\(q_{2}^{*}\\) purchase? question \nutility function \\(u\\left(\\cdot,\\cdot\\right)\\), prices \\(p_{1}\\) \n\\(p_{2}\\), budget \\(C\\) exogenous. optimal purchase\n\\(q_{1}^{*}\\) \\(q_{2}^{*}\\) endogenous.terms “endogenous” “exogenous” microeconomics carried\nmultiple-equation econometric models. \nsingle-equation regression model\n\\[y_{}=x_{}'\\beta+e_{}\\label{eq:generative}\\] part \nequation system. make simple, single-equation model say\n\\(x_{ik}\\) endogenous, endogenous variable, \n\\(\\mathrm{cov}\\left(x_{ik},e_{}\\right)\\neq0\\); otherwise \\(x_{ik}\\) \nexogenous variable.Empirical works using linear regressions routinely challenged \nquestions endogeneity. questions plague economic seminars \nreferee reports. defend empirical strategies quantitative economic\nstudies, important understand sources potential\nendogeneity thoroughly discuss attempts resolving endogeneity.","code":""},{"path":"endogeneity.html","id":"identification","chapter":"11 Endogeneity","heading":"11.1 Identification","text":"Endogeneity usually implies difficulty identifying parameter \ninterest \\(\\left(y_{},x_{}\\right)\\). Identification \ncritical interpretation empirical economic research. say \nparameter identified mapping parameter \nmodel distribution observed variable one--one;\notherwise say parameter -identified. \nabstract definition, let us discuss family linear\nregression context.linear projection model implies moment equation\n\\[\\mathbb{E}\\left[x_{}x_{}'\\right]\\beta=\\mathbb{E}\\left[x_{}y_{}\\right]. (citation)\\]\n\\(E\\left[x_{}x_{}'\\right]\\) full rank, \n\\(\\beta=\\left(\\mathbb{E}\\left[x_{}x_{}'\\right]\\right)^{-1}\\mathbb{E}\\left[x_{}y_{}\\right]\\)\nfunction quantities population moment \nidentified. contrary, \\(x_{k}\\)’s perfect collinear \n\\(\\mathbb{E}\\left[x_{}x_{}'\\right]\\) rank deficient, \nmultiple \\(\\beta\\) satisfies \\(k\\)-equation system\n(\\[eq:k-equation-FOC\\]). Identification fails.Suppose \\(x_{}\\) scalar random variable, \\[\\begin{pmatrix}x_{}\\\\\ne_{}\n\\end{pmatrix}\\sim N\\left(\\begin{pmatrix}0\\\\\n0\n\\end{pmatrix},\\begin{pmatrix}1 & \\sigma_{xe}\\\\\n\\sigma_{xe} & 1\n\\end{pmatrix}\\right)\\] follows joint normal distribution, \ndependent variable \\(y_{}\\) generated \n(\\[eq:generative\\]). joint normal assumption implies \nconditional mean\n\\[\\mathbb{E}\\left[y_{}|x_{}\\right]=\\beta x_{}+\\mathbb{E}\\left[e_{}|x_{}\\right]=\\left(\\beta+\\sigma_{xe}\\right)x_{}\\]\ncoincides linear projection model, \\(\\beta+\\sigma_{xe}\\) \nlinear projection coefficient. observable random variable\n\\(\\left(y_{},x_{}\\right)\\), can learn \\(\\beta+\\sigma_{xe}\\). \nlearn \\(\\sigma_{xe}\\) data due unobservable\n\\(e_{}\\), way recover \\(\\beta\\). exactly \nomitted variable bias discussed earlier course.\ngap lies available data \\(\\left(y_{},x_{}\\right)\\) \nidentification model. special case assume\n\\(\\sigma_{xe}=0\\), endogeneity vanishes \\(\\beta\\) identified.linear projection model far general model \ncourse justifies OLS. OLS consistent linear projection\ncoefficient. definition linear projection model,\n\\(\\mathbb{E}\\left[x_{}e_{}\\right]=0\\) room \nendogeneity linear projection model. words, talk\nendogeneity, must working linear projection\nmodel, coefficients pursue structural parameter, rather\nlinear projection coefficients.econometrics often interested model economic\ninterpretation. common practice empirical research assumes \nobserved data generated parsimonious model, next\nstep estimate unknown parameters model. Since \noften possible name factors included regressors \ncorrelated included regressors mean time\nalso affects \\(y_{}\\), endogeneity becomes fundamental problem.resolve endogeneity, seek extra variables data structure \nmay guarantee identification model. often used\nmethods () fixed effect model (ii) instrumental variables:fixed effect model requires multiple observations, often\nacross time, collected individual \\(\\). Moreover, \nsource endogeneity time invariant enters model\nadditively form \\[y_{}=x_{}'\\beta+u_{},\\] \n\\(u_{}=\\alpha_{}+\\epsilon_{}\\) composite error. panel\ndata approach extends \\(\\left(y_{},x_{}\\right)\\) \n\\(\\left(y_{},x_{}\\right)_{=1}^{T}\\) data available along\ntime dimension.fixed effect model requires multiple observations, often\nacross time, collected individual \\(\\). Moreover, \nsource endogeneity time invariant enters model\nadditively form \\[y_{}=x_{}'\\beta+u_{},\\] \n\\(u_{}=\\alpha_{}+\\epsilon_{}\\) composite error. panel\ndata approach extends \\(\\left(y_{},x_{}\\right)\\) \n\\(\\left(y_{},x_{}\\right)_{=1}^{T}\\) data available along\ntime dimension.instrumental variable approach extends\n\\(\\left(y_{},x_{}\\right)\\) \\(\\left(y_{},x_{},z_{}\\right)\\),\nextra random variable \\(z_{}\\) called instrument\nvariable. assumed \\(z_{}\\) orthogonal error\n\\(e_{}\\) . Therefore, along model adds extra variable\n\\(z_{}\\).instrumental variable approach extends\n\\(\\left(y_{},x_{}\\right)\\) \\(\\left(y_{},x_{},z_{}\\right)\\),\nextra random variable \\(z_{}\\) called instrument\nvariable. assumed \\(z_{}\\) orthogonal error\n\\(e_{}\\) . Therefore, along model adds extra variable\n\\(z_{}\\).Either panel data approach instrumental variable approach\nentails extra information beyond \\(\\left(y_{},x_{}\\right)\\). Without\nextra data, way resolve identification failure.\nJust linear project model available joint distribution\n\\(\\left(y_{},x_{}\\right)\\) existence suitable moments, \npure statistical point view linear IV model artifact depends\nchoice \\(\\left(y_{},x_{},z_{}\\right)\\) without\nreferencing economics. essence, linear IV model seeks \nlinear combination \\(y_{}-\\beta x_{}\\) orthogonal linear\nspace spanned \\(z_{}\\).","code":""},{"path":"endogeneity.html","id":"instruments","chapter":"11 Endogeneity","heading":"11.2 Instruments","text":"two requirements valid IVs: orthogonality relevance.\nOrthogonality entails model correctly specified. \nrelevance violated, meaning IVs correlated \nendogenous variable, multiple parameters can generate \nobservable data. Identification, standard definition \neconometrics, breaks .structural equation model economic interest. Consider \nfollowing linear structural model\n\\[y_{}=x_{1i}'\\beta_{1}+z_{1i}'\\beta_{2}+\\epsilon_{},\\label{eq:basic_1}\\]\n\\(x_{1i}\\) \\(k_{1}\\)-dimensional endogenous explanatory\nvariables, \\(z_{1i}\\) \\(k_{2}\\)-dimensional exogenous explanatory\nvariables intercept included. addition, \\(z_{2i}\\), \n\\(k_{3}\\)-dimensional excluded exogenous variables. Let \\(K=k_{1}+k_{2}\\)\n\\(L=k_{2}+k_{3}\\). Denote \\(x_{}=\\left(x_{1i}',z_{1i}'\\right)'\\) \n\\(K\\)-dimensional explanatory variable, \n\\(z_{}=\\left(z_{1i}',z_{2i}'\\right)\\) \\(L\\)-dimensional exogenous\nvector.call exogenous variable instrument variables, simply\ninstruments. Let \\(\\beta=\\left(\\beta_{1}',\\beta_{2}'\\right)'\\) \n\\(K\\)-dimensional parameter interest. now , rewrite\n(\\[eq:basic\\_1\\]) \n\\[y_{}=x_{}'\\beta+\\epsilon_{},\\label{eq:basic_2}\\] \nvector instruments \\(z_{}\\).estimating structural econometric model, must check\nidentification. context \n(\\[eq:basic\\_2\\]), identification requires true value\n\\(\\beta_{0}\\) value parameters space satisfies \nmoment condition\n\\[\\mathbb{E}\\left[z_{}\\left(y_{}-x_{}'\\beta\\right)\\right]=0_{L}.\\label{eq:moment}\\]\nrank condition sufficient necessary identification.\\(\\mathrm{rank}\\left(\\mathbb{E}\\left[z_{}x_{}'\\right]\\right)=K\\).Note \\(\\mathbb{E}\\left[x_{}'z_{}\\right]\\) \\(K\\times L\\) matrix.\nrank condition implies order condition \\(L\\geq K\\), says\nnumber excluded instruments must fewer number\nendogenous variables.parameter (\\[eq:moment\\]) identified rank condition\nholds.(“” direction). \\(\\tilde{\\beta}\\) \n\\(\\tilde{\\beta}\\neq\\beta_{0}\\), \\[\\begin{aligned}\n\\mathbb{E}\\left[z_{}\\left(y_{}-x_{}'\\tilde{\\beta}\\right)\\right] & =\\mathbb{E}\\left[z_{}\\left(y_{}-x_{}'\\beta_{0}\\right)\\right]+\\mathbb{E}\\left[z_{}x_{}'\\right]\\left(\\beta_{0}-\\tilde{\\beta}\\right)\\\\\n& =0_{L}+\\mathbb{E}\\left[z_{}x_{}'\\right]\\left(\\beta_{0}-\\tilde{\\beta}\\right).\\end{aligned}\\]\n\n\\(\\mathrm{rank}\\left(\\mathbb{E}\\left[z_{}x_{}'\\right]\\right)=K\\), \n\n\\(\\mathbb{E}\\left[z_{}x_{}'\\right]\\left(\\beta_{0}-\\tilde{\\beta}\\right)=0_{L}\\)\n\\(\\beta_{0}-\\tilde{\\beta}=0_{K}\\), violates\n\\(\\tilde{\\beta}\\neq\\beta_{0}\\). Therefore \\(\\beta_{0}\\) unique value\nsatisfies (\\[eq:moment\\]).(“” direction left exercise. Hint: \ncontrapositiveness, rank condition fails, model \nidentified. can easily prove claim making example.)","code":""},{"path":"endogeneity.html","id":"sources-of-endogeneity","chapter":"11 Endogeneity","heading":"11.3 Sources of Endogeneity","text":"econometricians mostly work non-experimental data, \noverstate importance endogeneity problem. go \nexamples.know first-difference (FD) estimator consistent \n(static) panel data model. Nevertheless, FD estimator encounters\ndifficulty dynamic panel model\n\\[y_{}=\\beta_{1}+\\beta_{2}y_{,t-1}+\\beta_{3}x_{}+\\alpha_{}+\\epsilon_{},\\label{eq:dymPanel}\\]\neven assume\n\\[\\mathbb{E}\\left[\\epsilon_{}|\\alpha_{},x_{i1},\\ldots,x_{},y_{,t-1},y_{,t-2},\\ldots,y_{i0}\\right]=0,\\ \\ \\forall s\\geq t\\label{eq:dyn_mean_0}\\]\ntaking difference equation\n(\\[eq:dymPanel\\]) periods \\(t\\) \\(t-1\\), \n\\[\\left(y_{}-y_{,t-1}\\right)=\\beta_{2}\\left(y_{-1}-y_{,t-2}\\right)+\\beta_{3}\\left(x_{}-x_{,t-1}\\right)+\\left(\\epsilon_{}-\\epsilon_{,t-1}\\right).\\label{eq:dyn_mean_1}\\]\n(\\[eq:dyn\\_mean\\_0\\]),\n\\(\\mathbb{E}\\left[\\left(x_{}-x_{,t-1}\\right)\\left(\\epsilon_{}-\\epsilon_{,t-1}\\right)\\right]=0\\),\n\n\\[\\mathbb{E}\\left[\\left(y_{,t-1}-y_{,t-2}\\right)\\left(\\epsilon_{}-\\epsilon_{,t-1}\\right)\\right]=-\\mathbb{E}\\left[y_{,t-1}\\epsilon_{,t-1}\\right]=-\\mathbb{E}\\left[\\epsilon_{,t-1}^{2}\\right]\\neq0.\\]\nTherefore coefficients \\(\\beta_{2}\\) \\(\\beta_{3}\\) \nidentified linear regression model\n(\\[eq:dyn\\_mean\\_1\\]).Instruments example easy find. Notice \nlinear relationship\n(\\[eq:dymPanel\\]) implies \\[\\begin{aligned}\n&  & \\mathbb{E}\\left[\\epsilon_{,t}-\\epsilon_{,t-1}|\\alpha_{},x_{i1},\\ldots,x_{},\\epsilon_{,t-2},\\epsilon_{,t-3},\\ldots,\\epsilon_{i1},y_{i0}\\right]\\\\\n& = & \\mathbb{E}\\left[\\epsilon_{,t}-\\epsilon_{,t-1}|\\alpha_{},x_{i1},\\ldots,x_{},y_{,t-2},y_{,t-3},\\ldots,y_{i0}\\right]=0\\end{aligned}\\]\naccording assumption\n(\\[eq:dyn\\_mean\\_0\\]). relationship gives orthogonal\ncondition form\n\\[\\mathbb{E}\\left[\\left(\\epsilon_{,t}-\\epsilon_{,t-1}\\right)f\\left(\\epsilon_{,t-2},\\epsilon_{,t-3},\\ldots,\\epsilon_{i1}\\right)\\right]=0.\\]\nwords, function \\(y_{,t-2},y_{,t-3},\\ldots,y_{i1}\\) \northogonal error term\n\\(\\left(\\epsilon{}_{,t-1}-\\epsilon_{,t-2}\\right)\\). excluded\nIVs naturally generated model .Another classical source endogeneity measurement error.Endogeneity also emerges explanatory variables directly\nobservable replaced measurement error. Suppose true\nlinear model \n\\[y_{}=\\beta_{1}+\\beta_{2}x_{}^{*}+u_{},\\label{eq:measurement_error}\\]\n\\(\\mathbb{E}\\left[u_{}|x_{}^{*}\\right]=0\\). observe\n\\(x_{}^{*}\\) observe \\(x_{}\\), measurement \\(x_{}^{*}\\), \nlinked \\[x_{}=x_{}^{*}+v_{}\\] \n\\(\\mathbb{E}\\left[v_{}|x_{}^{*},u_{}\\right]=0\\). formulation \nmeasurement error called classical measurement error.\nSubstitute unobservable \\(x_{}^{*}\\) \n(\\[eq:measurement\\_error\\]),\n\\[y_{}=\\beta_{1}+\\beta_{2}\\left(x_{}-v_{}\\right)+u_{}=\\beta_{1}+\\beta_{2}x_{}+e_{}\\label{eq:measurement_error2}\\]\n\\(e_{}=u_{}-\\beta_{2}v_{}\\). correlation\n\\[\\mathbb{E}\\left[x_{}e_{}\\right]=\\mathbb{E}\\left[\\left(x_{}^{*}+v_{}\\right)\\left(u_{}-\\beta_{2}v_{}\\right)\\right]=-\\beta_{2}\\mathbb{E}\\left[v_{}^{2}\\right]\\neq0.\\]\nOLS\n(\\[eq:measurement\\_error2\\]) deliver consistent\nestimator.Alternatively, can look problem classical measurement\nerror expression linear projection coefficient. know\n\n(\\[eq:measurement\\_error\\])\n\\(\\beta_{2}^{\\mathrm{infeasible}}=\\mathrm{cov}\\left[x_{}^{*},y_{}\\right]/\\mathrm{var}\\left[x_{}^{*}\\right].\\)\ncontrast, regression \\(y_{}\\) observable \\(x_{}\\) \ncorresponding linear projection coefficient \n\\[\\beta_{2}^{\\mathrm{feasible}}=\\frac{\\mathrm{cov}\\left[x_{},y_{}\\right]}{\\mathrm{var}\\left[x_{}\\right]}=\\frac{\\mathrm{cov}\\left[x_{}^{*}+v_{},y_{}\\right]}{\\mathrm{var}\\left[x_{}^{*}+v_{}\\right]}=\\frac{\\mathrm{cov}\\left[x_{}^{*},y_{}\\right]}{\\mathrm{var}\\left[x_{}^{*}\\right]+\\mathrm{var}\\left[v_{}\\right]}.\\]\nclear \n\\(|\\beta_{2}^{\\mathrm{feasible}}|\\leq|\\beta_{2}^{\\mathrm{infeasible}}|\\)\nequality holds \\(\\mathrm{var}\\left[v_{}\\right]=0\\) (\nmeasurement error). called attenuation bias due \nmeasurement error.Next, give two examples equation systems, one microeconomics\nmacroeconomics.Let \\(p_{}\\) \\(q_{}\\) good’s log-price log-quantity \n\\(\\)-th market, iid across markets. interested \ndemand curve \\[p_{}=\\alpha_{d}-\\beta_{d}q_{}+e_{di}\\label{eq:demand}\\]\n\\(\\beta_{d}\\geq0\\) supply curve\n\\[p_{}=\\alpha_{s}+\\beta_{s}q_{}+e_{si}\\label{eq:supply}\\] \n\\(\\beta_{s}\\geq0\\). use simple linear specification \ncoefficient \\(\\beta_{d}\\) can interpreted demand elasticity \n\\(\\beta_{s}\\) supply elasticity. Undergraduate microeconomics teaches\ndeterministic form add error term cope data.\nCan learn elasticities regression \\(p_{}\\) \\(q_{}\\)?two equations can written matrix form\n\\[\\begin{pmatrix}1 & \\beta_{d}\\\\\n1 & -\\beta_{s}\n\\end{pmatrix}\\begin{pmatrix}p_{}\\\\\nq_{}\n\\end{pmatrix}=\\begin{pmatrix}\\alpha_{d}\\\\\n\\alpha_{s}\n\\end{pmatrix}+\\begin{pmatrix}e_{di}\\\\\ne_{si}\n\\end{pmatrix}.\\label{eq:structural}\\] Microeconomic terminology calls\n\\(\\left(p_{},q_{}\\right)\\) endogenous variables \n\\(\\left(e_{di},e_{si}\\right)\\) exogenous variables.\n(\\[eq:structural\\]) structural equation \nmotivated economic theory coefficients bear economic\nmeaning. rule trivial case \\(\\beta_{d}=\\beta_{s}=0\\), can\nsolve \\[\\begin{aligned}\n\\begin{pmatrix}p_{}\\\\\nq_{}\n\\end{pmatrix} & =\\begin{pmatrix}1 & \\beta_{d}\\\\\n1 & -\\beta_{s}\n\\end{pmatrix}^{-1}\\left[\\begin{pmatrix}\\alpha_{d}\\\\\n\\alpha_{s}\n\\end{pmatrix}+\\begin{pmatrix}e_{di}\\\\\ne_{si}\n\\end{pmatrix}\\right]\\nonumber \\\\\n& =\\frac{1}{\\beta_{s}+\\beta_{d}}\\begin{pmatrix}\\beta_{s} & \\beta_{d}\\\\\n1 & -1\n\\end{pmatrix}\\left[\\begin{pmatrix}\\alpha_{d}\\\\\n\\alpha_{s}\n\\end{pmatrix}+\\begin{pmatrix}e_{di}\\\\\ne_{si}\n\\end{pmatrix}\\right].\\label{eq:reduced}\\end{aligned}\\] equation\n(\\[eq:reduced\\]) called reduced form—endogenous\nvariables expressed explicit functions parameters \nexogenous variables. particular,\n\\[q_{}=\\left(\\alpha_{d}+e_{di}-\\alpha_{s}-e_{si}\\right)/\\left(\\beta_{s}+\\beta_{d}\\right)\\]\nlog-price correlated \\(e_{si}\\) \\(e_{di}\\). \n\\(q_{}\\) endogenous (econometric sense) either\n(\\[eq:demand\\]) \n(\\[eq:supply\\]), neither demand elasticity supply\nelasticity identified \\(\\left(p_{},q_{}\\right)\\). Indeed, \n\\[p_{}=\\left(\\beta_{s}\\alpha_{d}+\\beta_{d}\\alpha_{s}+\\beta_{s}e_{di}+\\beta_{d}e_{si}\\right)/\\left(\\beta_{s}+\\beta_{d}\\right)\\]\n(\\[eq:reduced\\]), linear projection coefficient \\(p_{}\\)\n\\(q_{}\\) \n\\[\\frac{\\mathrm{cov}\\left[p_{},q_{}\\right]}{\\mathrm{var}\\left[q_{}\\right]}=\\frac{\\beta_{s}\\sigma_{d}^{2}-\\beta_{d}\\sigma_{s}^{2}+\\left(\\beta_{d}-\\beta_{s}\\right)\\sigma_{sd}}{\\beta_{d}^{2}\\sigma_{d}^{2}+\\beta_{d}\\sigma_{s}^{2}+2\\beta_{d}\\beta_{s}\\sigma_{sd}},\\]\n\\(\\sigma_{d}^{2}=\\mathrm{var}\\left[e_{di}\\right]\\),\n\\(\\sigma_{s}^{2}=\\mathrm{var}\\left[e_{si}\\right]\\) \n\\(\\sigma_{sd}=\\mathrm{cov}\\left[e_{di},e_{si}\\right]\\).classical example demand-supply system. structural\nparameter directly identified observed\n\\(\\left(p_{},q_{}\\right)\\) outcome equilibrium—\ncrossing demand curve supply curve. identify \ndemand curve, need instrument shifts supply curve\n; vice versa.model borrowed Hayashi (2000, p.193) originated \nHaavelmo (1943). econometrician interested learning\n\\(\\beta_{2}\\), marginal propensity consumption, \nKeynesian-type equation\n\\[C_{}=\\beta_{1}+\\beta_{2}Y_{}+u_{}\\label{eq:keynes}\\] \\(C_{}\\)\nhousehold consumption, \\(Y_{}\\) GNP, \\(u_{}\\) \nunobservable error. However, \\(Y_{}\\) \\(C_{}\\) connected \naccounting equality (error) \\[Y_{}=C_{}+I_{},\\] \\(I_{}\\)\ninvestment. assume \\(\\mathbb{E}\\left[u_{}|I_{}\\right]=0\\) \ninvestment determined advance. example,\n\\(\\left(Y_{}C_{}\\right)\\) endogenous \\(\\left(I_{},u_{}\\right)\\)\nexogenous. Put two equations together structural form\n\\[\\begin{pmatrix}1 & -\\beta_{2}\\\\\n-1 & 1\n\\end{pmatrix}\\begin{pmatrix}C_{}\\\\\nY_{}\n\\end{pmatrix}=\\begin{pmatrix}\\beta_{1}\\\\\n0\n\\end{pmatrix}+\\begin{pmatrix}u_{}\\\\\nI_{}\n\\end{pmatrix}.\\] corresponding reduced form \\[\\begin{aligned}\n\\begin{pmatrix}C_{}\\\\\nY_{}\n\\end{pmatrix} & =\\begin{pmatrix}1 & -\\beta_{2}\\\\\n-1 & 1\n\\end{pmatrix}^{-1}\\left[\\begin{pmatrix}\\beta_{1}\\\\\n0\n\\end{pmatrix}+\\begin{pmatrix}u_{}\\\\\nI_{}\n\\end{pmatrix}\\right]\\\\\n& =\\frac{1}{1-\\beta_{2}}\\begin{pmatrix}1 & \\beta_{2}\\\\\n1 & 1\n\\end{pmatrix}\\left[\\begin{pmatrix}\\beta_{1}\\\\\n0\n\\end{pmatrix}+\\begin{pmatrix}u_{}\\\\\nI_{}\n\\end{pmatrix}\\right]\\\\\n& =\\frac{1}{1-\\beta_{2}}\\begin{pmatrix}\\beta_{1}+u_{}+\\beta_{2}I_{}\\\\\n\\beta_{1}+u_{}+I_{}\n\\end{pmatrix}.\\end{aligned}\\] OLS\n(\\[eq:keynes\\]) inconsistent reduced-form\n\\(Y_{}=\\frac{1}{1-\\beta_{2}}\\left(\\beta_{1}+u_{}+I_{}\\right)\\) implies\n\\(\\mathbb{E}\\left[Y_{}u_{}\\right]=\\mathbb{E}\\left[u_{}^{2}\\right]/\\left(1-\\beta_{2}\\right)\\neq0\\).","code":""},{"path":"endogeneity.html","id":"summary-9","chapter":"11 Endogeneity","heading":"11.4 Summary","text":"Even though often deal single equation model potential\nendogenous variables, underlying structural system may involve\nmultiple equations. simultaneous equation model classical\neconometric modeling approach, still actively applied \nstructural economic studies. economic model “structural”, \nkeep mind causal mechanism. Instead identifying causal\neffect control group treatment group Chapter 2, \nlook causality economic structural perspective.Historical notes: Instruments originally appeared Philip\nWright (1928) identifying coefficient endogenous\nvariables. believed collaborative idea Philip’s son\nSewall Wright. demand supply analysis attributed \nWorking (1927), measurement error study dated back\nFricsh (1934).reading: Causality holy grail econometrics.\nPearl Mackenzie (2018) popular book philosophical depth. \ndelight read. (Chen, Hong, Nekipelov 2011) survey modern nonlinear\nmeasurement error models.","code":""},{"path":"generalized-method-of-moments.html","id":"generalized-method-of-moments","chapter":"12 Generalized Method of Moments","heading":"12 Generalized Method of Moments","text":"Generalized method moments (GMM) (Hansen 1982) \nestimation principle extends method moments. seeks \nparameter value minimizes quadratic form moments. \nparticularly useful estimating structural economic models \nmoment conditions can derived underlying economic theory. GMM\nemerges one popular estimators modern econometrics. \nincludes conventional methods like two-stage least squares (2SLS)\nthree-stage least square special cases.","code":""},{"path":"generalized-method-of-moments.html","id":"instrumental-regression","chapter":"12 Generalized Method of Moments","heading":"12.1 Instrumental Regression","text":"first discuss estimation linear single structural equation\n\\[y_{}=x_{}'\\beta+\\epsilon_{}\\] \\(K\\) regressors. Identification\nprerequisite structural estimation. now always\nassume model identified: \\(L\\times1\\) vector \ninstruments \\(z_{}\\) \n\\(\\mathbb{E}\\left[z_{}\\epsilon_{}\\right]=0_{L}\\) \n\\(\\Sigma:=\\mathbb{E}\\left[z_{}x_{}'\\right]\\) full column rank.\nDenote \\(\\beta_{0}\\) root equation\n\\(E\\left[z_{}\\left(y_{}-x_{}'\\beta\\right)\\right]=0_{L}\\), \nuniquely identified.","code":""},{"path":"generalized-method-of-moments.html","id":"just-identification","chapter":"12 Generalized Method of Moments","heading":"12.1.1 Just-identification","text":"\\(L=K\\), instrumental regression model just-identified, \nexactly identified. orthogonality condition implies\n\\[\\Sigma\\beta_{0}=\\mathbb{E}\\left[z_{}y_{}\\right],\\] can solve\nexpress \\(\\beta_{0}\\) \n\\[\\beta_{0}=\\Sigma^{-1}\\mathbb{E}\\left[z_{}y_{}\\right]\\label{eq:just_beta}\\]\nclosed form.closed-form solution naturally motivates estimator \nreplace population methods sample moments \nmethod--moments estimator. Nevertheless, postpone discussion \nestimator next section.","code":""},{"path":"generalized-method-of-moments.html","id":"over-identification","chapter":"12 Generalized Method of Moments","heading":"12.1.2 Over-identification","text":"\\(L>K\\), model -identified. orthogonality condition\nstill implies\n\\[\\Sigma\\beta_{0}=\\mathbb{E}\\left[z_{}y_{}\\right],\\label{eq:moment2}\\]\n\\(\\Sigma\\) square matrix write \\(\\beta_{0}\\) \n(\\[eq:just\\_beta\\]). order express \\(\\beta_{0}\\) explicitly,\ndefine criterion function\n\\[Q\\left(\\beta\\right)=\\mathbb{E}\\left[z_{}\\left(y_{}-x_{}\\beta\\right)\\right]'W\\mathbb{E}\\left[z_{}\\left(y_{}-x_{}\\beta\\right)\\right],\\]\n\\(W\\) \\(L\\times L\\) positive-definite non-random symmetric\nmatrix. (choice \\(W\\) discussed soon.) \nquadratic form, \\(Q\\left(\\beta\\right)\\geq0\\) \\(\\beta\\).\nIdentification indicates \\(Q\\left(\\beta\\right)=0\\) \n\\(\\beta=\\beta_{0}\\). Therefore conclude\n\\[\\beta_{0}=\\arg\\min_{\\beta}Q\\left(\\beta\\right)\\] unique\nminimizer. Since \\(Q\\left(\\beta\\right)\\) smooth function \\(\\beta\\),\nminimizer \\(\\beta_{0}\\) can characterized first-order\ncondition\n\\[0_{K}=\\frac{\\partial}{\\partial\\beta}Q\\left(\\beta_{0}\\right)=-2\\Sigma'W\\mathbb{E}\\left[z_{}\\left(y_{}-x_{}\\beta_{0}\\right)\\right]\\]\nRearranging equation, \n\\[\\Sigma'W\\Sigma\\beta_{0}=\\Sigma'W\\mathbb{E}\\left[z_{}y_{}\\right].\\]\nrank condition \\(\\Sigma'W\\Sigma\\) invertible can\nsolve\n\\[\\beta_{0}=\\left(\\Sigma'W\\Sigma\\right)^{-1}\\Sigma'W\\mathbb{E}\\left[z_{}y_{}\\right].\\label{eq:over_beta}\\]\nmoments (\\(L\\)) number unknown parameters\n(\\(K\\)), call generalized method moments.equation can derived pre-multiplying \\(\\Sigma'W\\) \nsides (\\[eq:moment2\\]) without referring minimization problem.Although separate discussion just-identified case \n-identified case, latter\n(\\[eq:\\_beta\\]) actually takes\n(\\[eq:just\\_beta\\]) special case. sense, GMM \ngenuine generalization method moments. see point,\nnotice \\(L=K\\), given \\(W\\) \\[\\begin{aligned}\n\\beta_{0} & =\\left(\\Sigma'W\\Sigma\\right)^{-1}\\Sigma'W\\mathbb{E}\\left[z_{}y_{}\\right]=\\Sigma^{-1}W^{-1}(\\Sigma')^{-1}\\Sigma'W\\mathbb{E}\\left[z_{}y_{}\\right]\\\\\n& =\\Sigma^{-1}W^{-1}W\\mathbb{E}\\left[z_{}y_{}\\right]=\\Sigma^{-1}\\mathbb{E}\\left[z_{}y_{}\\right],\\end{aligned}\\]\n\\(\\Sigma\\) square matrix. say, just-identified\ncase \\(W\\) plays role choices \\(W\\) lead \nexplicit solution \\(\\beta_{0}\\).","code":""},{"path":"generalized-method-of-moments.html","id":"gmm-estimator","chapter":"12 Generalized Method of Moments","heading":"12.2 GMM Estimator","text":"practice, use sample moments replace corresponding\npopulation moments. GMM estimator mimics population formula.\n\\[\\begin{aligned}\n\\widehat{\\beta} & = & \\left(\\frac{1}{n}\\sum x_{}z_{}'W\\frac{1}{n}\\sum z_{}x_{}'\\right)^{-1}\\frac{1}{n}\\sum x_{}z_{}'W\\frac{1}{n}\\sum z_{}y_{}\\\\\n& = & \\left(\\frac{X'Z}{n}W\\frac{Z'X}{n}\\right)^{-1}\\frac{X'Z}{n}W\\frac{Z'y}{n}\\\\\n& = & \\left(X'ZWZ'X\\right)^{-1}X'ZWZ'y.\\end{aligned}\\] \njust-identification, expression includes 2SLS estimator\n\\[\\hat{\\beta}=\\left(\\frac{Z'X}{n}\\right)^{-1}\\frac{Z'y}{n}=\\left(Z'X\\right)^{-1}Z'y\\]\nspecial case.GMM estimator \\(\\hat{\\beta}\\) can obtained minimizing\n\\[Q_{n}\\left(\\beta\\right)=\\left[\\frac{1}{n}\\sum_{=1}^{n}z_{}\\left(y_{}-x_{}\\beta\\right)\\right]'W\\left[\\frac{1}{n}\\sum_{=1}^{n}z_{}\\left(y_{}-x_{}\\beta\\right)\\right]=\\frac{\\left(y-X\\beta\\right)'Z}{n}W\\frac{Z'\\left(y-X\\beta\\right)}{n},\\]\nconcisely\n\\(\\hat{\\beta}=\\arg\\min_{\\beta}\\left(y-X\\beta\\right)'ZWZ'\\left(y-X\\beta\\right).\\)Now check asymptotic properties \\(\\widehat{\\beta}\\). \nassumptions order.\\(Z'X/n\\stackrel{\\mathrm{p}}{\\}\\Sigma\\) \n\\(Z'\\epsilon/n\\stackrel{\\mathrm{p}}{\\}0_{L}\\)..1 assumes can apply law large numbers, \nsample moments \\(Z'X/n\\) \\(Z'\\epsilon/n\\) converge probability \npopulation counterparts.Assumption .1, \\(\\widehat{\\beta}\\) consistent.step similar consistency proof OLS. \\[\\begin{aligned}\n\\widehat{\\beta} & =\\left(X'ZWZ'X\\right)^{-1}X'ZWZ'\\left(X'\\beta_{0}+\\epsilon\\right)\\\\\n& =\\beta_{0}+\\left(\\frac{X'Z}{n}W\\frac{Z'X}{n}\\right)^{-1}\\frac{X'Z}{n}W\\frac{Z'\\epsilon}{n}\\\\\n& \\stackrel{\\mathrm{p}}{\\}\\beta_{0}+\\left(\\Sigma'W\\Sigma\\right)^{-1}\\Sigma'W0=\\beta_{0}.\\qedhere\\end{aligned}\\]check asymptotic normality, assume central limit theorem\ncan applied.\\(\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}z_{}\\epsilon_{}\\stackrel{d}{\\}N\\left(0_{L},\\Omega\\right)\\),\n\\(\\Omega=\\mathbb{E}\\left[z_{}z_{}'\\epsilon_{}^{2}\\right].\\)Assumptions .1 .2,\n\\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{0}\\right)\\stackrel{d}{\\}N\\left(0_{K},\\left(\\Sigma'W\\Sigma\\right)^{-1}\\Sigma'W\\Omega W\\Sigma\\left(\\Sigma'W\\Sigma\\right)^{-1}\\right).\\label{eq:normality}\\]Multiply \\(\\widehat{\\beta}-\\beta_{0}\\) scaling factor \\(\\sqrt{n}\\),\n\\[\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{0}\\right)=\\left(\\frac{X'Z}{n}W\\frac{Z'X}{n}\\right)^{-1}\\frac{X'Z}{n}W\\frac{Z'\\epsilon}{\\sqrt{n}}=\\left(\\frac{X'Z}{n}W\\frac{Z'X}{n}\\right)^{-1}\\frac{X'Z}{n}W\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}z_{}'\\epsilon_{}.\\]\nconclusion follows Slutsky’s theorem \n\\[\\frac{X'Z}{n}W\\frac{Z'X}{n}\\stackrel{\\mathrm{p}}{\\}\\Sigma'W\\Sigma\\]\n\n\\[\\frac{X'Z}{n}W\\frac{1}{\\sqrt{n}}\\sum z_{}'\\epsilon_{}\\stackrel{d}{\\}\\Sigma'W\\times N\\left(0,\\Omega\\right)\\sim N\\left(0,\\Sigma'W\\Omega W\\Sigma\\right).\\qedhere\\]","code":""},{"path":"generalized-method-of-moments.html","id":"efficient-gmm","chapter":"12 Generalized Method of Moments","heading":"12.2.1 Efficient GMM","text":"clear (\\[eq:normality\\]) GMM estimator’s asymptotic variance\ndepends choice \\(W\\). \\(W\\) makes asymptotic variance \nsmall possible? answer \\(W=\\Omega^{-1}\\), \nefficient asymptotic variance \n\\[\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}\\Sigma'\\Omega^{-1}\\Omega\\Omega^{-1}\\Sigma\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}=\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}.\\]positive definite symmetric matrix \\(W\\), difference\n\\[\\left(\\Sigma'W\\Sigma\\right)^{-1}\\Sigma'W\\Omega W\\Sigma\\left(\\Sigma'W\\Sigma\\right)^{-1}-\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}\\]\npositive semi-definite.simplify notation, denote\n\\(:=W\\Sigma\\left(\\Sigma'W\\Sigma\\right)^{-1}\\) \n\\(B:=\\Omega^{-1}\\Sigma\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}\\) \ndifference two matrices becomes \\[\\begin{aligned}\n&  & \\left(\\Sigma'W\\Sigma\\right)^{-1}\\Sigma'W\\Omega W\\Sigma\\left(\\Sigma'W\\Sigma\\right)^{-1}-\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}\\\\\n& = & '\\Omega -B'\\Omega B\\\\\n& = & \\left(-B+B\\right)'\\Omega\\left(-B+B\\right)-B'\\Omega B\\\\\n& = & \\left(-B\\right)'\\Omega\\left(-B\\right)+\\left(-B\\right)'\\Omega B+B'\\Omega\\left(-B\\right).\\end{aligned}\\]\nNotice \\[\\begin{aligned}\nB'\\Omega & =\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}\\Sigma'\\Omega^{-1}\\Omega W\\Sigma\\left(\\Sigma'W\\Sigma\\right)^{-1}\\\\\n& =\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}\\Sigma'W\\Sigma\\left(\\Sigma'W\\Sigma\\right)^{-1}=\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}=B'\\Omega B,\\end{aligned}\\]\nimplies \\(B'\\Omega\\left(-B\\right)=0\\) \n\\(\\left(-B\\right)'\\Omega B=0\\). thus conclude \n\\[\\left(\\Sigma'W\\Sigma\\right)^{-1}\\Sigma'W\\Omega W\\Sigma\\left(\\Sigma'W\\Sigma\\right)^{-1}-\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}=\\left(-B\\right)'\\Omega\\left(-B\\right)\\]\npositive semi-definite.","code":""},{"path":"generalized-method-of-moments.html","id":"two-step-gmm","chapter":"12 Generalized Method of Moments","heading":"12.2.2 Two-Step GMM","text":"two-step GMM one way construct feasible efficient GMM\nestimator.Choose valid \\(W\\), say \\(W=I_{L}\\), get consistent (\ninefficient general) estimator\n\\(\\hat{\\beta}^{\\sharp}=\\hat{\\beta}^{\\sharp}\\left(W\\right)\\). Save \nresidual \\(\\widehat{\\epsilon}_{}=y_{}-x_{}'\\hat{\\beta}^{\\sharp}\\)\nestimate variance matrix\n\\(\\widehat{\\Omega}=\\frac{1}{n}\\sum z_{}z_{}'\\widehat{\\epsilon}_{}^{2}.\\)\nNotice \\(\\widehat{\\Omega}\\) consistent \\(\\Omega\\).Choose valid \\(W\\), say \\(W=I_{L}\\), get consistent (\ninefficient general) estimator\n\\(\\hat{\\beta}^{\\sharp}=\\hat{\\beta}^{\\sharp}\\left(W\\right)\\). Save \nresidual \\(\\widehat{\\epsilon}_{}=y_{}-x_{}'\\hat{\\beta}^{\\sharp}\\)\nestimate variance matrix\n\\(\\widehat{\\Omega}=\\frac{1}{n}\\sum z_{}z_{}'\\widehat{\\epsilon}_{}^{2}.\\)\nNotice \\(\\widehat{\\Omega}\\) consistent \\(\\Omega\\).Set \\(W=\\widehat{\\Omega}^{-1}\\) obtain second estimator\n\\[\\widehat{\\beta}^{\\natural}=\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})=\\left(X'Z\\widehat{\\Omega}^{-1}Z'X\\right)^{-1}X'Z\\widehat{\\Omega}^{-1}Z'y.\\]\nsecond estimator asymptotic efficient.Set \\(W=\\widehat{\\Omega}^{-1}\\) obtain second estimator\n\\[\\widehat{\\beta}^{\\natural}=\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})=\\left(X'Z\\widehat{\\Omega}^{-1}Z'X\\right)^{-1}X'Z\\widehat{\\Omega}^{-1}Z'y.\\]\nsecond estimator asymptotic efficient.Show \\(\\widehat{\\Omega}\\stackrel{p}{\\}\\Omega\\), \n\\(\\sqrt{n}\\left(\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})-\\widehat{\\beta}\\left(\\Omega^{-1}\\right)\\right)\\stackrel{p}{\\}0\\).\nwords, feasible estimator\n\\(\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})\\) asymptotically\nequivalent infeasible efficient estimator\n\\(\\widehat{\\beta}\\left(\\Omega^{-1}\\right)\\).","code":""},{"path":"generalized-method-of-moments.html","id":"two-stage-least-squares","chapter":"12 Generalized Method of Moments","heading":"12.2.3 Two Stage Least Squares","text":"assume conditional homoskedasticity\n\\(\\mathbb{E}\\left[\\epsilon_{}^{2}|z_{}\\right]=\\sigma^{2}\\), \n\\[\\Omega=\\mathbb{E}\\left[z_{}z_{}'\\epsilon_{}^{2}\\right]=\\mathbb{E}\\left[z_{}z_{}'\\mathbb{E}\\left[\\epsilon_{}^{2}|z_{}\\right]\\right]=\\sigma^{2}\\mathbb{E}\\left[z_{}z_{}'\\right].\\]\nfirst-step two-step GMM can estimate variance \nerror term \n\\(\\widehat{\\sigma}^{2}=\\frac{1}{n}\\sum_{=1}^{n}\\widehat{\\epsilon}_{}^{2}\\)\nvariance matrix \n\\(\\widehat{\\Omega}=\\widehat{\\sigma}^{2}\\frac{1}{n}\\sum_{=1}^{n}z_{}z_{}'=\\widehat{\\sigma}^{2}Z'Z/n\\).\nplug \\(W=\\widehat{\\Omega}^{-1}\\) GMM estimator,\n\\[\\begin{aligned}\n\\widehat{\\beta} & = & \\left(X'Z\\left(\\widehat{\\sigma}^{2}\\frac{Z'Z}{n}\\right)^{-1}Z'X\\right)^{-1}X'Z\\left(\\widehat{\\sigma}^{2}\\frac{Z'Z}{n}\\right)^{-1}Z'y\\\\\n& = & \\left(X'Z\\left(Z'Z\\right)^{-1}Z'X\\right)^{-1}X'Z\\left(Z'Z\\right)^{-1}Z'y.\\end{aligned}\\]\nexactly expression 2SLS \\(L>K\\). Therefore, 2SLS\ncan viewed special case GMM weighting matrix\n\\(\\left(Z'Z/n\\right)^{-1}\\). conditional homoskedasticity, 2SLS \nefficient estimator. 2SLS inefficient general cases \nheteroskedasticity, despite popularity.2SLS gets name can obtained using two steps: first\nregress \\(X\\) instruments \\(Z\\), regress \\(y\\) fitted\nvalue along included exogenous variables. However, 2SLS can\nactually obtained one step using equation. \nspecial case GMM.efficient estimator difficult implement, \neconometric theorist prefer efficient estimator \ninefficient estimator. benefits using efficient estimator \nlimited accurate coefficient estimation. Many specification\ntests, example \\(J\\)-statistic introduce soon, count \nefficient estimator lead familiar \\(\\chi^{2}\\) distribution\nnull hypotheses. Otherwise null asymptotic distributions\nnon-standard thereby critical values must found Monte\nCarlo simulations.","code":""},{"path":"generalized-method-of-moments.html","id":"gmm-in-nonlinear-model","chapter":"12 Generalized Method of Moments","heading":"12.3 GMM in Nonlinear Model","text":"principle GMM can used models parameter enters\nmoment conditions nonlinearly. Let\n\\(g_{}\\left(\\beta\\right)=g\\left(w_{},\\beta\\right)\\mapsto\\mathbb{R}^{L}\\)\nfunction data \\(w_{}\\) parameter \\(\\beta\\). economic\ntheory implies \\(\\mathbb{E}\\left[g_{}\\left(\\beta\\right)\\right]=0\\), \nstatisticians call estimating equations, can write GMM\npopulation criterion function \n\\[Q\\left(\\beta\\right)=\\mathbb{E}\\left[g_{}\\left(\\beta\\right)\\right]'W\\mathbb{E}\\left[g_{}\\left(\\beta\\right)\\right]\\]Nonlinear models nest linear model special case. linear\nIV model previous section, data \n\\(w_{}=\\left(y_{},x_{},z_{}\\right)\\), moment function \n\\(g\\left(w_{},\\beta\\right)=z_{}'\\left(y_{}-x_{}\\beta\\right)\\).practice use sample moments mimic population moments \ncriterion function\n\\[Q_{n}\\left(\\beta\\right)=\\left(\\frac{1}{n}\\sum_{=1}^{n}g_{}\\left(\\beta\\right)\\right)'W\\left(\\frac{1}{n}\\sum_{=1}^{n}g_{}\\left(\\beta\\right)\\right).\\]\nGMM estimator defined \n\\[\\hat{\\beta}=\\arg\\min_{\\beta}Q_{n}\\left(\\beta\\right).\\] \nnonlinear models, closed-form solution general unavailable,\nasymptotic properties can still established. state \nasymptotic properties without proofs.() model identified, \n\\[\\mathbb{P}\\left\\{ \\sup_{\\beta\\\\mathcal{B}}\\big|\\frac{1}{n}\\sum_{=1}^{n}g_{}\\left(\\beta\\right)-\\mathbb{E}\\left[g_{}\\left(\\beta\\right)\\right]\\big|>\\varepsilon\\right\\} \\to0\\]\nconstant \\(\\varepsilon>0\\) parametric space\n\\(\\mathcal{B}\\) closed set, \n\\(\\hat{\\beta}\\stackrel{\\mathrm{p}}{\\}\\beta.\\)\n(b) addition\n\\(\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}g_{}\\left(\\beta_{0}\\right)\\stackrel{d}{\\}N\\left(0,\\Omega\\right)\\)\n\n\\(\\Sigma=\\mathbb{E}\\left[\\frac{\\partial}{\\partial\\beta'}g_{}\\left(\\beta_{0}\\right)\\right]\\)\nfull column rank, \n\\[\\sqrt{n}\\left(\\hat{\\beta}-\\beta_{0}\\right)\\stackrel{d}{\\}N\\left(0,\\left(\\Sigma'W\\Sigma\\right)^{-1}\\left(\\Sigma'W\\Omega W\\Sigma\\right)\\left(\\Sigma'W\\Sigma\\right)^{-1}\\right)\\]\n\n\\(\\Omega=\\mathbb{E}\\left[g_{}\\left(\\beta_{0}\\right)g_{}\\left(\\beta_{0}\\right)'\\right]\\).\n(c) choose \\(W=\\Omega^{-1}\\), GMM estimator efficient,\nasymptotic variance becomes\n\\(\\left(\\Sigma'\\Omega^{-1}\\Sigma\\right)^{-1}\\).list assumptions statement incomplete. \nlay key conditions neglect technical details.\\(Q_{n}\\left(\\beta\\right)\\) measures close moments zeros.\ncan serve test statistic proper scaling. null\nhypothesis \\(\\mathbb{E}\\left[g_{}\\left(\\beta\\right)\\right]=0_{L}\\), \nSargan-Hansen \\(J\\)-test checks whether moment condition violated.\ntest statistic \\[\\begin{aligned}\nJ\\left(\\widehat{\\beta}\\right) & =nQ_{n}\\left(\\widehat{\\beta}\\right)=n\\left(\\frac{1}{n}\\sum_{=1}^{n}g_{}\\left(\\widehat{\\beta}\\right)\\right)'\\widehat{\\Omega}^{-1}\\left(\\frac{1}{n}\\sum_{=1}^{n}g_{}\\left(\\widehat{\\beta}\\right)\\right)\\\\\n& =\\left(\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}g_{}\\left(\\widehat{\\beta}\\right)\\right)'\\widehat{\\Omega}^{-1}\\left(\\frac{1}{\\sqrt{n}}\\sum_{=1}^{n}g_{}\\left(\\widehat{\\beta}\\right)\\right)\\end{aligned}\\]\n\\(\\widehat{\\Omega}\\) consistent estimator \\(\\Omega\\), \n\\(\\widehat{\\beta}\\) efficient estimator, example, two-step\nGMM estimator \\(\\widehat{\\beta}^{\\natural}(\\widehat{\\Omega}^{-1})\\). \nstatistic converges distribution \\(\\chi^{2}\\) random variable \ndegree freedom \\(L-K\\). , null,\n\\[J\\left(\\widehat{\\beta}\\right)\\stackrel{d}{\\}\\chi^{2}\\left(L-K\\right).\\]\nnull hypothesis false, test statistic tends \nlarge likely reject null.","code":""},{"path":"generalized-method-of-moments.html","id":"summary-10","chapter":"12 Generalized Method of Moments","heading":"12.4 Summary","text":"popularity GMM econometrics comes fact economic\ntheory often informative enough underlying parametric\nrelationship amongst variables. Instead, many economic assumptions\nsuggest moment restrictions. example, efficient market\nhypothesis postulates future price movement \\(\\Delta p_{t+1}\\)\npredicted available past information set \\(\\mathscr{}_{t}\\)\n\\(\\mathbb{E}\\left[\\Delta p_{t+1}|\\mathscr{}_{t}\\right]=0\\). \nimplies functions variables information set\n\\(\\mathscr{}_{t}\\) orthogonal \\(\\Delta p_{t+1}\\). plethora \nmoment conditions can constructed order test efficient\nmarket hypothesis.Conceptually simple though, GMM many practical issues reality.\nvast econometric literature issues GMM \nremedies.Historical notes: 2SLS attributed Theil (1953). \nlinear IV model, \\(J\\)-statistic proposed \nSargan (1958), Hansen (1982) extended nonlinear\nmodels.reading: quadratic form GMM makes difficult \naccommodate many moments big data problems. Empirical\nlikelihood alternative estimator GMM estimate models\ndefined moment restrictions. Shi (2016) solves \nestimation problem high-dimensional moments framework \nempirical likelihood.Zhentao Shi. Dec 3, 2020.","code":""}]
