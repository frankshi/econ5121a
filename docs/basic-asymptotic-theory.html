<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>6 Basic Asymptotic Theory | Econ5121</title>
<meta name="author" content="Zhentao Shi">
<meta name="description" content="Our universe, though enormous, consists of fewer than \(10^{82}\) atoms, which is a finite number. However, mathematical ideas are not bounded by secular realities. Asymptotic theory is about...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="6 Basic Asymptotic Theory | Econ5121">
<meta property="og:type" content="book">
<meta property="og:description" content="Our universe, though enormous, consists of fewer than \(10^{82}\) atoms, which is a finite number. However, mathematical ideas are not bounded by secular realities. Asymptotic theory is about...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="6 Basic Asymptotic Theory | Econ5121">
<meta name="twitter:description" content="Our universe, though enormous, consists of fewer than \(10^{82}\) atoms, which is a finite number. However, mathematical ideas are not bounded by secular realities. Asymptotic theory is about...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Alice-0.4.1/font.css" rel="stylesheet">
<link href="libs/_DM%20Mono-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Spectral-0.4.1/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Econ5121</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="conditional-expectation.html"><span class="header-section-number">3</span> Conditional Expectation</a></li>
<li><a class="" href="least-squares-linear-algebra.html"><span class="header-section-number">4</span> Least Squares: Linear Algebra</a></li>
<li><a class="" href="least-squares-finite-sample-theory.html"><span class="header-section-number">5</span> Least Squares: Finite Sample Theory</a></li>
<li><a class="active" href="basic-asymptotic-theory.html"><span class="header-section-number">6</span> Basic Asymptotic Theory</a></li>
<li><a class="" href="asymptotic-properties-of-least-squares.html"><span class="header-section-number">7</span> Asymptotic Properties of Least Squares</a></li>
<li><a class="" href="asymptotic-properties-of-mle.html"><span class="header-section-number">8</span> Asymptotic Properties of MLE</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></li>
<li><a class="" href="panel-data.html"><span class="header-section-number">10</span> Panel Data</a></li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">11</span> Endogeneity</a></li>
<li><a class="" href="generalized-method-of-moments.html"><span class="header-section-number">12</span> Generalized Method of Moments</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/zhentaoshi/Econ5121A">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="basic-asymptotic-theory" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Basic Asymptotic Theory<a class="anchor" aria-label="anchor" href="#basic-asymptotic-theory"><i class="fas fa-link"></i></a>
</h1>
<p>Our universe, though enormous, consists of fewer than <span class="math inline">\(10^{82}\)</span> atoms,
which is a finite number. However, mathematical ideas are not bounded by
secular realities. Asymptotic theory is about behaviors of statistics
when the sample size is arbitrarily large up to infinity. It is a set of
approximation techniques to simplify complicated finite-sample analysis.
Asymptotic theory is the cornerstone of modern econometrics. It sheds
lights on estimation and inference procedures under much more general
conditions than what are covered by exact finite sample theory.</p>
<p>Nevertheless, we always have at hand a finite sample, and mostly it is
difficult to increase the sample size in reality. Asymptotic theory
rarely answers “how large is large”, and we must be cautious about the
treacherous landscape of <em>asymptopia</em>. In the era of big data, albeit
the sheer size of data balloons dramatically, we build more
sophisticated models to better capture heterogeneity in the data. Large
sample is a relative notion to the complexity of the model and
underlying (in)dependence structure of the data.</p>
<p>Both the classical parametric approach, which is based on hard-to-verify
parametric assumptions, and the asymptotic approach, which is predicated
on imaginary infinite sequences, deviate from the reality. Which
approach is more constructive can only be judged case by case. The
prevalence of asymptotic theory is its mathematical amenability and
generality. The law of evolution elevates asymptotic theory to the
throne of mathematical statistics of our time.</p>
<div id="modes-of-convergence" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Modes of Convergence<a class="anchor" aria-label="anchor" href="#modes-of-convergence"><i class="fas fa-link"></i></a>
</h2>
<p>We first review what is <em>convergence</em> for a non-random sequence, which
you learned in high school. Let <span class="math inline">\(z_{1},z_{2},\ldots\)</span> be an infinite
sequence of non-random variables.</p>
<p>Convergence of this non-random sequence means that for any
<span class="math inline">\(\varepsilon&gt;0\)</span>, there exists an <span class="math inline">\(N\left(\varepsilon\right)\)</span> such that
for all <span class="math inline">\(n&gt;N\left(\varepsilon\right)\)</span>, we have
<span class="math inline">\(\left|z_{n}-z\right|&lt;\varepsilon\)</span>. We say <span class="math inline">\(z\)</span> is the limit of <span class="math inline">\(z_{n}\)</span>,
and write <span class="math inline">\(z_{n}\to z\)</span> or <span class="math inline">\(\lim_{n\to\infty}z_{n}=z\)</span>.</p>
<p>Instead of a deterministic sequence, we are interested in the
convergence of a sequence of random variables. Since a random variable
is “random” thanks to the induced probability measure by the measurable
function, we must be clear what <em>convergence</em> means. Several modes of
convergence are widely used.</p>
<p>We say a sequence of random variables <span class="math inline">\(\left(z_{n}\right)\)</span> converges in
probability to <span class="math inline">\(z\)</span>, where <span class="math inline">\(z\)</span> can be either a random variable or a
non-random constant, if for any <span class="math inline">\(\varepsilon&gt;0\)</span>, the probability
<span class="math inline">\(P\left\{ \omega:\left|z_{n}\left(\omega\right)-z\right|&lt;\varepsilon\right\} \to1\)</span>
(or equivalently
<span class="math inline">\(P\left\{ \omega:\left|z_{n}\left(\omega\right)-z\right|\geq\varepsilon\right\} \to0\)</span>)
as <span class="math inline">\(n\to\infty\)</span>. We can write <span class="math inline">\(z_{n}\stackrel{p}{\to}z\)</span> or
<span class="math inline">\(\mathrm{plim}_{n\to\infty}z_{n}=z\)</span>.</p>
<p>A sequence of random variables <span class="math inline">\(\left(z_{n}\right)\)</span> converges in
squared-mean to <span class="math inline">\(z\)</span>, where <span class="math inline">\(z\)</span> can be either a random variable or a
non-random constant, if <span class="math inline">\(E\left[\left(z_{n}-z\right)^{2}\right]\to0.\)</span> It
is denoted as <span class="math inline">\(z_{n}\stackrel{m.s.}{\to}z\)</span>.</p>
<p>In these definitions either
<span class="math inline">\(P\left\{ \omega:\left|z_{n}\left(\omega\right)-z\right|&gt;\varepsilon\right\}\)</span>
or <span class="math inline">\(E\left[\left(z_{n}-z\right)^{2}\right]\)</span> is a non-random quantity,
and it converges to 0 as a non-random sequence.</p>
<p>Squared-mean convergence is stronger than convergence in probability.
That is, <span class="math inline">\(z_{n}\stackrel{m.s.}{\to}z\)</span> implies <span class="math inline">\(z_{n}\stackrel{p}{\to}z\)</span>
but the converse is untrue. Here is an example.</p>
<p><span id="eg:in_p_in_ms" label="eg:in_p_in_ms"><span class="math display">\[eg:in\_p\_in\_ms\]</span></span><span class="math inline">\((z_{n})\)</span> is
a sequence of binary random variables: <span class="math inline">\(z_{n}=\sqrt{n}\)</span> with probability
<span class="math inline">\(1/n\)</span>, and <span class="math inline">\(z_{n}=0\)</span> with probability <span class="math inline">\(1-1/n\)</span>. Then
<span class="math inline">\(z_{n}\stackrel{p}{\to}0\)</span> but <span class="math inline">\(z_{n}\stackrel{m.s.}{\nrightarrow}0\)</span>. To
verify these claims, notice that for any <span class="math inline">\(\varepsilon&gt;0\)</span>, we have
<span class="math inline">\(P\left(\omega:\left|z_{n}\left(\omega\right)-0\right|&lt;\varepsilon\right)=P\left(\omega:z_{n}\left(\omega\right)=0\right)=1-1/n\rightarrow1\)</span>
and thereby <span class="math inline">\(z_{n}\stackrel{p}{\to}0\)</span>. On the other hand,
<span class="math inline">\(E\left[\left(z_{n}-0\right)^{2}\right]=n\cdot1/n+0\cdot(1-1/n)=1\nrightarrow0,\)</span>
so <span class="math inline">\(z_{n}\stackrel{m.s.}{\nrightarrow}0\)</span>.</p>
<p>Example <a href="basic-asymptotic-theory.html#eg:in_p_in_ms" reference-type="ref" reference="eg:in_p_in_ms"><span class="math display">\[eg:in\_p\_in\_ms\]</span></a> highlights the difference between the two
modes of convergence. Convergence in probability does not count what
happens on a subset in the sample space of small probability.
Squared-mean convergence deals with the average over the entire
probability space. If a random variable can take a wild value, with
small probability though, it may blow away the squared-mean convergence.
On the contrary, such irregularity does not undermine convergence in
probability.</p>
<p>Both convergence in probability and squared-mean convergence are about
convergence of random variables to a target random variable or constant.
That is, the distribution of <span class="math inline">\(z_{n}-z\)</span> is concentrated around 0 as
<span class="math inline">\(n\to\infty\)</span>. Instead, <em>convergence in distribution</em> is about the
convergence of CDF, but not the random variable. Let
<span class="math inline">\(F_{z_{n}}\left(\cdot\right)\)</span> be the CDF of <span class="math inline">\(z_{n}\)</span> and
<span class="math inline">\(F_{z}\left(\cdot\right)\)</span> be the CDF of <span class="math inline">\(z\)</span>.</p>
<p>We say a sequence of random variables <span class="math inline">\(\left(z_{n}\right)\)</span> converges in
distribution to a random variable <span class="math inline">\(z\)</span> if
<span class="math inline">\(F_{z_{n}}\left(a\right)\to F_{z}\left(a\right)\)</span> as <span class="math inline">\(n\to\infty\)</span> at each
point <span class="math inline">\(a\in\mathbb{R}\)</span> such that where <span class="math inline">\(F_{z}\left(\cdot\right)\)</span> is
continuous. We write <span class="math inline">\(z_{n}\stackrel{d}{\to}z\)</span>.</p>
<p>Convergence in distribution is the weakest mode. If
<span class="math inline">\(z_{n}\stackrel{p}{\to}z\)</span>, then <span class="math inline">\(z_{n}\stackrel{d}{\to}z\)</span>. The converse
is not true in general, unless <span class="math inline">\(z\)</span> is a non-random constant (A constant
<span class="math inline">\(z\)</span> can be viewed as a degenerate random variables, with a corresponding
“CDF” <span class="math inline">\(F_{z}\left(\cdot\right)=1\left\{ \cdot\geq z\right\}\)</span>.</p>
<p>Let <span class="math inline">\(x\sim N\left(0,1\right)\)</span>. If <span class="math inline">\(z_{n}=x+1/n\)</span>, then
<span class="math inline">\(z_{n}\stackrel{p}{\to}x\)</span> and of course <span class="math inline">\(z_{n}\stackrel{d}{\to}x\)</span>.
However, if <span class="math inline">\(z_{n}=-x+1/n\)</span>, or <span class="math inline">\(z_{n}=y+1/n\)</span> where
<span class="math inline">\(y\sim N\left(0,1\right)\)</span> is independent of <span class="math inline">\(x\)</span>, then
<span class="math inline">\(z_{n}\stackrel{d}{\to}x\)</span> but <span class="math inline">\(z_{n}\stackrel{p}{\nrightarrow}x\)</span>.</p>
<p><span class="math inline">\((z_{n})\)</span> is a sequence of binary random variables: <span class="math inline">\(z_{n}=n\)</span> with
probability <span class="math inline">\(1/\sqrt{n}\)</span>, and <span class="math inline">\(z_{n}=0\)</span> with probability <span class="math inline">\(1-1/\sqrt{n}\)</span>.
Then <span class="math inline">\(z_{n}\stackrel{d}{\to}z=0.\)</span> Because
<span class="math display">\[F_{z_{n}}\left(a\right)=\begin{cases}
0 &amp; a&lt;0\\
1-1/\sqrt{n} &amp; 0\leq a\leq n\\
1 &amp; a\geq n
\end{cases}.\]</span> <span class="math inline">\(F_{z}\left(a\right)=\begin{cases} 0, &amp; a&lt;0\\ 1 &amp; a\geq0 \end{cases}\)</span>. It is easy to verify that <span class="math inline">\(F_{z_{n}}\left(a\right)\)</span>
converges to <span class="math inline">\(F_{z}\left(a\right)\)</span> <em>pointwisely</em> on each point in
<span class="math inline">\(\left(-\infty,0\right)\cup\left(0,+\infty\right)\)</span>, where
<span class="math inline">\(F_{z}\left(a\right)\)</span> is continuous.</p>
<p>So far we have talked about convergence of scalar variables. These three
modes of converges can be easily generalized to random vectors. In
particular, the <em>Cramer-Wold device</em> collapses a random vector into a
random vector via arbitrary linear combination. We say a sequence of
<span class="math inline">\(K\)</span>-dimensional random vectors <span class="math inline">\(\left(z_{n}\right)\)</span> converge in
distribution to <span class="math inline">\(z\)</span> if <span class="math inline">\(\lambda'z_{n}\stackrel{d}{\to}\lambda'z\)</span> for any
<span class="math inline">\(\lambda\in\mathbb{R}^{K}\)</span> and <span class="math inline">\(\left\Vert \lambda\right\Vert _{2}=1.\)</span></p>
</div>
<div id="law-of-large-numbers" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Law of Large Numbers<a class="anchor" aria-label="anchor" href="#law-of-large-numbers"><i class="fas fa-link"></i></a>
</h2>
<p>(Weak) law of large numbers (LLN) is a collection of statements about
convergence in probability of the sample average to its population
counterpart. The basic form of LLN is:
<span class="math display">\[\frac{1}{n}\sum_{i=1}^{n}(z_{i}-E[z_{i}])\stackrel{p}{\to}0\]</span> as
<span class="math inline">\(n\to\infty\)</span>. Various versions of LLN work under different assumptions
about some features and/or dependence of the underlying random
variables.</p>
<div id="cherbyshev-lln" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> Cherbyshev LLN<a class="anchor" aria-label="anchor" href="#cherbyshev-lln"><i class="fas fa-link"></i></a>
</h3>
<p>We illustrate LLN by the simple example of Chebyshev LLN, which can be
proved by elementary calculation. It utilizes the <em>Chebyshev
inequality</em>.</p>
<ul>
<li>
<em>Chebyshev inequality</em>: If a random variable <span class="math inline">\(x\)</span> has a finite second
moment <span class="math inline">\(E\left[x^{2}\right]&lt;\infty\)</span>, then we have
<span class="math inline">\(P\left\{ \left|x\right|&gt;\varepsilon\right\} \leq E\left[x^{2}\right]/\varepsilon^{2}\)</span>
for any constant <span class="math inline">\(\varepsilon&gt;0\)</span>.</li>
</ul>
<p>Show that if <span class="math inline">\(r_{2}\geq r_{1}\geq1\)</span>, then
<span class="math inline">\(E\left[\left|x\right|^{r_{2}}\right]&lt;\infty\)</span> implies
<span class="math inline">\(E\left[\left|x\right|^{r_{1}}\right]&lt;\infty.\)</span> (Hint: use Holder’s
inequality.)</p>
<p>The Chebyshev inequality is a special case of the <em>Markov inequality</em>.</p>
<ul>
<li>
<em>Markov inequality</em>: If a random variable <span class="math inline">\(x\)</span> has a finite <span class="math inline">\(r\)</span>-th
absolute moment <span class="math inline">\(E\left[\left|x\right|^{r}\right]&lt;\infty\)</span> for some
<span class="math inline">\(r\ge1\)</span>, then we have
<span class="math inline">\(P\left\{ \left|x\right|&gt;\varepsilon\right\} \leq E\left[\left|x\right|^{r}\right]/\varepsilon^{r}\)</span>
any constant <span class="math inline">\(\varepsilon&gt;0\)</span>.</li>
</ul>
<p>It is easy to verify the Markov inequality.
<span class="math display">\[\begin{aligned}E\left[\left|x\right|^{r}\right] &amp; =\int_{\left|x\right|&gt;\varepsilon}\left|x\right|^{r}dF_{X}+\int_{\left|x\right|\leq\varepsilon}\left|x\right|^{r}dF_{X}\\
&amp; \geq\int_{\left|x\right|&gt;\varepsilon}\left|x\right|^{r}dF_{X}\\
&amp; \geq\varepsilon^{r}\int_{\left|x\right|&gt;\varepsilon}dF_{X}=\varepsilon^{r}P\left\{ \left|x\right|&gt;\varepsilon\right\} .
\end{aligned}\]</span> Rearrange the above inequality and we obtain the Markov
inequality.</p>
<p>Let the <em>partial sum</em> <span class="math inline">\(S_{n}=\sum_{i=1}^{n}x_{i}\)</span>, where
<span class="math inline">\(\mu_{i}=E\left[x_{i}\right]\)</span> and
<span class="math inline">\(\sigma_{i}^{2}=\mathrm{var}\left[x_{i}\right]\)</span>. We apply the Chebyshev
inequality to the sample mean
<span class="math inline">\(z_{n}=\overline{x}-\bar{\mu}=n^{-1}\left(S_{n}-E\left[S_{n}\right]\right)\)</span>.
<span class="math display">\[\begin{aligned}
P\left\{ \left|z_{n}\right|\geq\varepsilon\right\}  &amp; =P\left\{ n^{-1}\left|S_{n}-E\left[S_{n}\right]\right|\geq\varepsilon\right\} \nonumber \\
&amp; \leq E\left[\left(n^{-1}\sum_{i=1}^{n}\left(x_{i}-\mu_{i}\right)\right)^{2}\right]/\varepsilon^{2}\nonumber \\
&amp; =\left(n\varepsilon\right)^{-2}\left\{ E\left[\sum_{i=1}^{n}\left(x_{i}-\mu_{i}\right)^{2}\right]+\sum_{i=1}^{n}\sum_{j\neq i}E\left[\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right]\right\} \nonumber \\
&amp; =\left(n\varepsilon\right)^{-2}\left\{ \sum_{i=1}^{n}\mathrm{var}\left(x_{i}\right)+\sum_{i=1}^{n}\sum_{j\neq i}\mathrm{cov}\left(x_{i},x_{j}\right)\right\} .\label{eq:cheby_mean}\end{aligned}\]</span>
Convergence in probability holds if the right-hand side shrinks to 0 as
<span class="math inline">\(n\to\infty\)</span>. For example, If <span class="math inline">\(x_{1},\ldots,x_{n}\)</span> are iid with
<span class="math inline">\(\mathrm{var}\left(x_{1}\right)=\sigma^{2}\)</span>, then the RHS of
(<a href="#eq:cheby_mean" reference-type="ref" reference="eq:cheby_mean"><span class="math display">\[eq:cheby\_mean\]</span></a>) is
<span class="math inline">\(\left(n\varepsilon\right)^{-2}\left(n\sigma^{2}\right)=o\left(n^{-1}\right)\to0\)</span>.
This result gives the Chebyshev LLN:</p>
<ul>
<li>Chebyshev LLN: If <span class="math inline">\(\left(z_{1},\ldots,z_{n}\right)\)</span> is a sample of
iid observations, <span class="math inline">\(E\left[z_{1}\right]=\mu\)</span> , and
<span class="math inline">\(\sigma^{2}=\mathrm{var}\left[z_{1}\right]&lt;\infty\)</span> exists, then
<span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}z_{i}\stackrel{p}{\to}\mu.\)</span>
</li>
</ul>
<p>The convergence in probability can be indeed maintained under much more
general conditions than under iid case. The random variables in the
sample do not have to be identically distributed, and they do not have
to be independent either.</p>
<p>Consider an inid (independent but non-identically distributed) sample
<span class="math inline">\(\left(x_{1},\ldots,x_{n}\right)\)</span> with <span class="math inline">\(E\left[x_{i}\right]=0\)</span> and
<span class="math inline">\(\mathrm{var}\left[x_{i}\right]=\sqrt{n}c\)</span> for some constant <span class="math inline">\(c&gt;0\)</span>. Use
the Chebyshev inequality to show that
<span class="math inline">\(n^{-1}\sum_{i=1}^{n}x_{i}\stackrel{p}{\to}0\)</span>.</p>
<p>Consider the time series moving average model
<span class="math inline">\(x_{i}=\varepsilon_{i}+\theta\varepsilon_{i-1}\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span>,
where <span class="math inline">\(\left|\theta\right|&lt;1\)</span>, <span class="math inline">\(E\left[\varepsilon_{i}\right]=0\)</span>,
<span class="math inline">\(\mathrm{var}\left[\varepsilon_{i}\right]=\sigma^{2}\)</span>, and
<span class="math inline">\(\left(\varepsilon_{i}\right)_{i=0}^{n}\)</span> iid. Use the Chebyshev
inequality to show that <span class="math inline">\(n^{-1}\sum_{i=1}^{n}x_{i}\stackrel{p}{\to}0\)</span>.</p>
<p>Another useful LLN is the <em>Kolmogorov LLN</em>. Since its derivation
requires more advanced knowledge of probability theory, we state the
result without proof.</p>
<ul>
<li>Kolmogorov LLN: If <span class="math inline">\(\left(z_{1},\ldots,z_{n}\right)\)</span> is a sample of
iid observations and <span class="math inline">\(E\left[z_{1}\right]=\mu\)</span> exists, then
<span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}z_{i}\stackrel{p}{\to}\mu\)</span>.</li>
</ul>
<p>Compared with the Chebyshev LLN, the Kolmogorov LLN only requires the
existence of the population mean, but not any higher moments. On the
other hand, iid is essential for the Kolmogorov LLN.</p>
<p>Consider three distributions: standard normal <span class="math inline">\(N\left(0,1\right)\)</span>,
<span class="math inline">\(t\left(2\right)\)</span> (zero mean, infinite variance), and the Cauchy
distribution (no moments exist). We plot paths of the sample average
with <span class="math inline">\(n=2^{1},2^{2},\ldots,2^{20}\)</span>. We will see that the sample averages
of <span class="math inline">\(N\left(0,1\right)\)</span> and <span class="math inline">\(t\left(2\right)\)</span> converge, but that of the
Cauchy distribution does not.</p>
<p><strong>knitrout</strong></p>
</div>
</div>
<div id="central-limit-theorem" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Central Limit Theorem<a class="anchor" aria-label="anchor" href="#central-limit-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>The central limit theorem (CLT) is a collection of probability results
about the convergence in distribution to a stable distribution. The
limiting distribution is usually the Gaussian distribution. The basic
form of the CLT is:</p>
<ul>
<li>
<em>Under some conditions to be spelled out</em>, the sample average of
<em>zero-mean</em> random variables <span class="math inline">\(\left(z_{1},\ldots,z_{n}\right)\)</span>
multiplied by <span class="math inline">\(\sqrt{n}\)</span> satisfies
<span class="math display">\[\frac{1}{\sqrt{n}}\sum_{i=1}^{n}z_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}\right)\]</span>
as <span class="math inline">\(n\to\infty\)</span>.</li>
</ul>
<p>Various versions of CLT work under different assumptions about the
random variables. <em>Lindeberg-Levy CLT</em> is the simplest CLT.</p>
<ul>
<li>If the sample <span class="math inline">\(\left(x_{1},\ldots,x_{n}\right)\)</span> is iid,
<span class="math inline">\(E\left[x_{1}\right]=0\)</span> and
<span class="math inline">\(\mathrm{var}\left[x_{1}\right]=\sigma^{2}&lt;\infty\)</span>, then
<span class="math inline">\(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}\right)\)</span>.</li>
</ul>
<p>Lindeberg-Levy CLT can be proved by the <em>moment generating function</em>.
For any random variable <span class="math inline">\(x\)</span>, the function
<span class="math inline">\(M_{x}\left(t\right)=E\left[\exp\left(xt\right)\right]\)</span> is called its
the <em>moment generating function</em> (MGF) if it exists. MGF fully describes
a distribution, just like PDF or CDF. For example, the MGF of
<span class="math inline">\(N\left(\mu,\sigma^{2}\right)\)</span> is
<span class="math inline">\(\exp\left(\mu t+\frac{1}{2}\sigma^{2}t^{2}\right)\)</span>.</p>
<p>If <span class="math inline">\(E\left[\left|x\right|^{k}\right]&lt;\infty\)</span> for a positive integer <span class="math inline">\(k\)</span>,
then
<span class="math display">\[M_{X}\left(t\right)=1+tE\left[X\right]+\frac{t^{2}}{2}E\left[X^{2}\right]+\ldots\frac{t}{k!}E\left[X^{k}\right]+O\left(t^{k+1}\right).\]</span>
Under the assumption of Lindeberg-Levy CLT,
<span class="math display">\[M_{\frac{X_{i}}{\sqrt{n}}}\left(t\right)=1+\frac{t^{2}}{2n}\sigma^{2}+O\left(\frac{t^{3}}{n^{3/2}}\right)\]</span>
for all <span class="math inline">\(i\)</span>, and by independence we have
<span class="math display">\[\begin{aligned}M_{\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}}\left(t\right) &amp; =\prod_{i=1}^{n}M_{\frac{X_{i}}{\sqrt{n}}}\left(t\right)=\left(1+\frac{t^{2}}{2n}\sigma^{2}+O\left(\frac{t^{3}}{n^{3/2}}\right)\right)^{n}\\
&amp; \to\exp\left(\frac{\sigma^{2}}{2}t^{2}\right),
\end{aligned}\]</span> where the limit is exactly the characteristic function
of <span class="math inline">\(N\left(0,\sigma^{2}\right)\)</span>.</p>
<p>This proof with MGF is simple and elementary. Its drawback is that not
all distributions have a well-defined MGF. A more general proof can be
carried out by replacing MGF with the <em>characteristic function</em>
<span class="math inline">\(\varphi_{x}\left(t\right)=E\left[\exp\left(\mathrm{i}xt\right)\right]\)</span>,
where “<span class="math inline">\(\mathrm{i}\)</span>” is the imaginary number. The characteristic
function is the <em>Fourier transform</em> of the probability measure and it
always exists. Such a proof will require background knowledge of Fourier
transform and inverse transform, which we do not pursuit here.</p>
<ul>
<li><p>Lindeberg-Feller CLT: <span class="math inline">\(\left(x_{i}\right)_{i=1}^{n}\)</span> is inid. If the
<em>Lindeberg condition</em> is satisfied (for any fixed <span class="math inline">\(\varepsilon&gt;0\)</span>,
<span class="math inline">\(\frac{1}{s_{n}^{2}}\sum_{i=1}^{n}E\left[x_{i}^{2}\cdot\boldsymbol{1}\left\{ \left|x_{i}\right|\geq\varepsilon s_{n}\right\} \right]\to0\)</span>
where <span class="math inline">\(s_{n}=\sqrt{\sum_{i=1}^{n}\sigma_{i}^{2}}\)</span>), then we have
<span class="math display">\[\frac{\sum_{i=1}^{n}x_{i}}{s_{n}}\stackrel{d}{\to}N\left(0,1\right).\]</span></p></li>
<li><p>Lyapunov CLT: <span class="math inline">\(\left(x_{i}\right)_{i=1}^{n}\)</span> is inid. If
<span class="math inline">\(\max_{i\leq n}E\left[\left|x_{i}\right|^{3}\right]&lt;C&lt;\infty,\)</span> then
we have
<span class="math display">\[\frac{\sum_{i=1}^{n}x_{i}}{s_{n}}\stackrel{d}{\to}N\left(0,1\right).\]</span></p></li>
</ul>
<p>This is a simulated example.</p>
<p><span class="math display">\[knitrout\]</span></p>
</div>
<div id="tools-for-transformations" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Tools for Transformations<a class="anchor" aria-label="anchor" href="#tools-for-transformations"><i class="fas fa-link"></i></a>
</h2>
<p>In their original forms, LLN deals with the sample mean, and CLT handles
the scaled (by <span class="math inline">\(\sqrt{n}\)</span>) and/or standardized (by standard deviation)
sample mean. However, most of the econometric estimators of interest are
functions of sample means. For example, in the OLS estimator
<span class="math display">\[\widehat{\beta}=\left(\frac{1}{n}\sum_{i}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i}x_{i}y_{i}\]</span>
involves matrix inverse and the matrix-vector multiplication. We need
tools to handle transformations.</p>
<ul>
<li><p>Continuous mapping theorem 1: If <span class="math inline">\(x_{n}\stackrel{p}{\to}a\)</span> and
<span class="math inline">\(f\left(\cdot\right)\)</span> is continuous at <span class="math inline">\(a\)</span>, then
<span class="math inline">\(f\left(x_{n}\right)\stackrel{p}{\to}f\left(a\right)\)</span>.</p></li>
<li><p>Continuous mapping theorem 2: If <span class="math inline">\(x_{n}\stackrel{d}{\to}x\)</span> and
<span class="math inline">\(f\left(\cdot\right)\)</span> is continuous almost surely on the support of
<span class="math inline">\(x\)</span>, then <span class="math inline">\(f\left(x_{n}\right)\stackrel{d}{\to}f\left(x\right)\)</span>.</p></li>
<li>
<p>Slutsky’s theorem: If <span class="math inline">\(x_{n}\stackrel{d}{\to}x\)</span> and
<span class="math inline">\(y_{n}\stackrel{p}{\to}a\)</span>, then</p>
<ul>
<li><p><span class="math inline">\(x_{n}+y_{n}\stackrel{d}{\to}x+a\)</span></p></li>
<li><p><span class="math inline">\(x_{n}y_{n}\stackrel{d}{\to}ax\)</span></p></li>
<li><p><span class="math inline">\(x_{n}/y_{n}\stackrel{d}{\to}x/a\)</span> if <span class="math inline">\(a\neq0\)</span>.</p></li>
</ul>
</li>
</ul>
<p>Slutsky’s theorem consists of special cases of the continuous mapping
theorem 2. Only because the addition, multiplication and division are
encountered so frequently in practice, we list it as a separate theorem.</p>
<ul>
<li>Delta method: if
<span class="math inline">\(\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega\right)\)</span>,
and <span class="math inline">\(f\left(\cdot\right)\)</span> is continuously differentiable at
<span class="math inline">\(\theta_{0}\)</span> (meaning
<span class="math inline">\(\frac{\partial}{\partial\theta}f\left(\cdot\right)\)</span> is continuous
at <span class="math inline">\(\theta_{0}\)</span>), then we have
<span class="math display">\[\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}N\left(0,\frac{\partial f}{\partial\theta'}\left(\theta_{0}\right)\Omega\left(\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)\right)'\right).\]</span>
</li>
</ul>
<p>Take a Taylor expansion of <span class="math inline">\(f\left(\widehat{\theta}\right)\)</span> around
<span class="math inline">\(f\left(\theta_{0}\right)\)</span>:
<span class="math display">\[f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)=\frac{\partial f\left(\dot{\theta}\right)}{\partial\theta'}\left(\widehat{\theta}-\theta_{0}\right),\]</span>
where <span class="math inline">\(\dot{\theta}\)</span> lies on the line segment between <span class="math inline">\(\widehat{\theta}\)</span>
and <span class="math inline">\(\theta_{0}\)</span>. Multiply <span class="math inline">\(\sqrt{n}\)</span> on both sides,
<span class="math display">\[\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)=\frac{\partial f\left(\dot{\theta}\right)}{\partial\theta'}\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right).\]</span>
Because <span class="math inline">\(\widehat{\theta}\stackrel{p}{\to}\theta_{0}\)</span> implies
<span class="math inline">\(\dot{\theta}\stackrel{p}{\to}\theta_{0}\)</span> and
<span class="math inline">\(\frac{\partial}{\partial\theta'}f\left(\cdot\right)\)</span> is continuous at
<span class="math inline">\(\theta_{0}\)</span>, we have
<span class="math inline">\(\frac{\partial}{\partial\theta'}f\left(\dot{\theta}\right)\stackrel{p}{\to}\frac{\partial f\left(\theta_{0}\right)}{\partial\theta'}\)</span>
by the continuous mapping theorem 1. In view of
<span class="math inline">\(\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega\right)\)</span>,
Slutsky’s Theorem implies
<span class="math display">\[\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}\frac{\partial f\left(\theta_{0}\right)}{\partial\theta'}N\left(0,\Omega\right)\]</span>
and the conclusion follows.</p>
</div>
<div id="summary-4" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Summary<a class="anchor" aria-label="anchor" href="#summary-4"><i class="fas fa-link"></i></a>
</h2>
<p>Asymptotic theory is a topic with vast breadth and depth. In this
chapter we only scratch the very surface of it. We will discuss in the
next chapter how to apply the asymptotic tools we learned here to the
OLS estimator.</p>
<p><strong>Historical notes</strong>: Before 1980s, most econometricians did not have a
good training in mathematical rigor to master asymptotic theory. A few
prominent young (at that time) econometricians came to the field and
changed the situation, among them were Halbert White (UCSD), Peter
C.B. Phillips (Yale) and Peter Robinson (LSE), to name a few.</p>
<p><strong>Further reading</strong>: Halbert White (1950-2012) wrote an accessible book
<span class="citation">(<a href="generalized-method-of-moments.html#ref-white2014asymptotic" role="doc-biblioref">White 2000</a> first edition 1984)</span> to introduce asymptotics to
econometricians. This book remains popular among researchers and
graduate students in economics. <span class="citation">Davidson (<a href="generalized-method-of-moments.html#ref-davidson1994stochastic" role="doc-biblioref">1994</a>)</span> is a longer and
more self-contained monograph.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="least-squares-finite-sample-theory.html"><span class="header-section-number">5</span> Least Squares: Finite Sample Theory</a></div>
<div class="next"><a href="asymptotic-properties-of-least-squares.html"><span class="header-section-number">7</span> Asymptotic Properties of Least Squares</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#basic-asymptotic-theory"><span class="header-section-number">6</span> Basic Asymptotic Theory</a></li>
<li><a class="nav-link" href="#modes-of-convergence"><span class="header-section-number">6.1</span> Modes of Convergence</a></li>
<li>
<a class="nav-link" href="#law-of-large-numbers"><span class="header-section-number">6.2</span> Law of Large Numbers</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#cherbyshev-lln"><span class="header-section-number">6.2.1</span> Cherbyshev LLN</a></li></ul>
</li>
<li><a class="nav-link" href="#central-limit-theorem"><span class="header-section-number">6.3</span> Central Limit Theorem</a></li>
<li><a class="nav-link" href="#tools-for-transformations"><span class="header-section-number">6.4</span> Tools for Transformations</a></li>
<li><a class="nav-link" href="#summary-4"><span class="header-section-number">6.5</span> Summary</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/zhentaoshi/Econ5121A/blob/master/05-lecture.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/zhentaoshi/Econ5121A/edit/master/05-lecture.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Econ5121</strong>" was written by Zhentao Shi. It was last built on 2022-06-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
